{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_complex_np(data):\n",
    "\n",
    "    data = data.numpy()\n",
    "    return data[..., 0] + 1j * data[..., 1]\n",
    "\n",
    "\n",
    "def to_tensor(data):\n",
    "\n",
    "    if np.iscomplexobj(data):\n",
    "        data = np.stack((data.real, data.imag), axis=-1)\n",
    "    return torch.from_numpy(data)\n",
    "\n",
    "\n",
    "def apply_mask(data, mask_func, seed=None):\n",
    "\n",
    "    shape = np.array(data.shape)\n",
    "    shape[:-3] = 1\n",
    "    mask = mask_func(shape, seed)\n",
    "    return torch.where(mask == 0, torch.Tensor([0]), data), mask\n",
    "\n",
    "\n",
    "def fft2(data):\n",
    "\n",
    "    assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.fft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def ifft2(data):\n",
    "\n",
    "    assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.ifft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def complex_abs(data):\n",
    "\n",
    "    assert data.size(-1) == 2\n",
    "    return (data ** 2).sum(dim=-1).sqrt()\n",
    "\n",
    "\n",
    "def root_sum_of_squares(data, dim=0):\n",
    "\n",
    "    return torch.sqrt((data ** 2).sum(dim))\n",
    "\n",
    "\n",
    "def center_crop(data, shape):\n",
    "\n",
    "    assert 0 < shape[0] <= data.shape[-2]\n",
    "    assert 0 < shape[1] <= data.shape[-1]\n",
    "    w_from = (data.shape[-2] - shape[0]) // 2\n",
    "    h_from = (data.shape[-1] - shape[1]) // 2\n",
    "    w_to = w_from + shape[0]\n",
    "    h_to = h_from + shape[1]\n",
    "    return data[..., w_from:w_to, h_from:h_to]\n",
    "\n",
    "\n",
    "def complex_center_crop(data, shape):\n",
    "\n",
    "    assert 0 < shape[0] <= data.shape[-3]\n",
    "    assert 0 < shape[1] <= data.shape[-2]\n",
    "    w_from = (data.shape[-3] - shape[0]) // 2\n",
    "    h_from = (data.shape[-2] - shape[1]) // 2\n",
    "    w_to = w_from + shape[0]\n",
    "    h_to = h_from + shape[1]\n",
    "    return data[..., w_from:w_to, h_from:h_to, :]\n",
    "\n",
    "\n",
    "def normalize(data, mean, stddev, eps=0.):\n",
    "\n",
    "    return (data - mean) / (stddev + eps)\n",
    "\n",
    "\n",
    "def normalize_instance(data, eps=0.):\n",
    "\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    return normalize(data, mean, std, eps), mean, std\n",
    "    \n",
    "\n",
    "def roll(x, shift, dim):\n",
    "\n",
    "    if isinstance(shift, (tuple, list)):\n",
    "        assert len(shift) == len(dim)\n",
    "        for s, d in zip(shift, dim):\n",
    "            x = roll(x, s, d)\n",
    "        return x\n",
    "    shift = shift % x.size(dim)\n",
    "    if shift == 0:\n",
    "        return x\n",
    "    left = x.narrow(dim, 0, x.size(dim) - shift)\n",
    "    right = x.narrow(dim, x.size(dim) - shift, shift)\n",
    "    return torch.cat((right, left), dim=dim)\n",
    "\n",
    "\n",
    "def fftshift(x, dim=None):\n",
    "\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [dim // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = x.shape[dim] // 2\n",
    "    else:\n",
    "        shift = [x.shape[i] // 2 for i in dim]\n",
    "    return roll(x, shift, dim)\n",
    "\n",
    "\n",
    "def ifftshift(x, dim=None):\n",
    "\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [(dim + 1) // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = (x.shape[dim] + 1) // 2\n",
    "    else:\n",
    "        shift = [(x.shape[i] + 1) // 2 for i in dim]\n",
    "    return roll(x, shift, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskFunc:\n",
    "\n",
    "    def __init__(self, center_fractions, accelerations):\n",
    "       \n",
    "        if len(center_fractions) != len(accelerations):\n",
    "            raise ValueError('Number of center fractions should match number of accelerations')\n",
    "\n",
    "        self.center_fractions = center_fractions\n",
    "        self.accelerations = accelerations\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, shape, seed=None):\n",
    "\n",
    "        if len(shape) < 3:\n",
    "            raise ValueError('Shape should have 3 or more dimensions')\n",
    "\n",
    "        self.rng.seed(seed)\n",
    "        num_cols = shape[-2]\n",
    "\n",
    "        choice = self.rng.randint(0, len(self.accelerations))\n",
    "        center_fraction = self.center_fractions[choice]\n",
    "        acceleration = self.accelerations[choice]\n",
    "\n",
    "        num_low_freqs = int(round(num_cols * center_fraction))\n",
    "        prob = (num_cols / acceleration - num_low_freqs) / (num_cols - num_low_freqs)\n",
    "        mask = self.rng.uniform(size=num_cols) < prob\n",
    "        pad = (num_cols - num_low_freqs + 1) // 2\n",
    "        mask[pad:pad + num_low_freqs] = True\n",
    "\n",
    "        mask_shape = [1 for _ in shape]\n",
    "        mask_shape[-2] = num_cols\n",
    "        mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None):\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "        data_list[train_and_val[i]] = [] \n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "            if not os.path.isfile(subject_data_path): continue\n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "            \n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        \n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "\n",
    "    fname, rawdata_path, slice = subject_id\n",
    "    \n",
    "    with h5py.File(rawdata_path, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = ifft2(slice_kspace), ifft2(masked_kspace) \n",
    "\n",
    "    norm = complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    img_gt, img_und, masked_kspace = img_gt/norm, img_und/norm, masked_kspace/norm \n",
    "    \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), masked_kspace.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = '/home/kevinxu/Documents/NC2019MRI/train'\n",
    "    data_path_val = '/home/kevinxu/Documents/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val)\n",
    "\n",
    "    acc = 8\n",
    "    cen_fract = 0.04 \n",
    "    seed = False \n",
    "    num_batch = 1\n",
    "    num_workers = 12\n",
    "    \n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=num_batch, num_workers=num_workers) \n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_gt, img_und, masked_kspace, masks, norm = sample\n",
    "        \n",
    "        A = masks[...,0].squeeze()\n",
    "        B = torch.log(complex_abs(masked_kspace) + 1e-9).squeeze()\n",
    "        C = center_crop(complex_abs(img_und), [320,320])\n",
    "        D = complex_abs(img_gt).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
    "            f'drop_prob={self.drop_prob})'\n",
    "\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob)\n",
    "\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob)]\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            nn.Conv2d(ch // 2, out_chans, kernel_size=1), \n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        stack = []\n",
    "        output = input\n",
    "\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UnetModel(in_chans=1, out_chans=1, chans=32, num_pool_layers=4, drop_prob=0.5).to(device)\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_count = []\n",
    "    \n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_gt, img_und, masked_kspace, masks, norm = sample\n",
    "        \n",
    "        img_und = complex_abs(img_und)\n",
    "        img_gt = complex_abs(img_gt)\n",
    "        \n",
    "        img_und = img_und.unsqueeze(0)\n",
    "        img_gt = img_gt.unsqueeze(0)\n",
    "       \n",
    "        img_und = center_crop(img_und, [320, 320])\n",
    "        img_gt = center_crop(img_gt, [320, 320])\n",
    "        \n",
    "        X = img_und\n",
    "        Y = img_gt\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        output = model(X)\n",
    "        loss = F.l1_loss(output, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_count.append(loss)\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 5 + 1):\n",
    "    train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
