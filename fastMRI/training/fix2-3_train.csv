1 0 1.244938850402832
1 100 0.7796289874705703
1 200 0.5586188308745861
1 300 0.4899327901363367
1 400 0.4663325946380372
1 500 0.47614992600986794
1 600 0.4363674262816293
1 700 0.4585282823690361
1 800 0.4194917233745795
1 900 0.42082699996014783
1 1000 0.42132062070030224
1 1100 0.4230455138896187
1 1200 0.4297106680029335
1 1300 0.41241922736419084
1 1400 0.3910413435233098
1 1500 0.40487884361205406
1 1600 0.4173287255024001
1 1700 0.3999455952425806
1 1800 0.3766656723534566
1 1900 0.40793122420927064
1 2000 0.3948077470302291
1 2100 0.38958181896087796

Train Epoch: 1 Average training loss: 0.4629 

2 0 0.19456157088279724
2 100 0.3088213159912835
2 200 0.37451631099950816
2 300 0.3907058236093774
2 400 0.4085019153049222
2 500 0.40510823834855714
2 600 0.3845876898987685
2 700 0.38205678533912135
2 800 0.35394348044790663
2 900 0.3901470097835792
2 1000 0.3962704491824579
2 1100 0.37921519732858744
2 1200 0.3757822599796596
2 1300 0.37447890958509333
2 1400 0.3920129060218418
2 1500 0.39387579324123406
2 1600 0.38799895543021823
2 1700 0.3828126375804687
2 1800 0.379153267705483
2 1900 0.41093142933090376
2 2000 0.37398075066442843
2 2100 0.37683907370499803

Train Epoch: 2 Average training loss: 0.3794 

3 0 0.18174731731414795
3 100 0.31887694041597064
3 200 0.3658447079292731
3 300 0.39026297334404336
3 400 0.3764364874797222
3 500 0.3963123673644299
3 600 0.38131992262111886
3 700 0.40949758545880843
3 800 0.3936852575442744
3 900 0.3894354128397194
3 1000 0.36589985886309645
3 1100 0.35003815078521255
3 1200 0.32164184827904746
3 1300 0.34525100108725504
3 1400 0.3493189767013956
3 1500 0.36331134432330753
3 1600 0.384778651843242
3 1700 0.38243074431156726
3 1800 0.3492300630524424
3 1900 0.37975316308699475
3 2000 0.3584129712047807
3 2100 0.366233260756504

Train Epoch: 3 Average training loss: 0.3637 

4 0 0.25834599137306213
4 100 0.3255808636132021
4 200 0.3487401438294826
4 300 0.36467432917672804
4 400 0.35094889186931455
4 500 0.35539054475922927
4 600 0.3693013594081859
4 700 0.343499633016217
4 800 0.39617597738628707
4 900 0.3711573149847499
4 1000 0.3779923000341091
4 1100 0.3695892681483681
4 1200 0.37037954786967653
4 1300 0.3568439935173649
4 1400 0.3556658034483012
4 1500 0.34855631827248323
4 1600 0.36810335540972816
4 1700 0.36821869213252145
4 1800 0.36552361822188906
4 1900 0.34152351181337637
4 2000 0.3575677363676846
4 2100 0.3490200270308596

Train Epoch: 4 Average training loss: 0.3573 

5 0 0.46565788984298706
5 100 0.3916010911671555
5 200 0.3671957272863901
5 300 0.3682272493850338
5 400 0.36227163209222146
5 500 0.34757484838778197
5 600 0.32083545470703145
5 700 0.34359728308954446
5 800 0.35079007923949607
5 900 0.36706029286305
5 1000 0.37137801284926514
5 1100 0.3672783702415207
5 1200 0.3709405378869388
5 1300 0.33701629534231853
5 1400 0.3316169727418049
5 1500 0.3523314274139677
5 1600 0.3924593515609602
5 1700 0.3700560415602834
5 1800 0.397733321683018
5 1900 0.37719896505977807
5 2000 0.35609643038075134
5 2100 0.3662593494575179

Train Epoch: 5 Average training loss: 0.3637 

6 0 0.6517624855041504
6 100 0.4855717760588764
6 200 0.40134047230507286
6 300 0.36556733432544614
6 400 0.3683308773948805
6 500 0.3639798168670304
6 600 0.36320845303961574
6 700 0.3519636283110155
6 800 0.3624239089326759
6 900 0.33262998788641
6 1000 0.33898700455605935
6 1100 0.3365736073035632
6 1200 0.3204372501727263
6 1300 0.3271945599598637
6 1400 0.3326699667944594
6 1500 0.3339720413701566
6 1600 0.34518896490710277
6 1700 0.3662736475874418
6 1800 0.3622274814237712
6 1900 0.37327165745967567
6 2000 0.3789313156970224
6 2100 0.35891004168730833

Train Epoch: 6 Average training loss: 0.3680 

7 0 0.25263312458992004
7 100 0.30592816894932234
7 200 0.31161370792056353
7 300 0.3230044807851905
7 400 0.3592571890324313
7 500 0.3654108278784046
7 600 0.33751511076583285
7 700 0.3763735543676686
7 800 0.38581947891810975
7 900 0.33653908823657874
7 1000 0.3010124463753975
7 1100 0.3487309870563844
7 1200 0.34862668295661375
7 1300 0.37146010703212623
7 1400 0.3642677405811969
7 1500 0.3469070752088111
7 1600 0.36878273915486065
7 1700 0.33275396740841334
7 1800 0.3608653438499227
7 1900 0.3436435297204012
7 2000 0.317694529618811
7 2100 0.36224742140078964

Train Epoch: 7 Average training loss: 0.3446 

8 0 0.28896594047546387
8 100 0.3264223488990735
8 200 0.3544435168999459
8 300 0.35166996111009496
8 400 0.34485620510070675
8 500 0.366275751256522
8 600 0.3555582532707481
8 700 0.34010342580428077
8 800 0.3427257824383755
8 900 0.3463189811778875
8 1000 0.3514420340695667
8 1100 0.3293684769554993
8 1200 0.3619756308456884
8 1300 0.36172203401087005
8 1400 0.35380319861206705
8 1500 0.343630789601707
8 1600 0.348232751455407
8 1700 0.3406767557800055
8 1800 0.3501513272787963
8 1900 0.3490376790062206
8 2000 0.33141268732883344
8 2100 0.3458831979583881

Train Epoch: 8 Average training loss: 0.3472 

9 0 0.28446319699287415
9 100 0.33964922556674204
9 200 0.3396719141122829
9 300 0.3294307260686131
9 400 0.3505000050319002
9 500 0.35440452140393874
9 600 0.35085969216851687
9 700 0.34589975234740356
9 800 0.3312621627410918
9 900 0.3383917060305983
9 1000 0.3371361104179383
9 1100 0.3351720263126795
9 1200 0.3324211499992571
9 1300 0.3539005513064716
9 1400 0.3421896879314088
9 1500 0.3490429387221141
9 1600 0.34677713912262986
9 1700 0.3375110885374515
9 1800 0.3457794607388401
9 1900 0.34780562647433994
9 2000 0.3387956481789249
9 2100 0.3314529572528088

Train Epoch: 9 Average training loss: 0.3440 

10 0 0.2869797945022583
10 100 0.30563145700971617
10 200 0.3047790375623033
10 300 0.3244019977289657
10 400 0.35249249797799914
10 500 0.3252463299506153
10 600 0.3623953215262172
10 700 0.33765786153412236
10 800 0.3678226185550498
10 900 0.3672422901273623
10 1000 0.33490935226957
10 1100 0.338473744417348
10 1200 0.33136250435805836
10 1300 0.3568656078282303
10 1400 0.3291899992561805
10 1500 0.33340985792334177
10 1600 0.35627447852613026
10 1700 0.3935191743791881
10 1800 0.34115684851811867
10 1900 0.35189594020646914
10 2000 0.3735704450735699
10 2100 0.3334566058143348

Train Epoch: 10 Average training loss: 0.3428 

11 0 0.310581773519516
11 100 0.3337373393296489
11 200 0.33810324741222736
11 300 0.3314547245569272
11 400 0.33147384714357353
11 500 0.3343661417019411
11 600 0.3393404637629049
11 700 0.3396597395335561
11 800 0.33380302206862705
11 900 0.3533298959970937
11 1000 0.3443635998028417
11 1100 0.3450726328490919
11 1200 0.34706943921776184
11 1300 0.34436453916743176
11 1400 0.3499653857420582
11 1500 0.3257888458832836
11 1600 0.3743867064947921
11 1700 0.3672322192322494
11 1800 0.333929005102003
11 1900 0.3356561665158391
11 2000 0.35448538271517877
11 2100 0.3461972552502165

Train Epoch: 11 Average training loss: 0.3442 

12 0 0.1162315160036087
12 100 0.28398014770258545
12 200 0.33519636278653775
12 300 0.32957675810773196
12 400 0.337192109252948
12 500 0.3468904949181817
12 600 0.3350091482184073
12 700 0.3241836770423613
12 800 0.3131527456462606
12 900 0.3318520106797549
12 1000 0.3134853312986759
12 1100 0.31149412046277236
12 1200 0.3315795763012638
12 1300 0.3485767933362901
12 1400 0.3529887315556265
12 1500 0.33117904931440895
12 1600 0.34895584333565083
12 1700 0.3518378097752508
12 1800 0.35248532507484054
12 1900 0.3423493923840235
12 2000 0.33945905193458165
12 2100 0.34337105816042385

Train Epoch: 12 Average training loss: 0.3292 

13 0 0.2826288938522339
13 100 0.31398172831447
13 200 0.33762811481408816
13 300 0.32635697520082
13 400 0.3249470733808421
13 500 0.328325366455618
13 600 0.36276133010465356
13 700 0.3453629542650982
13 800 0.34076908258697236
13 900 0.34821486040791666
13 1000 0.3295897020763525
13 1100 0.36591349493266445
13 1200 0.36745031297773295
13 1300 0.3587098138494797
13 1400 0.366279446664623
13 1500 0.3631632356067283
13 1600 0.31932875905679925
13 1700 0.33460021426568576
13 1800 0.34132924323464736
13 1900 0.3427171575915572
13 2000 0.3353009232752008
13 2100 0.3266518233852203

Train Epoch: 13 Average training loss: 0.3411 

14 0 0.14848169684410095
14 100 0.2518878492589119
14 200 0.3336669271160343
14 300 0.31689842003064367
14 400 0.32746338892734067
14 500 0.33210356516572354
14 600 0.32454949804383226
14 700 0.3341261031993472
14 800 0.35190714329231404
14 900 0.3464771486055809
14 1000 0.33992235150753614
14 1100 0.3457680717484341
14 1200 0.33315051070388424
14 1300 0.3401739553727383
14 1400 0.34049314288903526
14 1500 0.3299691795970588
14 1600 0.35429646496218964
14 1700 0.3585074651557669
14 1800 0.3318881495552589
14 1900 0.34193400336703655
14 2000 0.350750763433429
14 2100 0.33855259865760795

Train Epoch: 14 Average training loss: 0.3333 

15 0 0.28682368993759155
15 100 0.36294694562967345
15 200 0.33488020248710565
15 300 0.32965222541928996
15 400 0.35398824997371653
15 500 0.34139332885840623
15 600 0.352873581943077
15 700 0.3302404540816066
15 800 0.3357987431039343
15 900 0.34679605378304096
15 1000 0.3259579518524051
15 1100 0.34429256171928074
15 1200 0.3538229256890871
15 1300 0.35571902497306745
15 1400 0.3381272738257935
15 1500 0.3144911827659483
15 1600 0.3365337432870027
15 1700 0.3474680586983931
15 1800 0.34959399137042674
15 1900 0.32528248169072577
15 2000 0.3368582143519892
15 2100 0.33463763576850425

Train Epoch: 15 Average training loss: 0.3380 

16 0 0.24259719252586365
16 100 0.29245503348162183
16 200 0.3310182765935826
16 300 0.31751220947101294
16 400 0.33182536314944255
16 500 0.3499086814036645
16 600 0.3357769801900412
16 700 0.33144990798105584
16 800 0.35533839647842286
16 900 0.35667320093318644
16 1000 0.3350520740325998
16 1100 0.3519618368691183
16 1200 0.34670941918074466
16 1300 0.35708922034601426
16 1400 0.33638610764077703
16 1500 0.33986250770383075
16 1600 0.36071889642031485
16 1700 0.34907503407224466
16 1800 0.33815076382543374
16 1900 0.3390951018499162
16 2000 0.33491630674411327
16 2100 0.30742879258136463

Train Epoch: 16 Average training loss: 0.3360 

17 0 0.21198660135269165
17 100 0.29737530133191475
17 200 0.3335676461275382
17 300 0.33974245614078497
17 400 0.32471755261817214
17 500 0.3348514250620043
17 600 0.3356489277413829
17 700 0.33457065244069417
17 800 0.33908098682411986
17 900 0.3138090622476118
17 1000 0.3355647112629746
17 1100 0.3172394944347822
17 1200 0.33775846400105575
17 1300 0.3427741303260794
17 1400 0.34259026390750846
17 1500 0.35892591834773446
17 1600 0.3307735961932444
17 1700 0.33255679077062683
17 1800 0.3339875052951085
17 1900 0.34213022982596963
17 2000 0.336829035938808
17 2100 0.3547559913897526

Train Epoch: 17 Average training loss: 0.3327 

18 0 0.5632701516151428
18 100 0.4680537907995413
18 200 0.3762719484300259
18 300 0.3598144940367128
18 400 0.34012302866723454
18 500 0.3422601750053553
18 600 0.34674928668620353
18 700 0.3385969133703909
18 800 0.3175945119118938
18 900 0.3294918412573847
18 1000 0.3325611071741614
18 1100 0.32224643740727554
18 1200 0.3145269082949239
18 1300 0.33112135135912835
18 1400 0.328835490698551
18 1500 0.3208928767644986
18 1600 0.32500118658145055
18 1700 0.33128049019419004
18 1800 0.33435246684170894
18 1900 0.33185788622193235
18 2000 0.32487953735347885
18 2100 0.3334473065597929

Train Epoch: 18 Average training loss: 0.3470 

19 0 0.1626797467470169
19 100 0.26862893176429475
19 200 0.341908856602245
19 300 0.3215453785153626
19 400 0.3094779865583072
19 500 0.3156027906392491
19 600 0.3417363882065338
19 700 0.33078222077364444
19 800 0.3472423761943011
19 900 0.3219293310361672
19 1000 0.35439770318544717
19 1100 0.3225341315984358
19 1200 0.3189103769386278
19 1300 0.33027632721775424
19 1400 0.3440587087150988
19 1500 0.33686317390166653
19 1600 0.3477865409991852
19 1700 0.3656923865153545
19 1800 0.363399494525841
19 1900 0.3423079048623074
19 2000 0.3223069851286691
19 2100 0.3328849132300129

Train Epoch: 19 Average training loss: 0.3277 

20 0 0.2360331416130066
20 100 0.2829294009374988
20 200 0.3070188591281628
20 300 0.32714563927747853
20 400 0.33336515916192055
20 500 0.34173515277705374
20 600 0.3396303155139801
20 700 0.33118379323484576
20 800 0.3388639312025804
20 900 0.3434900653187522
20 1000 0.3305515450064782
20 1100 0.3115642062214281
20 1200 0.33857305618307154
20 1300 0.3451115497278112
20 1400 0.3649807724932862
20 1500 0.3541192304121408
20 1600 0.33200054605571133
20 1700 0.329775316783465
20 1800 0.32354447861134744
20 1900 0.3668613731901148
20 2000 0.3562315117602498
20 2100 0.33130552803109103

Train Epoch: 20 Average training loss: 0.3318 

21 0 0.30915093421936035
21 100 0.3236836748015991
21 200 0.3431058131878441
21 300 0.32713854478565424
21 400 0.3335953440170945
21 500 0.342387943033595
21 600 0.32529846381956684
21 700 0.34277723774098584
21 800 0.3286420686577443
21 900 0.3442620708929428
21 1000 0.32702673332226123
21 1100 0.3300683516718387
21 1200 0.35081243468059364
21 1300 0.3487775051208771
21 1400 0.34124269542272506
21 1500 0.3504459061131789
21 1600 0.32125753861805373
21 1700 0.31768919514512867
21 1800 0.34637702506979784
21 1900 0.33146876601679653
21 2000 0.31431881938102835
21 2100 0.3201121225258397

Train Epoch: 21 Average training loss: 0.3343 

22 0 0.12631069123744965
22 100 0.24490295339709114
22 200 0.29700937768771196
22 300 0.31444157874854795
22 400 0.32174042221289734
22 500 0.32664526053408777
22 600 0.3452899042681985
22 700 0.3425394333587241
22 800 0.323374981812967
22 900 0.3371682880178315
22 1000 0.3259924006000851
22 1100 0.31974898491331205
22 1200 0.326506250343049
22 1300 0.3258202845641645
22 1400 0.3384148686316308
22 1500 0.33155769125701773
22 1600 0.3420706638899585
22 1700 0.3351981160839872
22 1800 0.36178803725728886
22 1900 0.3674716582469033
22 2000 0.34131725065468943
22 2100 0.31752098990349237

Train Epoch: 22 Average training loss: 0.3246 

23 0 0.3130606710910797
23 100 0.3308354266473763
23 200 0.34354358224733544
23 300 0.34043629783253876
23 400 0.31045759781665916
23 500 0.32947867470579467
23 600 0.3300728206909884
23 700 0.3230440891047174
23 800 0.35161690116379274
23 900 0.3300361176425411
23 1000 0.3266321413199495
23 1100 0.32708630310815984
23 1200 0.31213068860165344
23 1300 0.32543928792895205
23 1400 0.32888644402928896
23 1500 0.3381382608294043
23 1600 0.3398319099509016
23 1700 0.31923886423090636
23 1800 0.32425908563873085
23 1900 0.3263389304315636
23 2000 0.339828501768379
23 2100 0.3440502142639447

Train Epoch: 23 Average training loss: 0.3312 

24 0 0.3054715096950531
24 100 0.3438253383626605
24 200 0.3645495223185194
24 300 0.34983520108914423
24 400 0.33911827967522024
24 500 0.30457506128242084
24 600 0.33235821305088714
24 700 0.3200235488206101
24 800 0.31809015074540786
24 900 0.3179504136649616
24 1000 0.34700672495580404
24 1100 0.35132910475242896
24 1200 0.33436483389278626
24 1300 0.3277803869307956
24 1400 0.3239085658165649
24 1500 0.32575652818818573
24 1600 0.3020627293905917
24 1700 0.32975292395056227
24 1800 0.3377343142915172
24 1900 0.3299399623605071
24 2000 0.3363503026757321
24 2100 0.32332599650419325

Train Epoch: 24 Average training loss: 0.3327 

25 0 0.30974718928337097
25 100 0.3241388501676711
25 200 0.3411221854954016
25 300 0.35281337878826496
25 400 0.3241111705103287
25 500 0.33667494094753464
25 600 0.3318610941041434
25 700 0.3359005826448592
25 800 0.3285840641322738
25 900 0.33288139899965763
25 1000 0.32083410195173606
25 1100 0.3004488367393058
25 1200 0.3168344522425004
25 1300 0.3472533612292734
25 1400 0.34554560658637473
25 1500 0.34702290433414507
25 1600 0.33964629101817145
25 1700 0.3311192199860261
25 1800 0.32466226615994553
25 1900 0.3290627469760401
25 2000 0.3321212700424441
25 2100 0.30690590808668844

Train Epoch: 25 Average training loss: 0.3310 

26 0 0.45992156863212585
26 100 0.3594797233101488
26 200 0.3558282648876857
26 300 0.3483983264677675
26 400 0.3372458444848075
26 500 0.2991176839246495
26 600 0.3163544300746587
26 700 0.3464044256278814
26 800 0.3333034931634991
26 900 0.3225947885866521
26 1000 0.3385556591224027
26 1100 0.3148449869526351
26 1200 0.3115104826539413
26 1300 0.3289494252272057
26 1400 0.3107678211531787
26 1500 0.3304819707205089
26 1600 0.34406084500637857
26 1700 0.3541895453477809
26 1800 0.3368583542148618
26 1900 0.3325843914036563
26 2000 0.33574697420087
26 2100 0.34363639106197896

Train Epoch: 26 Average training loss: 0.3378 

27 0 0.22751274704933167
27 100 0.2911871895951281
27 200 0.302845556230022
27 300 0.3194351997454543
27 400 0.30962101068372366
27 500 0.3301096467220748
27 600 0.31652212640572175
27 700 0.3141137180413659
27 800 0.34078124509946295
27 900 0.32441449571856157
27 1000 0.3563830979588996
27 1100 0.32460496087092183
27 1200 0.36058558316526645
27 1300 0.3453875488729239
27 1400 0.33359766395164303
27 1500 0.3033629684472714
27 1600 0.3252360648597011
27 1700 0.33984467256309425
27 1800 0.34291272879389123
27 1900 0.339124315960767
27 2000 0.3359112158202628
27 2100 0.33282177872199864

Train Epoch: 27 Average training loss: 0.3268 

28 0 0.2534182667732239
28 100 0.29837340387442074
28 200 0.330930049900778
28 300 0.32423164114981906
28 400 0.3094153445208992
28 500 0.31570456176950296
28 600 0.3319119977750716
28 700 0.34226346776460354
28 800 0.3223030425899922
28 900 0.35198326207490793
28 1000 0.3134948213791626
28 1100 0.3184605182234177
28 1200 0.3070816351445189
28 1300 0.33764071899183185
28 1400 0.3441145692645651
28 1500 0.3467006713849919
28 1600 0.3309791428091787
28 1700 0.3452508721461489
28 1800 0.3180620024962187
28 1900 0.32794491223305855
28 2000 0.33048138926311654
28 2100 0.34008452644920206

Train Epoch: 28 Average training loss: 0.3278 

29 0 0.08280006051063538
29 100 0.24244667439594017
29 200 0.2900054347058673
29 300 0.3218528200965761
29 400 0.3177708264853514
29 500 0.2971715451945341
29 600 0.30586917592732454
29 700 0.35065574692662577
29 800 0.3154586527591009
29 900 0.3386438226732159
29 1000 0.33603027987872297
29 1100 0.34100942179023597
29 1200 0.3423010822288348
29 1300 0.32770959050424586
29 1400 0.33230189612478295
29 1500 0.3488869384992351
29 1600 0.3315225374865774
29 1700 0.35833073268862026
29 1800 0.3521443629468475
29 1900 0.3069525811778401
29 2000 0.34065406433683243
29 2100 0.3457733692005598

Train Epoch: 29 Average training loss: 0.3192 

30 0 0.2029089778661728
30 100 0.2898762294825092
30 200 0.30325841526099917
30 300 0.32511802947443047
30 400 0.33012078562420477
30 500 0.31890874142892256
30 600 0.32309571392740577
30 700 0.31508290496879665
30 800 0.3328660858635877
30 900 0.3487043306112584
30 1000 0.355181130746528
30 1100 0.3280730372433628
30 1200 0.3454232981882883
30 1300 0.34548474703681503
30 1400 0.30442607376090897
30 1500 0.33100588577941514
30 1600 0.3397703832900989
30 1700 0.3205107067801973
30 1800 0.342214275996045
30 1900 0.34177654222974224
30 2000 0.3221699310868339
30 2100 0.32044179623442226

Train Epoch: 30 Average training loss: 0.3252 

31 0 0.16845323145389557
31 100 0.2683624878281272
31 200 0.30676777083035944
31 300 0.3551950584419634
31 400 0.3441779012270064
31 500 0.33517375151100687
31 600 0.34261548135022374
31 700 0.34303121610225623
31 800 0.34670790754816366
31 900 0.3557878580147226
31 1000 0.34552199716455556
31 1100 0.32146906021270044
31 1200 0.3512217390712679
31 1300 0.327283676135278
31 1400 0.3212524178769593
31 1500 0.3326283845018795
31 1600 0.3254656651887196
31 1700 0.3231526617819713
31 1800 0.3431139045628408
31 1900 0.3252867967106066
31 2000 0.29680290077085614
31 2100 0.28685454462373977

Train Epoch: 31 Average training loss: 0.3245 

32 0 0.38137614727020264
32 100 0.35464946382782553
32 200 0.34349405065717936
32 300 0.34314606287264643
32 400 0.3270405770407251
32 500 0.33955243436175414
32 600 0.3393348677893054
32 700 0.34565810312010153
32 800 0.3168146186134297
32 900 0.3284360301392478
32 1000 0.3186444415001586
32 1100 0.3308193607242458
32 1200 0.3133842705815876
32 1300 0.3472212435053497
32 1400 0.337630039926382
32 1500 0.34158108426584505
32 1600 0.30992732255042527
32 1700 0.31396207796630315
32 1800 0.3240813090068428
32 1900 0.31171306772801477
32 2000 0.3372485040752381
32 2100 0.3431915735755178

Train Epoch: 32 Average training loss: 0.3325 

33 0 0.2739471197128296
33 100 0.3248511200326909
33 200 0.3566942947464672
33 300 0.31853695029029483
33 400 0.3091618566423723
33 500 0.3128968571745881
33 600 0.3329317767798251
33 700 0.3370341469312687
33 800 0.31735292902317613
33 900 0.346289507255161
33 1000 0.33963272567418906
33 1100 0.3372909539431912
33 1200 0.3382470464614555
33 1300 0.34594307680703784
33 1400 0.339055680785055
33 1500 0.34781438574321005
33 1600 0.3330143181183873
33 1700 0.35476584916236276
33 1800 0.340947947507411
33 1900 0.3369208355778183
33 2000 0.33389396346021005
33 2100 0.2961036712021011

Train Epoch: 33 Average training loss: 0.3286 

34 0 0.2071133404970169
34 100 0.27308746142467893
34 200 0.30731965210479606
34 300 0.33692692971534804
34 400 0.3380063753531727
34 500 0.339401740286958
34 600 0.33149607399290665
34 700 0.32014487615204335
34 800 0.32479639704008484
34 900 0.3411776587369299
34 1000 0.3138727364640173
34 1100 0.3165849950120168
34 1200 0.3270920164957612
34 1300 0.3226381190108512
34 1400 0.31342333736781686
34 1500 0.3290881332907812
34 1600 0.31420392859731644
34 1700 0.33059000659237525
34 1800 0.3411927455914467
34 1900 0.3444236078091707
34 2000 0.31369105156571697
34 2100 0.3406377703849888

Train Epoch: 34 Average training loss: 0.3228 

35 0 0.09443026781082153
35 100 0.23113265780259049
35 200 0.30420212454054163
35 300 0.32008933747198
35 400 0.3291023385638406
35 500 0.3425470051829808
35 600 0.35064156646927935
35 700 0.3181284211748514
35 800 0.3452511816265126
35 900 0.33529759938672365
35 1000 0.31245657403636257
35 1100 0.3180097507528479
35 1200 0.31014854854131124
35 1300 0.30636011418823317
35 1400 0.3091043400502475
35 1500 0.31085747250292434
35 1600 0.3012855652226981
35 1700 0.3318121576276226
35 1800 0.3441544056694328
35 1900 0.33958775840653466
35 2000 0.31682485727388815
35 2100 0.33948487027351537

Train Epoch: 35 Average training loss: 0.3158 

36 0 0.27337563037872314
36 100 0.3041438446195488
36 200 0.2927903806677944
36 300 0.3211002449565821
36 400 0.31578297324606996
36 500 0.325097372572816
36 600 0.3486168650220195
36 700 0.32985579188840786
36 800 0.329002794393433
36 900 0.32057727998702956
36 1000 0.31802002772861254
36 1100 0.3205099686419662
36 1200 0.33374563044927774
36 1300 0.33799708732549566
36 1400 0.3137856804609018
36 1500 0.319662446817251
36 1600 0.3258401830684323
36 1700 0.3556862170948354
36 1800 0.34903394668434473
36 1900 0.31689236167439344
36 2000 0.3368464445020145
36 2100 0.3296450909021728

Train Epoch: 36 Average training loss: 0.3248 

37 0 0.5491815209388733
37 100 0.4108226071782498
37 200 0.3713345673418594
37 300 0.3346811055069943
37 400 0.3390093704837125
37 500 0.3162204401779563
37 600 0.322996911821421
37 700 0.3150370596010218
37 800 0.3206695985600322
37 900 0.33456582754822184
37 1000 0.32026425537567016
37 1100 0.30384437165027645
37 1200 0.31321395635103527
37 1300 0.3190082216023552
37 1400 0.2942156509832813
37 1500 0.310868628583175
37 1600 0.3220502578296182
37 1700 0.326696225453242
37 1800 0.3226598588150782
37 1900 0.3117840413046636
37 2000 0.33650763325158883
37 2100 0.3292157946717275

Train Epoch: 37 Average training loss: 0.3339 

38 0 0.16508710384368896
38 100 0.25040921364894947
38 200 0.29738131090979353
38 300 0.3039874345242212
38 400 0.3045400516495645
38 500 0.3216440454990203
38 600 0.3194288127871394
38 700 0.34429962985031515
38 800 0.3272594782897891
38 900 0.31092415330480416
38 1000 0.30502292791430025
38 1100 0.3352662898139154
38 1200 0.3438443380279503
38 1300 0.3500329966966634
38 1400 0.3276028447281905
38 1500 0.31372651973439397
38 1600 0.3241260426702926
38 1700 0.3597166678540925
38 1800 0.33290673142818045
38 1900 0.3239893951674262
38 2000 0.34330892944558455
38 2100 0.32806512510893815

Train Epoch: 38 Average training loss: 0.3195 

39 0 0.36134621500968933
39 100 0.33362534809621724
39 200 0.3152702625057812
39 300 0.31773470921077634
39 400 0.3456184751514683
39 500 0.33617589543355836
39 600 0.3238374843209104
39 700 0.33207906890303773
39 800 0.3282067825191472
39 900 0.3152703142497832
39 1000 0.336500250522383
39 1100 0.34251756984358633
39 1200 0.3219400868416278
39 1300 0.3333540545569802
39 1400 0.3138341140300556
39 1500 0.327990452570618
39 1600 0.30650035425106587
39 1700 0.33014678073486
39 1800 0.333238713522569
39 1900 0.350552753730325
39 2000 0.3136857242056843
39 2100 0.29937521675443624

Train Epoch: 39 Average training loss: 0.3285 

40 0 0.48825377225875854
40 100 0.4056960455074575
40 200 0.3483838654322862
40 300 0.3319616162435408
40 400 0.3256875592905823
40 500 0.33230101250064253
40 600 0.3171544887684064
40 700 0.3415120346188854
40 800 0.31751703074491944
40 900 0.32860814616506073
40 1000 0.32100508394105554
40 1100 0.3146225782099973
40 1200 0.33485353911441273
40 1300 0.3581751762457667
40 1400 0.3658095414124941
40 1500 0.3239766696724552
40 1600 0.3347513388506825
40 1700 0.3144770948186169
40 1800 0.3208044011335126
40 1900 0.3265765593483044
40 2000 0.31963618807666433
40 2100 0.32626467318035657

Train Epoch: 40 Average training loss: 0.3360 

41 0 0.5518651008605957
41 100 0.41190673122611754
41 200 0.3685148102624611
41 300 0.36206901619879134
41 400 0.33463895057031356
41 500 0.3329399058482035
41 600 0.3310142769201464
41 700 0.3292493999309815
41 800 0.3370845145456008
41 900 0.32809783159485584
41 1000 0.34666457302940384
41 1100 0.32763741909395017
41 1200 0.3271911475214655
41 1300 0.3325645721558972
41 1400 0.32531806227623444
41 1500 0.29283130709029
41 1600 0.31445374282709615
41 1700 0.31835251735247055
41 1800 0.32889449314771474
41 1900 0.32540811601754926
41 2000 0.29285332176516937
41 2100 0.33425412679548944

Train Epoch: 41 Average training loss: 0.3370 

42 0 0.23450767993927002
42 100 0.3088308189392654
42 200 0.3124701353156029
42 300 0.33638036005828315
42 400 0.31520930581270534
42 500 0.3149502696319141
42 600 0.322573599033552
42 700 0.316057170248216
42 800 0.3276909454111225
42 900 0.35438681281395423
42 1000 0.3654020690908018
42 1100 0.3219246090207392
42 1200 0.2936173699248863
42 1300 0.3234782285292165
42 1400 0.3259472934156274
42 1500 0.3518311697317078
42 1600 0.3208713988268165
42 1700 0.3294009791307115
42 1800 0.3148855421606575
42 1900 0.31929116428488624
42 2000 0.33099168293106257
42 2100 0.3327029208939615

Train Epoch: 42 Average training loss: 0.3221 

43 0 0.24967049062252045
43 100 0.27256506815554765
43 200 0.31528189007028634
43 300 0.33399122325030245
43 400 0.3079743026277548
43 500 0.33524247145906566
43 600 0.31549078107448775
43 700 0.34422401474353553
43 800 0.33679482219710954
43 900 0.3227842475634044
43 1000 0.33713363716430095
43 1100 0.31097071499724416
43 1200 0.3137559961790931
43 1300 0.2831436575237233
43 1400 0.3147705373470426
43 1500 0.3401259381737566
43 1600 0.34967517658206393
43 1700 0.3301707485489171
43 1800 0.32034255340260015
43 1900 0.32317640536095144
43 2000 0.33111293484587095
43 2100 0.3270959007277311

Train Epoch: 43 Average training loss: 0.3216 

44 0 0.1356203258037567
44 100 0.26334557651749146
44 200 0.3051322687340604
44 300 0.30761808942593943
44 400 0.3186542148899239
44 500 0.3412881952329417
44 600 0.34025023918093694
44 700 0.3557278726648445
44 800 0.33097561642855344
44 900 0.317352570130473
44 1000 0.3249784648540852
44 1100 0.2988169826330927
44 1200 0.30213404567301066
44 1300 0.3164759400032462
44 1400 0.31724569598523283
44 1500 0.3111402681917372
44 1600 0.32301369672778624
44 1700 0.3147203410228961
44 1800 0.30149160892017896
44 1900 0.32315778069309
44 2000 0.35743473414489596
44 2100 0.3443668031004007

Train Epoch: 44 Average training loss: 0.3157 

45 0 0.46649834513664246
45 100 0.38857175045714615
45 200 0.363473247317161
45 300 0.33830824764934053
45 400 0.339078127222901
45 500 0.3067196355962565
45 600 0.332867755868593
45 700 0.3311639012575136
45 800 0.30900354975069483
45 900 0.33180078676986263
45 1000 0.3215615706555925
45 1100 0.32375137909318596
45 1200 0.32664035956283183
45 1300 0.335531242693593
45 1400 0.3255198765836997
45 1500 0.35044415032450676
45 1600 0.32659347968684166
45 1700 0.3182545468892044
45 1800 0.3270415688835132
45 1900 0.31447691406021977
45 2000 0.3175208258157827
45 2100 0.32462444618796643

Train Epoch: 45 Average training loss: 0.3312 

46 0 0.9559754729270935
46 100 0.5529594210516882
46 200 0.413629420932342
46 300 0.3542097964147492
46 400 0.3373447155651602
46 500 0.33506227265821314
46 600 0.3351212122998348
46 700 0.3310515597199993
46 800 0.326336817647145
46 900 0.3109692772976438
46 1000 0.3364090408105048
46 1100 0.32625554791754163
46 1200 0.3028888353215026
46 1300 0.3385777034420996
46 1400 0.3283033114884216
46 1500 0.3028927216947387
46 1600 0.30195337875268996
46 1700 0.35382756912170904
46 1800 0.3327204614446732
46 1900 0.32286942856023293
46 2000 0.3169042737233251
46 2100 0.30980083509220724

Train Epoch: 46 Average training loss: 0.3549 

47 0 0.2592560648918152
47 100 0.29575438727534825
47 200 0.2905752756707517
47 300 0.3164262797607443
47 400 0.30561090496933924
47 500 0.3257758615468601
47 600 0.31832256835125067
47 700 0.32138028205578356
47 800 0.32664285262683285
47 900 0.32892682237782
47 1000 0.3368152048651101
47 1100 0.32476439565444454
47 1200 0.31460097373624657
47 1300 0.31881433158398526
47 1400 0.3537038384063437
47 1500 0.33932127603224044
47 1600 0.3478383845075682
47 1700 0.35318133484200154
47 1800 0.3486635294233701
47 1900 0.32562087505974396
47 2000 0.32318429563560536
47 2100 0.31684441280207976

Train Epoch: 47 Average training loss: 0.3221 

48 0 0.1756839156150818
48 100 0.2637231584482803
48 200 0.3104250687205677
48 300 0.2896968595750925
48 400 0.33709392092878854
48 500 0.3192155232923929
48 600 0.31434559408942464
48 700 0.33032430658040673
48 800 0.30741551622422836
48 900 0.3074358361805071
48 1000 0.3182231362777521
48 1100 0.2918668371782059
48 1200 0.33125702386107503
48 1300 0.35796301346954323
48 1400 0.3232572634960716
48 1500 0.3078626238377503
48 1600 0.3192678787390463
48 1700 0.3529985555202352
48 1800 0.35688033949860143
48 1900 0.34672613342593395
48 2000 0.31225433210950543
48 2100 0.31319309351995983

Train Epoch: 48 Average training loss: 0.3176 

49 0 0.19994649291038513
49 100 0.2700434038593405
49 200 0.30867996327972774
49 300 0.31284818856167046
49 400 0.31569012762526255
49 500 0.33942420320905337
49 600 0.33501700276103674
49 700 0.3321403715230261
49 800 0.32998212522942166
49 900 0.3171320643315007
49 1000 0.30546714187036095
49 1100 0.3216913764970538
49 1200 0.32302569607413983
49 1300 0.3367869419901098
49 1400 0.319133019371344
49 1500 0.33576472160431053
49 1600 0.3258141635310584
49 1700 0.33688000834600496
49 1800 0.3388070694518774
49 1900 0.32063928540813774
49 2000 0.2929122597437297
49 2100 0.3267135972203551

Train Epoch: 49 Average training loss: 0.3163 

50 0 0.10585273802280426
50 100 0.22431732780717586
50 200 0.28764804932715243
50 300 0.332348041807349
50 400 0.36097016142381694
50 500 0.3440292730681613
50 600 0.3126563099565329
50 700 0.3109312673182853
50 800 0.3300669687750057
50 900 0.3514541540198367
50 1000 0.3225314514524854
50 1100 0.33362323054492815
50 1200 0.29974159857634863
50 1300 0.31269357837080974
50 1400 0.30135864919546246
50 1500 0.2909744458292432
50 1600 0.3310936347707111
50 1700 0.31549083971214564
50 1800 0.327351802956881
50 1900 0.33531512353195153
50 2000 0.3145366131976204
50 2100 0.32046424722750105

Train Epoch: 50 Average training loss: 0.3133 

