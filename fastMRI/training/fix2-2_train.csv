1 0 2.5042600631713867
1 100 1.2913268910122695
1 200 0.7876018876152546
1 300 0.5988106317514844
1 400 0.49988989419510105
1 500 0.4649861784931045
1 600 0.4354667404874671
1 700 0.4491433247306581
1 800 0.4397433237189973
1 900 0.4147371126839228
1 1000 0.42758914566227346
1 1100 0.4086340940815445
1 1200 0.39430041972905416
1 1300 0.39060960257428484
1 1400 0.3737932294227875
1 1500 0.4051148397570292
1 1600 0.3878905427688449
1 1700 0.39030362145026143
1 1800 0.3684197807598205
1 1900 0.40852350829222406
1 2000 0.4277145223733789
1 2100 0.40928387699674557

Train Epoch: 1 Average training loss: 0.5270 

2 0 0.4744091331958771
2 100 0.4436664528667078
2 200 0.4074629172148377
2 300 0.37206046013003746
2 400 0.3827606812108718
2 500 0.38786834415784466
2 600 0.39909634523022913
2 700 0.3851631066460288
2 800 0.4070125680003951
2 900 0.40795634669526
2 1000 0.3777022411751303
2 1100 0.4094922783396502
2 1200 0.37522300663054775
2 1300 0.3911613042492017
2 1400 0.4161480765402986
2 1500 0.3968966606264855
2 1600 0.38954804678083
2 1700 0.4031381096927847
2 1800 0.38829996922811405
2 1900 0.3640312310670655
2 2000 0.3731700381938226
2 2100 0.3767116743757808

Train Epoch: 2 Average training loss: 0.3931 

3 0 0.17700400948524475
3 100 0.289086831203961
3 200 0.34128799508185764
3 300 0.36091215843842894
3 400 0.37197087549287455
3 500 0.35635573844119117
3 600 0.37007989633078764
3 700 0.3707124872729536
3 800 0.3579368450813103
3 900 0.36889179980482845
3 1000 0.3841202531344468
3 1100 0.397080218998386
3 1200 0.38120084867514376
3 1300 0.35726470476327765
3 1400 0.3666114726972528
3 1500 0.3782552899271369
3 1600 0.4035210133322997
3 1700 0.3982131991712241
3 1800 0.3984829076763812
3 1900 0.38538221577037657
3 2000 0.37077515521229254
3 2100 0.3777233572734495

Train Epoch: 3 Average training loss: 0.3662 

4 0 0.26533687114715576
4 100 0.3056838703386145
4 200 0.34554265513948856
4 300 0.3604537568771915
4 400 0.36510837962067344
4 500 0.3449038926512384
4 600 0.3731630307358063
4 700 0.3534778278063309
4 800 0.33873365718258663
4 900 0.41032986023856305
4 1000 0.3552068152689228
4 1100 0.37161093132986633
4 1200 0.3756540541031317
4 1300 0.35155578300805623
4 1400 0.36160921612768143
4 1500 0.37577705452955884
4 1600 0.3578864572388446
4 1700 0.36444929880968807
4 1800 0.3785791131998899
4 1900 0.3582646853688767
4 2000 0.3621298904421298
4 2100 0.3770088148421719

Train Epoch: 4 Average training loss: 0.3593 

5 0 0.9373510479927063
5 100 0.6018221522742516
5 200 0.4557462698101365
5 300 0.38053365140848694
5 400 0.3758293408395774
5 500 0.38626042654645376
5 600 0.37706113720602147
5 700 0.3533194731610003
5 800 0.35783731098128285
5 900 0.37793555114945715
5 1000 0.35869906658306105
5 1100 0.3358799222845553
5 1200 0.36215420255297887
5 1300 0.3390935325607018
5 1400 0.3630411580436613
5 1500 0.34725070050586065
5 1600 0.3226954800402721
5 1700 0.34445200794638303
5 1800 0.3328646533486139
5 1900 0.3394158849194544
5 2000 0.3571594341758793
5 2100 0.34053927463668154

Train Epoch: 5 Average training loss: 0.3841 

6 0 0.5293455719947815
6 100 0.4352586855776936
6 200 0.3688976151454193
6 300 0.38270159904070383
6 400 0.3573880285406537
6 500 0.336693833585983
6 600 0.3307214875916293
6 700 0.36928561860976655
6 800 0.3632761022763682
6 900 0.34842069097836326
6 1000 0.3573859820501173
6 1100 0.329596854916651
6 1200 0.3313413880088669
6 1300 0.3238090390744198
6 1400 0.33181644773454305
6 1500 0.3505686281975744
6 1600 0.34625527946122686
6 1700 0.3392669289459209
6 1800 0.3420711091797393
6 1900 0.3414166030653995
6 2000 0.3421186129546734
6 2100 0.3527388983689755

Train Epoch: 6 Average training loss: 0.3573 

7 0 0.1533532291650772
7 100 0.26015553036909483
7 200 0.337216623787701
7 300 0.3388005393161087
7 400 0.3203161152757263
7 500 0.38293298333707665
7 600 0.34454858846267894
7 700 0.33839537252689433
7 800 0.3321214331374863
7 900 0.3454844501825558
7 1000 0.35830388881966935
7 1100 0.34634235150115483
7 1200 0.35766605504477983
7 1300 0.3590464800834966
7 1400 0.3601711562458043
7 1500 0.34484369204614135
7 1600 0.3316317342821853
7 1700 0.334842468372892
7 1800 0.36058173157481144
7 1900 0.3835812075944026
7 2000 0.34309291604989184
7 2100 0.3437900564519988

Train Epoch: 7 Average training loss: 0.3387 

8 0 0.3859439790248871
8 100 0.34097368278657003
8 200 0.3361743269843693
8 300 0.3597791721936973
8 400 0.3725137114742965
8 500 0.3983283305803187
8 600 0.3826884647884285
8 700 0.3449307167415217
8 800 0.31029834603154866
8 900 0.3456254930755642
8 1000 0.3507977075999871
8 1100 0.35570738866761636
8 1200 0.3596573595443966
8 1300 0.34195542250037425
8 1400 0.3520935925994629
8 1500 0.3344757113515577
8 1600 0.3495355898148475
8 1700 0.3170050581354741
8 1800 0.32947420878334777
8 1900 0.33710700995537585
8 2000 0.33974691795940004
8 2100 0.3121881248553759

Train Epoch: 8 Average training loss: 0.3476 

9 0 0.2822876572608948
9 100 0.30747122408306526
9 200 0.3285049557785122
9 300 0.3662627751566798
9 400 0.3403032141489047
9 500 0.35961865698940176
9 600 0.36260284384048486
9 700 0.3695645032716191
9 800 0.3516584877134659
9 900 0.3516985159028267
9 1000 0.3529886792759287
9 1100 0.3230622065415814
9 1200 0.33127918435418935
9 1300 0.344775321784516
9 1400 0.34887899580058035
9 1500 0.33807883443391173
9 1600 0.32487538145525413
9 1700 0.31983238898434685
9 1800 0.32972261950079695
9 1900 0.3429615849667007
9 2000 0.3476541846596113
9 2100 0.32298234189092734

Train Epoch: 9 Average training loss: 0.3405 

10 0 0.08900675177574158
10 100 0.22442035904015217
10 200 0.29753531790118376
10 300 0.32566689617852485
10 400 0.3422276686375571
10 500 0.31830982577430966
10 600 0.34442752195953097
10 700 0.3599902787471808
10 800 0.33535324181017234
10 900 0.333926817110075
10 1000 0.33507063255553987
10 1100 0.32525627178068695
10 1200 0.33407852743752814
10 1300 0.3181969279321011
10 1400 0.34785222784500225
10 1500 0.33713839745596813
10 1600 0.36920642678178744
10 1700 0.3540921149542918
10 1800 0.34907365372009735
10 1900 0.33914208464437706
10 2000 0.3425207745946638
10 2100 0.3368284271130222

Train Epoch: 10 Average training loss: 0.3268 

11 0 0.6437051296234131
11 100 0.44700121799269876
11 200 0.35980464840060233
11 300 0.3488475191532976
11 400 0.36579406230785655
11 500 0.3677158801876124
11 600 0.3277967324718922
11 700 0.33082171985119835
11 800 0.305795477801592
11 900 0.3040140458143587
11 1000 0.31357220699252775
11 1100 0.3551461197263639
11 1200 0.3588726720430469
11 1300 0.33614936224513653
11 1400 0.328631772587933
11 1500 0.3426959349439966
11 1600 0.36625857698262515
11 1700 0.34966888128832524
11 1800 0.3334243731030308
11 1900 0.32309035005967884
11 2000 0.3244882143750274
11 2100 0.33093586166978045

Train Epoch: 11 Average training loss: 0.3489 

12 0 0.3416188061237335
12 100 0.3439621311578969
12 200 0.3309098717672577
12 300 0.32770544656847567
12 400 0.3132120022775145
12 500 0.33094989545505255
12 600 0.34210123006376003
12 700 0.36164983288731517
12 800 0.35158437426390826
12 900 0.3522133347847311
12 1000 0.3015957927924994
12 1100 0.3212897473311468
12 1200 0.3286255676602677
12 1300 0.34676125918327877
12 1400 0.3277411709559476
12 1500 0.3396583725624141
12 1600 0.35200518439852174
12 1700 0.345068157146089
12 1800 0.33966412439831717
12 1900 0.32843393136085564
12 2000 0.3420445419869057
12 2100 0.34621129182347116

Train Epoch: 12 Average training loss: 0.3366 

13 0 0.08117727935314178
13 100 0.2649526783879497
13 200 0.3366505855717071
13 300 0.34306866639488054
13 400 0.3467940185508523
13 500 0.3544581617364297
13 600 0.3621183068862064
13 700 0.31984532311513025
13 800 0.3249960893046841
13 900 0.33536689881729426
13 1000 0.3426350040156655
13 1100 0.3235354857442901
13 1200 0.31534837013467626
13 1300 0.28781886778447635
13 1400 0.30858679594768756
13 1500 0.3238117896642731
13 1600 0.31165656833932287
13 1700 0.32181133264044554
13 1800 0.29909798821196626
13 1900 0.3223020173366935
13 2000 0.3250235890585743
13 2100 0.35274506370890396

Train Epoch: 13 Average training loss: 0.3200 

14 0 0.5124008655548096
14 100 0.39514007830662273
14 200 0.36056468616145404
14 300 0.3529729069268422
14 400 0.3467379080166809
14 500 0.3346069548874013
14 600 0.29869611139526203
14 700 0.31076472308141034
14 800 0.3143849274362012
14 900 0.34142596480146714
14 1000 0.3014373949953931
14 1100 0.32663683004910576
14 1200 0.3273280467442072
14 1300 0.3431769850449822
14 1400 0.33408826816828585
14 1500 0.3538475881047325
14 1600 0.34037003719992115
14 1700 0.3315142254809551
14 1800 0.335216665200393
14 1900 0.3276294134864235
14 2000 0.31784460692615907
14 2100 0.3620297842602118

Train Epoch: 14 Average training loss: 0.3388 

15 0 0.5740206241607666
15 100 0.4227306103034049
15 200 0.3490547052404823
15 300 0.3224378435049353
15 400 0.3147282377549672
15 500 0.319534038371295
15 600 0.3138833995827719
15 700 0.3271710814935814
15 800 0.3199531646400989
15 900 0.34473408305731396
15 1000 0.35784835619464944
15 1100 0.3320567788468926
15 1200 0.3522132855930847
15 1300 0.36076782443286
15 1400 0.34832351576301374
15 1500 0.3286178996978381
15 1600 0.3283642822253929
15 1700 0.3365740530722395
15 1800 0.3404175999602394
15 1900 0.3246603416680127
15 2000 0.3116774777585981
15 2100 0.333863742286721

Train Epoch: 15 Average training loss: 0.3430 

16 0 0.2394370585680008
16 100 0.3003276692268039
16 200 0.31743677571859097
16 300 0.30690743874296744
16 400 0.3090604218956417
16 500 0.30013438082540855
16 600 0.3270886197903174
16 700 0.3495125982190808
16 800 0.3335961605950109
16 900 0.32775061573920256
16 1000 0.3368240854780866
16 1100 0.3471729472491144
16 1200 0.3544807982332369
16 1300 0.3300270805723771
16 1400 0.32732488860822134
16 1500 0.31616397831981125
16 1600 0.31844535407484037
16 1700 0.31532866874318993
16 1800 0.3201085177322538
16 1900 0.3403599941123513
16 2000 0.33273467794144324
16 2100 0.3428875715689802

Train Epoch: 16 Average training loss: 0.3246 

17 0 0.45313847064971924
17 100 0.3803657883711064
17 200 0.32015952239285417
17 300 0.3211276017668456
17 400 0.3337922433404964
17 500 0.324031799228855
17 600 0.3404242621017509
17 700 0.33633307153703684
17 800 0.3164411408913314
17 900 0.32320477433851247
17 1000 0.3175842056784373
17 1100 0.3451398501155331
17 1200 0.3117124251488009
17 1300 0.3307359189413218
17 1400 0.3138238771024196
17 1500 0.3445844789052339
17 1600 0.3301944025464726
17 1700 0.32802704131812055
17 1800 0.31504656825368516
17 1900 0.33749911213332334
17 2000 0.3381614339862064
17 2100 0.3617377652708469

Train Epoch: 17 Average training loss: 0.3335 

18 0 0.1995721459388733
18 100 0.2637121401886692
18 200 0.3101640774576408
18 300 0.3253339326463967
18 400 0.33288718807931655
18 500 0.34422504068326
18 600 0.34289508596570173
18 700 0.32056318559853697
18 800 0.32480276021475984
18 900 0.3069601207524293
18 1000 0.3066967803202088
18 1100 0.3025210288474677
18 1200 0.32074254374877975
18 1300 0.34000438202335365
18 1400 0.387498728761006
18 1500 0.3597422499904548
18 1600 0.32370677506175394
18 1700 0.31668147571550165
18 1800 0.3164700256882406
18 1900 0.30528671928661044
18 2000 0.3140891630336192
18 2100 0.31728710918971603

Train Epoch: 18 Average training loss: 0.3214 

19 0 0.2557092607021332
19 100 0.31203976705928876
19 200 0.31216248239062455
19 300 0.32819625505608097
19 400 0.31754644814445915
19 500 0.30860416404776075
19 600 0.3120886980472419
19 700 0.35244639404147166
19 800 0.33528534944465804
19 900 0.30906588561127757
19 1000 0.3327949558108337
19 1100 0.32908707652372426
19 1200 0.3440186047960159
19 1300 0.3596179948173337
19 1400 0.32286461215770984
19 1500 0.32230932229608494
19 1600 0.3078469726233339
19 1700 0.30804296332264075
19 1800 0.33207249190062826
19 1900 0.3200929193082863
19 2000 0.3384640892419842
19 2100 0.3384614729725741

Train Epoch: 19 Average training loss: 0.3242 

20 0 0.21403448283672333
20 100 0.29043321489244717
20 200 0.32504015989183976
20 300 0.32008665792635643
20 400 0.3333227180603687
20 500 0.32102360727075896
20 600 0.3147989286407029
20 700 0.31755682215377085
20 800 0.34285950110821295
20 900 0.3304822012485564
20 1000 0.3449015281206577
20 1100 0.3253011282227447
20 1200 0.3247889316439128
20 1300 0.2935132063627666
20 1400 0.3178112889784677
20 1500 0.3013850827411943
20 1600 0.31656155137269465
20 1700 0.3297168842963234
20 1800 0.3087609133901247
20 1900 0.34394374878979156
20 2000 0.3146113810300903
20 2100 0.33040573347555546

Train Epoch: 20 Average training loss: 0.3204 

21 0 0.1631872057914734
21 100 0.26919401205688037
21 200 0.3300720731967317
21 300 0.3175375771000724
21 400 0.3293280098064389
21 500 0.34113289262984564
21 600 0.31308575832731744
21 700 0.3074609408458304
21 800 0.33729218701275787
21 900 0.3373214351797554
21 1000 0.3352402282216745
21 1100 0.31637946509061776
21 1200 0.3409366688099918
21 1300 0.34140445477975884
21 1400 0.33910096543417045
21 1500 0.341738465576258
21 1600 0.3285775722189636
21 1700 0.30936235269781537
21 1800 0.2878100452235117
21 1900 0.2977875869392754
21 2000 0.3143169676151174
21 2100 0.3369325902301396

Train Epoch: 21 Average training loss: 0.3166 

22 0 0.38277655839920044
22 100 0.35305225255638034
22 200 0.320881527052873
22 300 0.34151823600337083
22 400 0.32746840890075585
22 500 0.3417464799962951
22 600 0.3059617618935595
22 700 0.3018658971578028
22 800 0.32893728481059337
22 900 0.3460858125825053
22 1000 0.33066469889194494
22 1100 0.32860485319660354
22 1200 0.3629629160005776
22 1300 0.3557775247143266
22 1400 0.33388619060308417
22 1500 0.3031620829126604
22 1600 0.3043461161478383
22 1700 0.3207626983237123
22 1800 0.3184029021851333
22 1900 0.31880439197818966
22 2000 0.3240970217746238
22 2100 0.33984020509394863

Train Epoch: 22 Average training loss: 0.3264 

23 0 0.15594831109046936
23 100 0.26859115517917076
23 200 0.27987461170106137
23 300 0.3314993079232157
23 400 0.3191550851840415
23 500 0.3370683880581905
23 600 0.3251056315837557
23 700 0.318199000178619
23 800 0.322290692525045
23 900 0.3302670770689486
23 1000 0.327718579199621
23 1100 0.35288993595052803
23 1200 0.3353072676791953
23 1300 0.34348834101876896
23 1400 0.3068217456511428
23 1500 0.33065934449163137
23 1600 0.3272182276268818
23 1700 0.30028501690239184
23 1800 0.31178266465203913
23 1900 0.3213577887489094
23 2000 0.322575516347702
23 2100 0.3459273630109909

Train Epoch: 23 Average training loss: 0.3154 

24 0 0.224787637591362
24 100 0.2660579380391725
24 200 0.30876981454503066
24 300 0.3143002042813357
24 400 0.3349726752120658
24 500 0.3229337121286521
24 600 0.2966850097619815
24 700 0.3179718849289433
24 800 0.30507267785586123
24 900 0.3157306963289892
24 1000 0.3411308343537705
24 1100 0.3361516869722929
24 1200 0.32466398393536755
24 1300 0.3381292808427686
24 1400 0.3108291629591506
24 1500 0.3599481271052371
24 1600 0.3427356216145207
24 1700 0.32403398643830644
24 1800 0.3320962026042088
24 1900 0.32072418679853826
24 2000 0.30947207758601036
24 2100 0.31370412451380936

Train Epoch: 24 Average training loss: 0.3197 

25 0 0.39292487502098083
25 100 0.35995358985386217
25 200 0.3392759627486648
25 300 0.33362034853553363
25 400 0.3143852557668657
25 500 0.3188679097224806
25 600 0.32504346080290164
25 700 0.3254548427648152
25 800 0.34098396533850356
25 900 0.3369200349603698
25 1000 0.32977206446069196
25 1100 0.28942717851628513
25 1200 0.31499443922091985
25 1300 0.3500759083939507
25 1400 0.31426557432724306
25 1500 0.3178206926284841
25 1600 0.335240323931281
25 1700 0.3045709619392029
25 1800 0.32380883285889156
25 1900 0.32364188274706746
25 2000 0.3335400633120256
25 2100 0.3172743064713886

Train Epoch: 25 Average training loss: 0.3263 

26 0 0.24918872117996216
26 100 0.3015513040895033
26 200 0.3146220792767677
26 300 0.31661860991700164
26 400 0.3199936011474683
26 500 0.32437681389550355
26 600 0.3383881044634821
26 700 0.3302940817594324
26 800 0.313041800957443
26 900 0.3093785282580254
26 1000 0.3094505380612713
26 1100 0.31183115824785956
26 1200 0.30083102314192767
26 1300 0.3062728633594398
26 1400 0.3249569001236151
26 1500 0.2879045642633737
26 1600 0.3202960073892975
26 1700 0.3234751759692118
26 1800 0.3365147459372255
26 1900 0.32625285860920444
26 2000 0.3344210099096814
26 2100 0.34074028950112756

Train Epoch: 26 Average training loss: 0.3181 

27 0 0.8604950904846191
27 100 0.5178955560029547
27 200 0.3929080762725283
27 300 0.3336307210808004
27 400 0.3307307849093157
27 500 0.3140650416503632
27 600 0.3180367448218613
27 700 0.3279955433446597
27 800 0.34168756963575747
27 900 0.3765377037959638
27 1000 0.33100740776439325
27 1100 0.3217527006832223
27 1200 0.29201155978038423
27 1300 0.2942715216523334
27 1400 0.2961787672005101
27 1500 0.3285261940303716
27 1600 0.3026136868980312
27 1700 0.31056765984554496
27 1800 0.3416202571102228
27 1900 0.34804039273358023
27 2000 0.33385535727259336
27 2100 0.3337147170561155

Train Epoch: 27 Average training loss: 0.3481 

28 0 0.2538319528102875
28 100 0.2777216474394255
28 200 0.29435345104683147
28 300 0.32795095168616345
28 400 0.32488620512769634
28 500 0.31337219622473
28 600 0.32914888153703376
28 700 0.31504419995260263
28 800 0.31419216018936486
28 900 0.3157415443011174
28 1000 0.3276398154732992
28 1100 0.32162021501300514
28 1200 0.324163903224817
28 1300 0.3123800258780088
28 1400 0.3474939056364529
28 1500 0.3256366375373849
28 1600 0.31842760797135056
28 1700 0.33180656109223794
28 1800 0.3204544815051663
28 1900 0.29286784125349546
28 2000 0.3217716615493869
28 2100 0.32794778870988994

Train Epoch: 28 Average training loss: 0.3167 

29 0 0.14401015639305115
29 100 0.25681268840740235
29 200 0.299724749463948
29 300 0.32543428972102567
29 400 0.35425261956741294
29 500 0.32535679330421535
29 600 0.32261458582372665
29 700 0.3079873452647587
29 800 0.3221997679970336
29 900 0.2762974646838639
29 1000 0.29483507371503664
29 1100 0.322665446835976
29 1200 0.3135719193236266
29 1300 0.324958457925741
29 1400 0.3076235623222016
29 1500 0.3265875476288932
29 1600 0.3061319847198198
29 1700 0.318153619752522
29 1800 0.3320553266466909
29 1900 0.35307191853356673
29 2000 0.34234237298318365
29 2100 0.31157557338271047

Train Epoch: 29 Average training loss: 0.3113 

30 0 0.11216147243976593
30 100 0.2318580725071592
30 200 0.2827099729677198
30 300 0.30562272589582523
30 400 0.3389088822174572
30 500 0.33816390869844454
30 600 0.3404562816851937
30 700 0.36022500358471876
30 800 0.3235467065424598
30 900 0.3432794097763215
30 1000 0.3140224682435296
30 1100 0.30271603319433354
30 1200 0.30797934348068945
30 1300 0.27699139004855666
30 1400 0.2953399962661603
30 1500 0.32259972641190654
30 1600 0.35374588313360195
30 1700 0.32236888336289415
30 1800 0.33602819207169327
30 1900 0.3046250619647639
30 2000 0.2911121890182204
30 2100 0.29863184698802403

Train Epoch: 30 Average training loss: 0.3100 

31 0 0.15786367654800415
31 100 0.23924925113318282
31 200 0.2988526361489146
31 300 0.30306443251762416
31 400 0.3300461462558142
31 500 0.30348835859839496
31 600 0.31329506526839246
31 700 0.3144628659735466
31 800 0.3206373785863713
31 900 0.3212830125253966
31 1000 0.31846324484807564
31 1100 0.3291651784808867
31 1200 0.3117602095912204
31 1300 0.3189500798119543
31 1400 0.29023944327685935
31 1500 0.29722480182069233
31 1600 0.29385990565979125
31 1700 0.2961005609961476
31 1800 0.3144310336902799
31 1900 0.31826882404396306
31 2000 0.3281231782264348
31 2100 0.3222461729857823

Train Epoch: 31 Average training loss: 0.3084 

32 0 0.2832723557949066
32 100 0.3276988883900263
32 200 0.30822941039387863
32 300 0.2997866650455604
32 400 0.3139119295940044
32 500 0.35638065497033683
32 600 0.33758416769929167
32 700 0.3272928314359747
32 800 0.32535788485970146
32 900 0.3104122071358936
32 1000 0.30664730527428224
32 1100 0.29633314663923066
32 1200 0.2910313176008383
32 1300 0.3257200467989762
32 1400 0.321167030997941
32 1500 0.3446257498829856
32 1600 0.3179318033211846
32 1700 0.33169678050736884
32 1800 0.3060466167591618
32 1900 0.3141616869472795
32 2000 0.29252245499640783
32 2100 0.32152350741036134

Train Epoch: 32 Average training loss: 0.3164 

33 0 0.21781860291957855
33 100 0.2675156304728872
33 200 0.28683151591437817
33 300 0.3177598364153471
33 400 0.3293819960481959
33 500 0.32111509207269373
33 600 0.334181803060422
33 700 0.3319594478524924
33 800 0.3184526876726903
33 900 0.3349448919657202
33 1000 0.3201019242984369
33 1100 0.3102423380048444
33 1200 0.31844579859253336
33 1300 0.29634902911226196
33 1400 0.31648145590933263
33 1500 0.3332285125995664
33 1600 0.3153326437818876
33 1700 0.3098726939500079
33 1800 0.3135295473025801
33 1900 0.3325387561668866
33 2000 0.3280880433367107
33 2100 0.32924488684893577

Train Epoch: 33 Average training loss: 0.3140 

34 0 0.21801792085170746
34 100 0.2960126706285627
34 200 0.2834213614139705
34 300 0.29232406279850703
34 400 0.30886293544062343
34 500 0.30034763686500504
34 600 0.29927535755495976
34 700 0.30717694668862455
34 800 0.2894150107423325
34 900 0.3276944465765364
34 1000 0.29179337969786834
34 1100 0.31351416624555567
34 1200 0.35456906944734773
34 1300 0.3118228485051129
34 1400 0.3074755979247094
34 1500 0.30957496211221186
34 1600 0.30164257517625276
34 1700 0.3359420667069124
34 1800 0.33840540999517615
34 1900 0.3102545278233773
34 2000 0.33646150687231097
34 2100 0.3382266324408641

Train Epoch: 34 Average training loss: 0.3117 

35 0 0.13047966361045837
35 100 0.25819254418138704
35 200 0.29390078136624903
35 300 0.2944222362687993
35 400 0.26283204130146137
35 500 0.2736590874296225
35 600 0.3149845347649512
35 700 0.33061974801940913
35 800 0.3467660145983903
35 900 0.33192652443007375
35 1000 0.35144134388004195
35 1100 0.3243845353350345
35 1200 0.3344706889414054
35 1300 0.324397368404903
35 1400 0.31744123734425306
35 1500 0.28698960731378176
35 1600 0.3124703359227938
35 1700 0.32105678602645576
35 1800 0.3153863607717952
35 1900 0.30643003128155527
35 2000 0.3125574641078212
35 2100 0.3370021340714191

Train Epoch: 35 Average training loss: 0.3086 

36 0 0.6778708100318909
36 100 0.43562305126138773
36 200 0.35104360851695005
36 300 0.3237435535234602
36 400 0.31304712001998386
36 500 0.3137606464153244
36 600 0.3131413534671578
36 700 0.3176601206068723
36 800 0.3313150655363533
36 900 0.3273352536746449
36 1000 0.3283857127357209
36 1100 0.3404010445824712
36 1200 0.3284047029423577
36 1300 0.3150791972090679
36 1400 0.3182335011201125
36 1500 0.3321647534531234
36 1600 0.32555384050667535
36 1700 0.30954943789417444
36 1800 0.3036629053395589
36 1900 0.3188423806877448
36 2000 0.31924441269505405
36 2100 0.31264394846892996

Train Epoch: 36 Average training loss: 0.3342 

37 0 0.14606480300426483
37 100 0.255063558526298
37 200 0.29087882250704444
37 300 0.3210440364183434
37 400 0.3224141527907737
37 500 0.3033491258409576
37 600 0.31520244740177017
37 700 0.3029552025205859
37 800 0.3071956649201717
37 900 0.3123058489597909
37 1000 0.29953481160230766
37 1100 0.3124055966655937
37 1200 0.33258401567184853
37 1300 0.30884153430788647
37 1400 0.2911371628888439
37 1500 0.31512525781983286
37 1600 0.33492079107589157
37 1700 0.319934854827737
37 1800 0.31793404875038106
37 1900 0.32096539534203367
37 2000 0.333170804792597
37 2100 0.31768753032842023

Train Epoch: 37 Average training loss: 0.3076 

38 0 0.4954410195350647
38 100 0.38646834712152817
38 200 0.35838519445082484
38 300 0.33905065894808467
38 400 0.3065718059055083
38 500 0.3486742401235564
38 600 0.35448488432307784
38 700 0.33353145437181203
38 800 0.32255077464911686
38 900 0.30421227345352375
38 1000 0.2992854795175678
38 1100 0.3059463221718703
38 1200 0.30414354938746163
38 1300 0.3047031712022962
38 1400 0.29749743642467696
38 1500 0.30383168902864255
38 1600 0.2981299253139173
38 1700 0.3490119083145028
38 1800 0.323282005919915
38 1900 0.2992819664369321
38 2000 0.3212084324779686
38 2100 0.3124745569306192

Train Epoch: 38 Average training loss: 0.3254 

39 0 0.18488749861717224
39 100 0.25448013096340577
39 200 0.30739245741584
39 300 0.29822656674034453
39 400 0.3050412758280116
39 500 0.3120109013524152
39 600 0.29294632489171357
39 700 0.323788377954939
39 800 0.31247463370845485
39 900 0.3206857099261851
39 1000 0.3318058522320107
39 1100 0.3247708038489644
39 1200 0.3105641358919316
39 1300 0.307083661289197
39 1400 0.29856710898874655
39 1500 0.31344309603573195
39 1600 0.30919801866731905
39 1700 0.29989056862879226
39 1800 0.3122344150906684
39 1900 0.31643554235159194
39 2000 0.33277002466896494
39 2100 0.33698225327623704

Train Epoch: 39 Average training loss: 0.3075 

40 0 0.16762806475162506
40 100 0.28871545556084505
40 200 0.317266937867125
40 300 0.2944651871856799
40 400 0.3060243649774057
40 500 0.30733756237116716
40 600 0.34496930737589493
40 700 0.3295271305418555
40 800 0.3117965309811094
40 900 0.30955264891656953
40 1000 0.30436089638949143
40 1100 0.28838162346754714
40 1200 0.29292375546408245
40 1300 0.2910119640408178
40 1400 0.30084024959615263
40 1500 0.3240630296119862
40 1600 0.3045143325543743
40 1700 0.3087199936909404
40 1800 0.3202477624917238
40 1900 0.32767326656535134
40 2000 0.31193211533316845
40 2100 0.322413889823707

Train Epoch: 40 Average training loss: 0.3063 

41 0 0.24283690750598907
41 100 0.2799867869261047
41 200 0.2869556651347359
41 300 0.30112607877034636
41 400 0.30284598305751287
41 500 0.28601546797665217
41 600 0.29638401940258036
41 700 0.3074792854014079
41 800 0.32990387181249403
41 900 0.3175543253848127
41 1000 0.3071812721056296
41 1100 0.29046184676603304
41 1200 0.33505049494284705
41 1300 0.3308238258480192
41 1400 0.316919803498605
41 1500 0.3144837484127259
41 1600 0.33863246084820686
41 1700 0.31925256770917476
41 1800 0.31444437240302164
41 1900 0.3061666420106866
41 2000 0.31691226522454924
41 2100 0.3215641622547917

Train Epoch: 41 Average training loss: 0.3078 

42 0 0.14645680785179138
42 100 0.2662061456246225
42 200 0.30267521103263967
42 300 0.3079595046218608
42 400 0.3038095113459649
42 500 0.29551673270298257
42 600 0.3068242512036416
42 700 0.30911735944369156
42 800 0.324575078704562
42 900 0.29558005821588856
42 1000 0.2996055148882819
42 1100 0.3272166571183413
42 1200 0.32214948245399966
42 1300 0.3357912900355207
42 1400 0.3020392126875344
42 1500 0.31036681362754187
42 1600 0.32845869790744986
42 1700 0.31292141569087056
42 1800 0.31631783312755635
42 1900 0.30593399063527266
42 2000 0.2858898578989686
42 2100 0.312110451003277

Train Epoch: 42 Average training loss: 0.3063 

43 0 0.2612248659133911
43 100 0.29493820257870756
43 200 0.29442431196089347
43 300 0.2902119528724135
43 400 0.3116049425189148
43 500 0.3122294954152142
43 600 0.29494215951769803
43 700 0.33735527327663595
43 800 0.3057526880433084
43 900 0.28865410674805353
43 1000 0.29848912396671884
43 1100 0.31035782634909254
43 1200 0.31084558386311273
43 1300 0.3059000118007395
43 1400 0.317554488124798
43 1500 0.3089132883439923
43 1600 0.3261802670778469
43 1700 0.36035560643288905
43 1800 0.30091071953078896
43 1900 0.30254997364545555
43 2000 0.3059477502243284
43 2100 0.31235236637366465

Train Epoch: 43 Average training loss: 0.3086 

44 0 0.3737061619758606
44 100 0.3315140075920936
44 200 0.32991181394816105
44 300 0.30487212942564507
44 400 0.3179227813365843
44 500 0.31441371269597407
44 600 0.30918062740922336
44 700 0.3078055563792336
44 800 0.3233523274108813
44 900 0.3187240360976478
44 1000 0.30245436842478873
44 1100 0.31481779097192053
44 1200 0.31965210421714724
44 1300 0.3090980331053478
44 1400 0.3413578484771277
44 1500 0.30926094401062165
44 1600 0.31786966311003284
44 1700 0.2937597526077419
44 1800 0.3023046045724967
44 1900 0.312580065096052
44 2000 0.30792882849823117
44 2100 0.31087083132435106

Train Epoch: 44 Average training loss: 0.3159 

45 0 0.058966685086488724
45 100 0.20549461618080295
45 200 0.28486150732106646
45 300 0.30331498499319254
45 400 0.3045968981368618
45 500 0.28935107832426016
45 600 0.27684642224574835
45 700 0.3023092958461745
45 800 0.3130572505428633
45 900 0.31238305566823066
45 1000 0.3213478459385947
45 1100 0.31109193207816055
45 1200 0.31268144213385646
45 1300 0.34962909463037
45 1400 0.3247982941904435
45 1500 0.29620436148287244
45 1600 0.30767163424459704
45 1700 0.31948225952001574
45 1800 0.3033461791219696
45 1900 0.31901084665365564
45 2000 0.3152141960781287
45 2100 0.33182440356197124

Train Epoch: 45 Average training loss: 0.2993 

46 0 0.22572454810142517
46 100 0.27124853753409184
46 200 0.27391609367963093
46 300 0.289754072943254
46 400 0.2950746318298129
46 500 0.28999961770860805
46 600 0.3215478502456907
46 700 0.30801670474649184
46 800 0.2953171652144421
46 900 0.32819894223170704
46 1000 0.32525960231311096
46 1100 0.31917235981469977
46 1200 0.3005960949774485
46 1300 0.32893538807984457
46 1400 0.3357383012788592
46 1500 0.3148401855472467
46 1600 0.31246168762459997
46 1700 0.3273389161813668
46 1800 0.33999556073153836
46 1900 0.304781464814307
46 2000 0.31518886628213527
46 2100 0.321812835276493

Train Epoch: 46 Average training loss: 0.3070 

47 0 0.11596353352069855
47 100 0.23862678609611535
47 200 0.2680891201109057
47 300 0.3052169980275799
47 400 0.3087416083012975
47 500 0.32180245838896365
47 600 0.2976748674895886
47 700 0.351713657782938
47 800 0.3374835087533134
47 900 0.31963548564164507
47 1000 0.31820034559561566
47 1100 0.30271962990730017
47 1200 0.3050027513243244
47 1300 0.3138455502224875
47 1400 0.29762106779593694
47 1500 0.28992124735081454
47 1600 0.29295586581681515
47 1700 0.31777017558940424
47 1800 0.30380597016347943
47 1900 0.31757966937699234
47 2000 0.30701445687435197
47 2100 0.30169873456982516

Train Epoch: 47 Average training loss: 0.3024 

48 0 0.10494370758533478
48 100 0.24928833943877848
48 200 0.2922039884172082
48 300 0.28367666496337357
48 400 0.30201597414486314
48 500 0.28069181540249294
48 600 0.313871744782414
48 700 0.30215750804497515
48 800 0.2939641743128965
48 900 0.2835873853759799
48 1000 0.29979726710722066
48 1100 0.33299785369542256
48 1200 0.3023531443675067
48 1300 0.31022478765299993
48 1400 0.3105527826592118
48 1500 0.32224408986978503
48 1600 0.3094114771750436
48 1700 0.3000219658507931
48 1800 0.31838252110925025
48 1900 0.3203911408476844
48 2000 0.2951322874226624
48 2100 0.3211593919202646

Train Epoch: 48 Average training loss: 0.3003 

49 0 0.3455031216144562
49 100 0.34444485577236444
49 200 0.2935746451452216
49 300 0.32649295363457714
49 400 0.32959358261138383
49 500 0.3134632336521296
49 600 0.3427784577026663
49 700 0.3006365984132858
49 800 0.31043410438981817
49 900 0.29736872470393044
49 1000 0.29640978691036335
49 1100 0.2847085271160984
49 1200 0.2957470757830983
49 1300 0.30225272979442563
49 1400 0.32084319285594304
49 1500 0.2936972453627941
49 1600 0.30526711385937766
49 1700 0.31090064038006326
49 1800 0.3106490385831797
49 1900 0.313805486598987
49 2000 0.324731692457024
49 2100 0.3173350691664085

Train Epoch: 49 Average training loss: 0.3112 

50 0 0.08385634422302246
50 100 0.21002706762256368
50 200 0.2768435447140964
50 300 0.28528676548365467
50 400 0.3094122462562388
50 500 0.33851712710245413
50 600 0.3303152587717421
50 700 0.31426203619185866
50 800 0.30275188880099446
50 900 0.32787083259178706
50 1000 0.30007537776439996
50 1100 0.30300902165659055
50 1200 0.3162705662961558
50 1300 0.31239807350853416
50 1400 0.31385910394956684
50 1500 0.2978503944199407
50 1600 0.33690837251276246
50 1700 0.3150437077087664
50 1800 0.31337600246017155
50 1900 0.3079356346623609
50 2000 0.28711349476259007
50 2100 0.3117083756395359

Train Epoch: 50 Average training loss: 0.3006 

