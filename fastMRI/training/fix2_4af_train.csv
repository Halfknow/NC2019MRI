1 0 1.673529028892517
1 100 0.945878916163759
1 200 0.5812910204167541
1 300 0.42386423960634684
1 400 0.3671724521028268
1 500 0.3166526210090998
1 600 0.30067434308709573
1 700 0.29951883370351445
1 800 0.31688666892082745
1 900 0.2997496734588862
1 1000 0.2776272463292819
1 1100 0.26481786528956125
1 1200 0.2696801898778504
1 1300 0.2872061133708716
1 1400 0.28332328476451546
1 1500 0.28854545950542393
1 1600 0.26800522172161473
1 1700 0.2710578426661698
1 1800 0.2798327432652388
1 1900 0.29430974895089973
1 2000 0.3071883320994504
1 2100 0.28786579224699693

Train Epoch: 1 Average training loss: 0.3727 

2 0 0.3086997866630554
2 100 0.2950948793053572
2 200 0.2883727875793476
2 300 0.26213062561133477
2 400 0.2706445115006459
2 500 0.27084750479788716
2 600 0.2671450629100304
2 700 0.24986096547989597
2 800 0.2821814153138208
2 900 0.26431417915427013
2 1000 0.2833486776480017
2 1100 0.286143823844015
2 1200 0.2729524654480568
2 1300 0.27707978040527514
2 1400 0.2823780157571804
2 1500 0.2711477629334344
2 1600 0.2596684418904816
2 1700 0.2728163858454755
2 1800 0.2636224500397433
2 1900 0.2439319059735419
2 2000 0.2653959315968056
2 2100 0.27509601746675455

Train Epoch: 2 Average training loss: 0.2741 

3 0 0.24055378139019012
3 100 0.25868062810872705
3 200 0.25117758494438996
3 300 0.25252177760343125
3 400 0.253705048046668
3 500 0.26328982900116865
3 600 0.29531476620842084
3 700 0.2616093663316612
3 800 0.26526532132102176
3 900 0.26511596672605586
3 1000 0.2711464767880076
3 1100 0.2595272196192083
3 1200 0.2755957593823869
3 1300 0.2703775482738312
3 1400 0.2723281003892769
3 1500 0.2700104157112033
3 1600 0.2848581506573856
3 1700 0.25687099401711666
3 1800 0.2621534721993065
3 1900 0.27861334836394736
3 2000 0.27066326743302416
3 2100 0.2670222172341837

Train Epoch: 3 Average training loss: 0.2664 

4 0 0.4996603727340698
4 100 0.3395732600729074
4 200 0.29824493125970225
4 300 0.28133809950727456
4 400 0.28454583601849504
4 500 0.27683335947424265
4 600 0.2627032404466671
4 700 0.26738524205822567
4 800 0.28010020794675927
4 900 0.2670587934856189
4 1000 0.27004811212905905
4 1100 0.25753533784329447
4 1200 0.2662112975651233
4 1300 0.2574875704977206
4 1400 0.2353922038766996
4 1500 0.2558460554302269
4 1600 0.2537252145098502
4 1700 0.25288775686428233
4 1800 0.243141855317598
4 1900 0.25846451812688875
4 2000 0.26686078861348345
4 2100 0.2589472238237269

Train Epoch: 4 Average training loss: 0.2745 

5 0 0.09670669585466385
5 100 0.2135194356768612
5 200 0.25712129894337143
5 300 0.2687211668526657
5 400 0.2738687209552961
5 500 0.2624993798385759
5 600 0.25705818409369824
5 700 0.23727165464103112
5 800 0.25496380087149767
5 900 0.24874156189957308
5 1000 0.2725116139806976
5 1100 0.2895833798535554
5 1200 0.27719122239475413
5 1300 0.25571337968269914
5 1400 0.26343177394005773
5 1500 0.26489239295698674
5 1600 0.2464225750853467
5 1700 0.23757760845604103
5 1800 0.24236069785051712
5 1900 0.23981479318402105
5 2000 0.2532756914541152
5 2100 0.2563075823974698

Train Epoch: 5 Average training loss: 0.2533 

6 0 0.31034162640571594
6 100 0.2540227188451206
6 200 0.24915874178662117
6 300 0.2566886202345696
6 400 0.2611964739775667
6 500 0.26651461974541873
6 600 0.2858593417303252
6 700 0.26684457601047623
6 800 0.26746676306862927
6 900 0.2669158870342226
6 1000 0.2643566595366069
6 1100 0.24713008394643668
6 1200 0.26777900763836465
6 1300 0.260752321025854
6 1400 0.2714506862712153
6 1500 0.2696243961276615
6 1600 0.275281458622385
6 1700 0.26425766402552425
6 1800 0.26056104973161526
6 1900 0.23839023024178613
6 2000 0.25972817158161304
6 2100 0.26825273138613975

Train Epoch: 6 Average training loss: 0.2625 

7 0 0.3852718472480774
7 100 0.3079917860339545
7 200 0.27384438441451237
7 300 0.26062951512891286
7 400 0.25364939793339353
7 500 0.24276712759496283
7 600 0.27582397705349093
7 700 0.26765029936588425
7 800 0.24079981673399165
7 900 0.2610052489509421
7 1000 0.2535324793804831
7 1100 0.2504710769741516
7 1200 0.2644707586953693
7 1300 0.25309673593317195
7 1400 0.2748690638675583
7 1500 0.26966427231409806
7 1600 0.24555217021247783
7 1700 0.2508373314324799
7 1800 0.24558708355890868
7 1900 0.26681745450022715
7 2000 0.25172225417090394
7 2100 0.24990449080608787

Train Epoch: 7 Average training loss: 0.2621 

8 0 0.060366250574588776
8 100 0.19240552309665718
8 200 0.21423158208840562
8 300 0.23187213688018685
8 400 0.25557881114621517
8 500 0.263724034484731
8 600 0.26796302902587465
8 700 0.2673556449678846
8 800 0.25026761250426754
8 900 0.25547924002242245
8 1000 0.2744219691285137
8 1100 0.24957137259912227
8 1200 0.23755591826396874
8 1300 0.24930041273527984
8 1400 0.25044815334901455
8 1500 0.2514740917096525
8 1600 0.2466943552183724
8 1700 0.25164241154887473
8 1800 0.2496478411733535
8 1900 0.23886926315203852
8 2000 0.2639198227463524
8 2100 0.2707570669203648

Train Epoch: 8 Average training loss: 0.2468 

9 0 0.5195466876029968
9 100 0.35312550213644905
9 200 0.2823109264614542
9 300 0.2769453145652499
9 400 0.24498387537462663
9 500 0.2406007096795573
9 600 0.2472198012789726
9 700 0.252721250818753
9 800 0.2511034420010402
9 900 0.2629918112262202
9 1000 0.2396051551314457
9 1100 0.2516792142115242
9 1200 0.2524378327041429
9 1300 0.25405914580223715
9 1400 0.2516402594729332
9 1500 0.2741030178301422
9 1600 0.2838048254810858
9 1700 0.2658155128435316
9 1800 0.25687501129014534
9 1900 0.23595483069680848
9 2000 0.2386889940183519
9 2100 0.2714986891508396

Train Epoch: 9 Average training loss: 0.2663 

10 0 0.08247004449367523
10 100 0.19200850476860665
10 200 0.23641177206387884
10 300 0.23471721082414443
10 400 0.25824167030246353
10 500 0.28979012564119605
10 600 0.24890678000875507
10 700 0.25539290729395836
10 800 0.24601532701793416
10 900 0.26999696808896323
10 1000 0.27250325327670166
10 1100 0.27061808480748195
10 1200 0.2665189221817015
10 1300 0.24905569496342186
10 1400 0.2245183862281799
10 1500 0.23639357148049367
10 1600 0.25069360837202026
10 1700 0.23967601664810043
10 1800 0.24084710275571272
10 1900 0.2570599918868458
10 2000 0.25688021722323967
10 2100 0.2592296041762082

Train Epoch: 10 Average training loss: 0.2450 

11 0 0.11738099902868271
11 100 0.19886226454790984
11 200 0.23921283474898944
11 300 0.2516810389097661
11 400 0.27912559377865126
11 500 0.2567670954848894
11 600 0.24970595370613738
11 700 0.23082526360690556
11 800 0.2486202848493255
11 900 0.24809729990014875
11 1000 0.24120728443135658
11 1100 0.2618550854734183
11 1200 0.2547102337274388
11 1300 0.26142521922617695
11 1400 0.2590275273343875
11 1500 0.2368832870099642
11 1600 0.2314732843927128
11 1700 0.2387567148033218
11 1800 0.24327802763028608
11 1900 0.26810634634685065
11 2000 0.2564640552394242
11 2100 0.2453137788603749

Train Epoch: 11 Average training loss: 0.2449 

12 0 0.1360013633966446
12 100 0.20405420547393976
12 200 0.2364545880800062
12 300 0.2425043855885631
12 400 0.2604189303894549
12 500 0.2515341414106989
12 600 0.23897792237754797
12 700 0.25380232946937026
12 800 0.26182066550336486
12 900 0.24510920484654558
12 1000 0.24550813155515136
12 1100 0.2649535443264091
12 1200 0.23654503538747507
12 1300 0.2554879006763547
12 1400 0.2666299238128378
12 1500 0.27275126730329163
12 1600 0.23626846169550303
12 1700 0.23966451616771295
12 1800 0.24169759835477553
12 1900 0.2568276293444035
12 2000 0.23684359800561997
12 2100 0.2368627050077984

Train Epoch: 12 Average training loss: 0.2461 

13 0 0.45332932472229004
13 100 0.30626724523012955
13 200 0.2604756558962455
13 300 0.2500682640659091
13 400 0.2680323367661787
13 500 0.26460637400531556
13 600 0.24268551287760776
13 700 0.24310856175979312
13 800 0.2420572896071557
13 900 0.24134565753351983
13 1000 0.2506134104293183
13 1100 0.25003976962671515
13 1200 0.27307130051829265
13 1300 0.2576986571916192
13 1400 0.23640189326657055
13 1500 0.24710555310033253
13 1600 0.2484657552976524
13 1700 0.25184472716741385
13 1800 0.2633120032171591
13 1900 0.25662449249696684
13 2000 0.24640855834950876
13 2100 0.24478675398018576

Train Epoch: 13 Average training loss: 0.2583 

14 0 0.0826331153512001
14 100 0.1754536603108716
14 200 0.23349324381992662
14 300 0.24368675913261006
14 400 0.23058116530974138
14 500 0.2419176393451391
14 600 0.25625825469735786
14 700 0.2562530448438744
14 800 0.2532733013309134
14 900 0.2342036755104729
14 1000 0.24377674353094303
14 1100 0.2548177289074845
14 1200 0.2584510107722415
14 1300 0.24370025381348737
14 1400 0.2482632553539756
14 1500 0.2337643433330488
14 1600 0.2444409499130449
14 1700 0.23548955190404605
14 1800 0.2474286834002798
14 1900 0.24300605815586335
14 2000 0.2431016792533684
14 2100 0.2403002421383721

Train Epoch: 14 Average training loss: 0.2408 

15 0 0.10929779708385468
15 100 0.19867943107447247
15 200 0.21148923915623669
15 300 0.2607086332870786
15 400 0.263024971574969
15 500 0.25007836132955524
15 600 0.24592536735680814
15 700 0.23682468022640232
15 800 0.23638300486094196
15 900 0.2378691859310956
15 1000 0.23363517573126394
15 1100 0.2674581929835308
15 1200 0.26804786397063673
15 1300 0.2511575360579963
15 1400 0.24551252966379825
15 1500 0.23277428056471577
15 1600 0.23852099145528696
15 1700 0.25733849505154366
15 1800 0.25936519339276837
15 1900 0.2791569793198938
15 2000 0.2554277772771706
15 2100 0.25018648237311764

Train Epoch: 15 Average training loss: 0.2424 

16 0 0.4389488995075226
16 100 0.29839783148392574
16 200 0.2713442700946993
16 300 0.2603882134019086
16 400 0.229622957484372
16 500 0.2318796455872718
16 600 0.22331963956330916
16 700 0.2266285965808694
16 800 0.23702221776654686
16 900 0.26014380700183093
16 1000 0.2525104920280386
16 1100 0.22835878426159945
16 1200 0.2348405082831603
16 1300 0.24623511971916454
16 1400 0.25635119421982894
16 1500 0.25349745770156307
16 1600 0.2832807651308085
16 1700 0.27022528869989115
16 1800 0.2511810784806271
16 1900 0.25331929650157314
16 2000 0.2653066062778515
16 2100 0.2544374731603003

Train Epoch: 16 Average training loss: 0.2536 

17 0 0.2786529064178467
17 100 0.24962386425691419
17 200 0.23418239999572193
17 300 0.23773957519552946
17 400 0.2282864009046627
17 500 0.23800638757556505
17 600 0.24561574591066326
17 700 0.24442456848420027
17 800 0.23883797741512502
17 900 0.25558959080234944
17 1000 0.24635040179193032
17 1100 0.24011826724263857
17 1200 0.2532765140155298
17 1300 0.2324270965205297
17 1400 0.245627384562661
17 1500 0.23179230146749527
17 1600 0.24712575094117734
17 1700 0.23702140459969337
17 1800 0.23887623800308147
17 1900 0.2448390244691531
17 2000 0.2596978379348466
17 2100 0.2602673884326484

Train Epoch: 17 Average training loss: 0.2462 

18 0 0.04625171795487404
18 100 0.1508917458252657
18 200 0.23022653750982428
18 300 0.22656952147747805
18 400 0.2559337868366279
18 500 0.2596180465529497
18 600 0.2219398504078508
18 700 0.2555015733426399
18 800 0.2470523496283091
18 900 0.25049056744234255
18 1000 0.2645030712597366
18 1100 0.2596953949885134
18 1200 0.22716431711272797
18 1300 0.2357852935036355
18 1400 0.2414507022633799
18 1500 0.24923654972428566
18 1600 0.2460324543515237
18 1700 0.251822535583583
18 1800 0.25426363590900025
18 1900 0.2480382860233503
18 2000 0.24473210421867214
18 2100 0.24497445516990227

Train Epoch: 18 Average training loss: 0.2374 

19 0 0.062401752918958664
19 100 0.18236571180491992
19 200 0.22883179411612856
19 300 0.21945408593611784
19 400 0.23371681718035292
19 500 0.25025545353180867
19 600 0.26031596780217714
19 700 0.24405885849557143
19 800 0.25185184007639094
19 900 0.2584731437337434
19 1000 0.2555943953681845
19 1100 0.25700540803979005
19 1200 0.24842705816023994
19 1300 0.2217560944324973
19 1400 0.23834516043001336
19 1500 0.2247066049929136
19 1600 0.2425314915031562
19 1700 0.24591662965217
19 1800 0.24554822179437397
19 1900 0.23705084649678987
19 2000 0.2365228814642633
19 2100 0.23647637637871954

Train Epoch: 19 Average training loss: 0.2391 

20 0 0.13020560145378113
20 100 0.2171343969069381
20 200 0.23737829977187927
20 300 0.23214667554398
20 400 0.2453285669212267
20 500 0.21576032085061136
20 600 0.23263910869000295
20 700 0.24165474190032324
20 800 0.26214096665962594
20 900 0.24514342715664075
20 1000 0.24860610549031734
20 1100 0.2319148978029886
20 1200 0.23674559914942617
20 1300 0.24629737216580735
20 1400 0.23820458582029136
20 1500 0.24697147796638594
20 1600 0.22595667621633453
20 1700 0.24520600369806927
20 1800 0.2713234268591754
20 1900 0.2528928756676103
20 2000 0.26509444828894796
20 2100 0.24050494260480232

Train Epoch: 20 Average training loss: 0.2396 

21 0 0.05556442216038704
21 100 0.178905422273618
21 200 0.23486988236939552
21 300 0.24826273517708794
21 400 0.23699502967642241
21 500 0.22331150033636318
21 600 0.21833178598104222
21 700 0.2346370209828552
21 800 0.24275666270666618
21 900 0.24042694734354983
21 1000 0.25436377750047356
21 1100 0.25922947276363967
21 1200 0.2563099302539416
21 1300 0.2708666979347384
21 1400 0.24263860627649694
21 1500 0.2400577211510311
21 1600 0.22514404672212074
21 1700 0.23745623208239378
21 1800 0.24315944604419276
21 1900 0.23804468808522372
21 2000 0.2332327081530608
21 2100 0.2452757504565195

Train Epoch: 21 Average training loss: 0.2335 

22 0 0.0541100800037384
22 100 0.15669420965084896
22 200 0.22380386908017927
22 300 0.22426146704603248
22 400 0.22814273201586194
22 500 0.228960936215351
22 600 0.22560768367424977
22 700 0.2523554865820609
22 800 0.2521212131865875
22 900 0.247416848766554
22 1000 0.23385358988205565
22 1100 0.2640906404892905
22 1200 0.2882030786452313
22 1300 0.2426787921129102
22 1400 0.24672141722762655
22 1500 0.25962974407320694
22 1600 0.2411052729076775
22 1700 0.2504970533016312
22 1800 0.24571201719999297
22 1900 0.2540157646779259
22 2000 0.25830840255603055
22 2100 0.26448048619602693

Train Epoch: 22 Average training loss: 0.2374 

23 0 0.6306197643280029
23 100 0.40173629014110185
23 200 0.29955130860885537
23 300 0.26328492398194975
23 400 0.22856446430738167
23 500 0.2606961365391743
23 600 0.23361737240098163
23 700 0.2396096624327184
23 800 0.2366691690932273
23 900 0.22535436734871078
23 1000 0.21896221834886706
23 1100 0.2472393203192076
23 1200 0.25558099070111373
23 1300 0.2516147020841757
23 1400 0.26527188059903223
23 1500 0.24289603591498826
23 1600 0.24760888329672703
23 1700 0.2491627854226594
23 1800 0.25140053626528275
23 1900 0.23677862136142658
23 2000 0.2504364539220673
23 2100 0.25185152534682237

Train Epoch: 23 Average training loss: 0.2622 

24 0 0.0995725616812706
24 100 0.1858321222692854
24 200 0.2009849430850591
24 300 0.21826638981096946
24 400 0.2392216050233025
24 500 0.2399513593626998
24 600 0.24298519348113784
24 700 0.25002836376435295
24 800 0.2580169728814601
24 900 0.24271007296926123
24 1000 0.25851729467132506
24 1100 0.24592251034512483
24 1200 0.24298114597471523
24 1300 0.23621168310248397
24 1400 0.25709933533889107
24 1500 0.24439653405426448
24 1600 0.23762215099841952
24 1700 0.23394144473281592
24 1800 0.2389720422936574
24 1900 0.23983069376761373
24 2000 0.27104098368145696
24 2100 0.2553132643291834

Train Epoch: 24 Average training loss: 0.2356 

25 0 0.1684241145849228
25 100 0.2235501517560468
25 200 0.22494383512424138
25 300 0.25958775032842635
25 400 0.2429223158280429
25 500 0.23252187061869112
25 600 0.23071256226499684
25 700 0.24154233659120497
25 800 0.2617549147253833
25 900 0.2604465215702023
25 1000 0.24468793740939082
25 1100 0.2402460693137841
25 1200 0.23269196446024298
25 1300 0.2404725881780901
25 1400 0.26095233797279904
25 1500 0.2610753614966695
25 1600 0.24459519705761448
25 1700 0.23818070512377487
25 1800 0.24361375605939078
25 1900 0.23838590931998424
25 2000 0.2342761636889965
25 2100 0.24229610523566425

Train Epoch: 25 Average training loss: 0.2410 

26 0 0.10865123569965363
26 100 0.2028978211666884
26 200 0.2125839481229429
26 300 0.21269751691551117
26 400 0.21987578018038323
26 500 0.23770736970246767
26 600 0.23926017860008952
26 700 0.24284165625229046
26 800 0.27663901287585735
26 900 0.2531772056598274
26 1000 0.2593704861640309
26 1100 0.25320798939376593
26 1200 0.23708702089068992
26 1300 0.2387807891731391
26 1400 0.23640790942423734
26 1500 0.24600524457209583
26 1600 0.2425768369282809
26 1700 0.23253665084918623
26 1800 0.2514309575529036
26 1900 0.25781392848208495
26 2000 0.2619213118627202
26 2100 0.25325200821013644

Train Epoch: 26 Average training loss: 0.2375 

27 0 0.10450968146324158
27 100 0.2122250690019439
27 200 0.23523001580045955
27 300 0.23453698813012816
27 400 0.23999163331089043
27 500 0.24770042647542773
27 600 0.2522884541834108
27 700 0.2271522807953411
27 800 0.2397495310019726
27 900 0.23862534319511805
27 1000 0.2225404527929731
27 1100 0.25624691122957083
27 1200 0.252591690518037
27 1300 0.24910059591901199
27 1400 0.2543353177529079
27 1500 0.23315027880006722
27 1600 0.24186567658382477
27 1700 0.24130689445956882
27 1800 0.246726864629842
27 1900 0.24240498211726194
27 2000 0.23646133449497753
27 2100 0.25239471058415475

Train Epoch: 27 Average training loss: 0.2372 

28 0 0.2477365881204605
28 100 0.23466521126178894
28 200 0.24430909202765705
28 300 0.25214103288230716
28 400 0.24864849142124257
28 500 0.2446045671163669
28 600 0.23980612567247933
28 700 0.239085141473565
28 800 0.24389268751349893
28 900 0.2294438289817433
28 1000 0.24117319997701608
28 1100 0.25182529586049685
28 1200 0.2414348644099145
28 1300 0.24320426681736157
28 1400 0.23504637807340903
28 1500 0.22248449689728092
28 1600 0.2486702452893469
28 1700 0.22124520814795184
28 1800 0.22763416430457586
28 1900 0.2423322086834865
28 2000 0.2554590923996072
28 2100 0.24937777948176543

Train Epoch: 28 Average training loss: 0.2414 

29 0 0.09457007050514221
29 100 0.19982954979242518
29 200 0.23349567894933068
29 300 0.2393636161072148
29 400 0.25178744802384184
29 500 0.23656840756119918
29 600 0.24805094155433574
29 700 0.2302847385629681
29 800 0.22711360851742513
29 900 0.24410788975040168
29 1000 0.2513649234208768
29 1100 0.26585027862671146
29 1200 0.2309808134970279
29 1300 0.2114530935411866
29 1400 0.2452760873764973
29 1500 0.2401237899573661
29 1600 0.23915294258838038
29 1700 0.24765063433655002
29 1800 0.26565801765308733
29 1900 0.23945672158253306
29 2000 0.2631104988852447
29 2100 0.24567113018463424

Train Epoch: 29 Average training loss: 0.2365 

30 0 0.10055288672447205
30 100 0.1916894750390963
30 200 0.22776608773671464
30 300 0.23728159879988425
30 400 0.2320433289475797
30 500 0.23847891316712824
30 600 0.25302563164412195
30 700 0.24408797025404072
30 800 0.2323499903077943
30 900 0.23353887945708324
30 1000 0.2380808410772045
30 1100 0.23852715591806503
30 1200 0.2364399083964361
30 1300 0.22672016930421293
30 1400 0.22025742473646476
30 1500 0.22771607149340237
30 1600 0.23960387378830866
30 1700 0.24419531267808597
30 1800 0.23653905402232223
30 1900 0.2280629765806691
30 2000 0.23282112649348802
30 2100 0.27032553299346246

Train Epoch: 30 Average training loss: 0.2329 

