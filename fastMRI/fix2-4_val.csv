1 0 0.20435862243175507
1 100 0.2520702011820196
1 200 0.30224549253935146
1 300 0.30540709960814955
1 400 0.32400939670457585
1 500 0.3192504471387846
1 600 0.3013226395945081
1 700 0.3261863777983992
1 800 0.31236312240886
1 900 0.3158913958105962
1 1000 0.30824956245433516
1 1100 0.3339278556109664
1 1200 0.3195148584093386
1 1300 0.29393569673022146
1 1400 0.30122170767888107
1 1500 0.30990192948366685
1 1600 0.3005356210643816
1 1700 0.30717210330461237
1 1800 0.2987571488253535
1 1900 0.29807568049374955
1 2000 0.3225474416919435
1 2100 0.3233055351190358

Train Epoch: 1 Average training loss: 0.3063 

2 0 0.1597936451435089
2 100 0.2490480886223069
2 200 0.2977921994376808
2 300 0.31786516839376905
2 400 0.2993733228261873
2 500 0.3231786933590087
2 600 0.31262887798595945
2 700 0.34089354734200233
2 800 0.33698054724921067
2 900 0.30571191799186304
2 1000 0.3009434733988994
2 1100 0.31527871404577423
2 1200 0.294935146795343
2 1300 0.3019036631395993
2 1400 0.3109251226502839
2 1500 0.3008578292362152
2 1600 0.30570488841232984
2 1700 0.27425358015728696
2 1800 0.29760534588001103
2 1900 0.28260728202708774
2 2000 0.2958721080640399
2 2100 0.3096326666196382

Train Epoch: 2 Average training loss: 0.3026 

3 0 0.4844823479652405
3 100 0.38496116005201925
3 200 0.3365452376951034
3 300 0.3077153440883129
3 400 0.3176836750918901
3 500 0.3266995614275865
3 600 0.32288642494203434
3 700 0.31693166663016775
3 800 0.3062555188470286
3 900 0.28382333479670224
3 1000 0.2961036670894078
3 1100 0.3026171606756146
3 1200 0.3227535787598076
3 1300 0.31218458938990523
3 1400 0.3184300999914723
3 1500 0.3202940985644776
3 1600 0.3426985485913189
3 1700 0.34167086200445157
3 1800 0.31990180914941796
3 1900 0.2989531367457878
3 2000 0.2933021267231424
3 2100 0.29677167627206846

Train Epoch: 3 Average training loss: 0.3184 

4 0 0.1488567590713501
4 100 0.2470669205829266
4 200 0.2823340675249993
4 300 0.3061980463845667
4 400 0.28764390112268073
4 500 0.28893034190846373
4 600 0.28472457693557146
4 700 0.28849040270364523
4 800 0.29347867127950006
4 900 0.317284506216195
4 1000 0.3135070857164754
4 1100 0.3244048181601139
4 1200 0.31151702705632195
4 1300 0.2994585354493707
4 1400 0.3185847303864246
4 1500 0.3398791114810813
4 1600 0.3147835280179976
4 1700 0.3135335903271947
4 1800 0.29641764685575084
4 1900 0.28127945691888967
4 2000 0.307400835800502
4 2100 0.33925736316997723

Train Epoch: 4 Average training loss: 0.3006 

5 0 0.1444326639175415
5 100 0.2627077761376285
5 200 0.29725430775088774
5 300 0.31133303487376723
5 400 0.2952046460916984
5 500 0.3151809911948132
5 600 0.32703438185952455
5 700 0.31501716982978173
5 800 0.30389356973768616
5 900 0.3010584066756731
5 1000 0.3006426593172354
5 1100 0.30931822237841666
5 1200 0.2844494925357585
5 1300 0.3061650027417867
5 1400 0.321110683610909
5 1500 0.3163830493468203
5 1600 0.2915018933012709
5 1700 0.3082572976883478
5 1800 0.3027674690086507
5 1900 0.32151068100211877
5 2000 0.2968918300313724
5 2100 0.31487299577507105

Train Epoch: 5 Average training loss: 0.3018 

6 0 0.4826153516769409
6 100 0.3815712822544415
6 200 0.33449373688977574
6 300 0.3490895739909527
6 400 0.3239436805478448
6 500 0.3148304431581508
6 600 0.3245337971325235
6 700 0.318554966787323
6 800 0.316783896593847
6 900 0.3175729440411226
6 1000 0.29424129759580786
6 1100 0.2822060805666801
6 1200 0.2771040372376411
6 1300 0.29822734315378907
6 1400 0.28116409647951845
6 1500 0.3119973144943964
6 1600 0.3112487885977312
6 1700 0.292630738945668
6 1800 0.29747834268253076
6 1900 0.30667030616072516
6 2000 0.30543833154731925
6 2100 0.2929128384302461

Train Epoch: 6 Average training loss: 0.3169 

7 0 0.2676340341567993
7 100 0.3183053070901179
7 200 0.32929645792522194
7 300 0.3209248538949719
7 400 0.3264667732080846
7 500 0.3263253187312213
7 600 0.2955687378688474
7 700 0.31035420851257955
7 800 0.3095332064837515
7 900 0.31003668663321254
7 1000 0.3028913175218098
7 1100 0.29227946141009503
7 1200 0.30947772922234607
7 1300 0.317897807791119
7 1400 0.30357715385044376
7 1500 0.30000145790231
7 1600 0.30643981978667423
7 1700 0.2812744843916913
7 1800 0.28213126478896644
7 1900 0.30562028503251965
7 2000 0.3165613951059626
7 2100 0.29363507481111556

Train Epoch: 7 Average training loss: 0.3072 

8 0 0.08354302495718002
8 100 0.22220985593411696
8 200 0.28306357757623885
8 300 0.2746445001793759
8 400 0.28084832967085255
8 500 0.29625876019974945
8 600 0.2942521095863056
8 700 0.29721036901431536
8 800 0.28594848554323093
8 900 0.29120259996253844
8 1000 0.29866005520685973
8 1100 0.29585067212720206
8 1200 0.31389922860927055
8 1300 0.32705989301108224
8 1400 0.32967614386938027
8 1500 0.29900929575262114
8 1600 0.31238512534450524
8 1700 0.3142798199831048
8 1800 0.31001039340448977
8 1900 0.33192794196609254
8 2000 0.31905236198745685
8 2100 0.3138190351021261

Train Epoch: 8 Average training loss: 0.2965 

9 0 0.3095290958881378
9 100 0.3292007672260002
9 200 0.2960452463915778
9 300 0.31641266381059746
9 400 0.30996346615316817
9 500 0.2993504691306409
9 600 0.3146594678232881
9 700 0.320014936036195
9 800 0.2888926979474691
9 900 0.29368237845857653
9 1000 0.2993908717144861
9 1100 0.3149515087178687
9 1200 0.31608593200108037
9 1300 0.3010244023577585
9 1400 0.2817791964962411
9 1500 0.3248854617484613
9 1600 0.2983534419027424
9 1700 0.29241747761851716
9 1800 0.30697285316513295
9 1900 0.32007262059065333
9 2000 0.2828844869033473
9 2100 0.29205996196348954

Train Epoch: 9 Average training loss: 0.3058 

10 0 0.2327115684747696
10 100 0.2677650490109958
10 200 0.3146121928646961
10 300 0.28933792343639747
10 400 0.31354634620627825
10 500 0.3067189428655139
10 600 0.29888210534822807
10 700 0.31211800769906123
10 800 0.295473322669615
10 900 0.30152882854150825
10 1000 0.3036857777788196
10 1100 0.3185489955562466
10 1200 0.292407429295567
10 1300 0.2973969321133361
10 1400 0.2914763815671245
10 1500 0.29860016655436106
10 1600 0.3004162224196108
10 1700 0.3084578210589992
10 1800 0.33637903195079893
10 1900 0.34776491034783813
10 2000 0.31929813261660067
10 2100 0.3010973121739777

Train Epoch: 10 Average training loss: 0.3047 

11 0 0.8829914927482605
11 100 0.5198386806691151
11 200 0.38388707251695403
11 300 0.3156711348845915
11 400 0.31415199165621727
11 500 0.3150255294345698
11 600 0.29115030275387743
11 700 0.32133513678275416
11 800 0.30188947096599106
11 900 0.27873757362349216
11 1000 0.32000880296918316
11 1100 0.30302900434369445
11 1200 0.31398145359465446
11 1300 0.3143071875749009
11 1400 0.3134246903065422
11 1500 0.3294965178219706
11 1600 0.3031779507673845
11 1700 0.3099676961272572
11 1800 0.3025158265704027
11 1900 0.30817285169368913
11 2000 0.3365820280500098
11 2100 0.30151517851916326

Train Epoch: 11 Average training loss: 0.3333 

12 0 0.6580272912979126
12 100 0.43417432430766856
12 200 0.35910983009539593
12 300 0.29841707877801804
12 400 0.30248962149652486
12 500 0.3395065895230151
12 600 0.31996505983086365
12 700 0.33453413414079064
12 800 0.31964379216937044
12 900 0.29580536054071743
12 1000 0.287457305522078
12 1100 0.32241866745123443
12 1200 0.29412296530222903
12 1300 0.28862701509079625
12 1400 0.32230227759669894
12 1500 0.32753524547912066
12 1600 0.29467022744666055
12 1700 0.3106824581913492
12 1800 0.3306749796268628
12 1900 0.2960360699036189
12 2000 0.2863118763709859
12 2100 0.29967251979915466

Train Epoch: 12 Average training loss: 0.3231 

13 0 0.3123684227466583
13 100 0.3286144487740242
13 200 0.3103202593323443
13 300 0.3067820938674909
13 400 0.29159341253815846
13 500 0.30163861743678183
13 600 0.3052110971249241
13 700 0.308027517361355
13 800 0.30479447601510024
13 900 0.2987848986666016
13 1000 0.3148488146577798
13 1100 0.30707413186801796
13 1200 0.3061405450375202
13 1300 0.2921542951212297
13 1400 0.32242123878203516
13 1500 0.3103377134412829
13 1600 0.3113238911605904
13 1700 0.29404929441535
13 1800 0.3159641019315357
13 1900 0.294417255103102
13 2000 0.3065764036439304
13 2100 0.31630755617540285

Train Epoch: 13 Average training loss: 0.3085 

14 0 0.6610392928123474
14 100 0.4470766777322977
14 200 0.3441846985261164
14 300 0.33168511798616973
14 400 0.319162325779497
14 500 0.3201847004459841
14 600 0.29841973529454424
14 700 0.3063425677907395
14 800 0.31993626835335653
14 900 0.30546073699319426
14 1000 0.28573463735554977
14 1100 0.32480385206227064
14 1200 0.3075764676030533
14 1300 0.3108974667157357
14 1400 0.30713756749878335
14 1500 0.32007115437607897
14 1600 0.28861118408302916
14 1700 0.29607760525694027
14 1800 0.31449166658301864
14 1900 0.30643946031000757
14 2000 0.29869290344146254
14 2100 0.29526913168337976

Train Epoch: 14 Average training loss: 0.3221 

15 0 0.07992766052484512
15 100 0.21781405631031395
15 200 0.2829029350919698
15 300 0.29424944416682713
15 400 0.2986804059144194
15 500 0.2841684814261478
15 600 0.31586335332977283
15 700 0.3107863884788169
15 800 0.3222111668233091
15 900 0.33005716379934535
15 1000 0.29572303529592053
15 1100 0.2999045421532214
15 1200 0.2797678849323563
15 1300 0.3077031537741272
15 1400 0.30116740892046584
15 1500 0.31141683557964395
15 1600 0.30180391337000734
15 1700 0.31055516269301436
15 1800 0.3040836826527433
15 1900 0.3006156408204627
15 2000 0.29275817758189987
15 2100 0.2959879226241302

Train Epoch: 15 Average training loss: 0.2953 

16 0 0.08496226370334625
16 100 0.2196706733835481
16 200 0.2603711396535332
16 300 0.32044429974485256
16 400 0.30769280869342897
16 500 0.2971870972574343
16 600 0.2942704002249976
16 700 0.3211143955711913
16 800 0.3033484035766972
16 900 0.2712294368983842
16 1000 0.27824753688189086
16 1100 0.2775165203720743
16 1200 0.27115703192416646
16 1300 0.3069255529040173
16 1400 0.3363131732798139
16 1500 0.33470803946747374
16 1600 0.32533022913968807
16 1700 0.326323537090443
16 1800 0.30907434601456313
16 1900 0.3114522145729271
16 2000 0.29956314352106134
16 2100 0.3264308404731365

Train Epoch: 16 Average training loss: 0.2943 

17 0 0.08766307681798935
17 100 0.24319391924735506
17 200 0.2813789595145107
17 300 0.3042407515125332
17 400 0.2844478474430899
17 500 0.2880479416442177
17 600 0.3136631451248258
17 700 0.3031487890294785
17 800 0.2952771712338683
17 900 0.30239046243599343
17 1000 0.3098442546097902
17 1100 0.31034861447487644
17 1200 0.31432292966737285
17 1300 0.2950004101752803
17 1400 0.2739742740872923
17 1500 0.2965331690686828
17 1600 0.3073777888950508
17 1700 0.3014676392132986
17 1800 0.3261281908941813
17 1900 0.3365414214070145
17 2000 0.31125794608072727
17 2100 0.31405095684584994

Train Epoch: 17 Average training loss: 0.2956 

18 0 0.08463138341903687
18 100 0.22079763704059482
18 200 0.27811160725463563
18 300 0.3039376661573826
18 400 0.3236823732878832
18 500 0.2955304547710354
18 600 0.3018790869938426
18 700 0.31703838194431544
18 800 0.2993675022374448
18 900 0.3275525898973534
18 1000 0.32070547715907155
18 1100 0.32497029468680627
18 1200 0.30425934010012656
18 1300 0.299375492857654
18 1400 0.2952679101706816
18 1500 0.29548860178521763
18 1600 0.29904662905836976
18 1700 0.2841610154378424
18 1800 0.26877908556897934
18 1900 0.2947673840920146
18 2000 0.3035161754772448
18 2100 0.3078022578994534

Train Epoch: 18 Average training loss: 0.2951 

19 0 0.25802814960479736
19 100 0.2807332700422543
19 200 0.2982691733729777
19 300 0.329136037050474
19 400 0.3067223787104758
19 500 0.28683879946778146
19 600 0.27785609209332723
19 700 0.30223082895995473
19 800 0.28910518015853093
19 900 0.29580004878668564
19 1000 0.2966882075991244
19 1100 0.3073823488987797
19 1200 0.32168042132137226
19 1300 0.3214842643040124
19 1400 0.2990878269355597
19 1500 0.3111842003587152
19 1600 0.3244294580453093
19 1700 0.3149254781738696
19 1800 0.2963589454244411
19 1900 0.2881547611600409
19 2000 0.2949164872021826
19 2100 0.30289747410807943

Train Epoch: 19 Average training loss: 0.3026 

20 0 0.1987433284521103
20 100 0.28214037211944815
20 200 0.2838471550272043
20 300 0.3101694681531635
20 400 0.31310151889135535
20 500 0.3460673684694912
20 600 0.3254853617946806
20 700 0.2956198395386392
20 800 0.30526276189652307
20 900 0.30863993096408215
20 1000 0.3227634721948866
20 1100 0.31318205728389575
20 1200 0.2984251754344674
20 1300 0.3132329804472917
20 1400 0.30095348596082633
20 1500 0.3241634693922626
20 1600 0.2933171491647239
20 1700 0.28559130988184733
20 1800 0.307035277749387
20 1900 0.29089218423894303
20 2000 0.3068138602761551
20 2100 0.29036755472792025

Train Epoch: 20 Average training loss: 0.3031 

21 0 0.3464306592941284
21 100 0.331013950875694
21 200 0.3180455771799929
21 300 0.3210857854085732
21 400 0.3009554910897255
21 500 0.3293595578017222
21 600 0.30910968163992886
21 700 0.31941655195340557
21 800 0.32358031767362444
21 900 0.31297964232931097
21 1000 0.30311485830586515
21 1100 0.3020666027283317
21 1200 0.3085898083916356
21 1300 0.2995685237059719
21 1400 0.28586050250447437
21 1500 0.2887457580119952
21 1600 0.31977962267630083
21 1700 0.3046861427720833
21 1800 0.3028675053118137
21 1900 0.29557702843679307
21 2000 0.29608382764606145
21 2100 0.30275535721215063

Train Epoch: 21 Average training loss: 0.3074 

22 0 0.5386126041412354
22 100 0.38185140042271554
22 200 0.3284507367315918
22 300 0.30817575428453114
22 400 0.3035477019444505
22 500 0.3124482943462118
22 600 0.31401467110156045
22 700 0.3040604391659882
22 800 0.29897142786740605
22 900 0.31265530763415184
22 1000 0.29958003713626014
22 1100 0.3269388824647616
22 1200 0.32027526509857446
22 1300 0.3073201500345479
22 1400 0.31626118976859796
22 1500 0.2773414029054282
22 1600 0.2991976420000196
22 1700 0.3223437407696786
22 1800 0.3230809014423388
22 1900 0.32026822493054785
22 2000 0.2948380685342863
22 2100 0.29918170047920384

Train Epoch: 22 Average training loss: 0.3175 

23 0 0.7361302375793457
23 100 0.47392041553806846
23 200 0.37660195379740957
23 300 0.33173577521979214
23 400 0.3482208372752544
23 500 0.31197046013063806
23 600 0.3212710155606621
23 700 0.310234867915306
23 800 0.29812817791942714
23 900 0.28992035627836804
23 1000 0.2894057798295265
23 1100 0.2885855068895649
23 1200 0.28897324337190844
23 1300 0.2839042247459293
23 1400 0.2751107907888518
23 1500 0.27139440531186493
23 1600 0.3004120122488353
23 1700 0.3037098495966755
23 1800 0.2753998723192601
23 1900 0.2826752211684562
23 2000 0.34096153254685796
23 2100 0.34847499478991173

Train Epoch: 23 Average training loss: 0.3233 

24 0 0.2816847860813141
24 100 0.2943401210749996
24 200 0.3110854689983782
24 300 0.29486906261226914
24 400 0.28444881122534676
24 500 0.2998627146571321
24 600 0.31813471929991144
24 700 0.3039995855653684
24 800 0.30591254095494186
24 900 0.2826075971090556
24 1000 0.3341875769653987
24 1100 0.3115749046260344
24 1200 0.2878042813757529
24 1300 0.3037284622433537
24 1400 0.31557464460899004
24 1500 0.3275109843578959
24 1600 0.30178397588975003
24 1700 0.3083099921341419
24 1800 0.29281410639067057
24 1900 0.30388649456438077
24 2000 0.2918418563114197
24 2100 0.3010452822982847

Train Epoch: 24 Average training loss: 0.3036 

25 0 0.30801618099212646
25 100 0.2951571483503283
25 200 0.29029556393177985
25 300 0.2881203205760727
25 400 0.28010551453192484
25 500 0.2973742809566329
25 600 0.30314317642219946
25 700 0.31689415260301984
25 800 0.3240690637234628
25 900 0.3046595619700868
25 1000 0.2986083017217452
25 1100 0.32254334954957053
25 1200 0.3020398605673003
25 1300 0.32040504180730717
25 1400 0.32269592454360574
25 1500 0.29801424647810615
25 1600 0.3019806124161128
25 1700 0.33570083396648415
25 1800 0.30356630984931704
25 1900 0.2926627264946175
25 2000 0.30023469675287606
25 2100 0.29360051567370765

Train Epoch: 25 Average training loss: 0.3043 

26 0 0.09345734864473343
26 100 0.21125927642661976
26 200 0.2659115673442131
26 300 0.30667830425321796
26 400 0.30989016899275146
26 500 0.2988466055180289
26 600 0.30219263721991535
26 700 0.30212068817596127
26 800 0.31514407940009215
26 900 0.3204283603208915
26 1000 0.30328512972210725
26 1100 0.31495741952164674
26 1200 0.29759855569356414
26 1300 0.2857449140951505
26 1400 0.2887542781187621
26 1500 0.2766931330286469
26 1600 0.3090725710672122
26 1700 0.28824985621711924
26 1800 0.33059576878383656
26 1900 0.29299225095327186
26 2000 0.3031292284625706
26 2100 0.3195462137217409

Train Epoch: 26 Average training loss: 0.2939 

27 0 0.7304803729057312
27 100 0.4488096269451395
27 200 0.3529007700748473
27 300 0.3123112862948966
27 400 0.310172525658901
27 500 0.3240909221065404
27 600 0.3315509903117826
27 700 0.32027642481278934
27 800 0.301943964846275
27 900 0.3061574648336005
27 1000 0.3098810109485125
27 1100 0.3341403557576649
27 1200 0.31793544956563524
27 1300 0.313631825761932
27 1400 0.30708489535993494
27 1500 0.2941167573395442
27 1600 0.2981956738022699
27 1700 0.32927138326678596
27 1800 0.29714478894112534
27 1900 0.30052011591848626
27 2000 0.2904467972849217
27 2100 0.2884026082851836

Train Epoch: 27 Average training loss: 0.3254 

28 0 0.46729785203933716
28 100 0.3596792212162151
28 200 0.30423852879479896
28 300 0.3093976866748666
28 400 0.32268619068009324
28 500 0.3150123275854378
28 600 0.29155732796219197
28 700 0.2778124272870829
28 800 0.3039638383093056
28 900 0.3218554377048866
28 1000 0.30791719246849736
28 1100 0.29464912025739537
28 1200 0.2904515563950509
28 1300 0.2974997787545326
28 1400 0.3075749740441112
28 1500 0.313308683030631
28 1600 0.30140510100813334
28 1700 0.3273175652183197
28 1800 0.30062890566609785
28 1900 0.31546324899403166
28 2000 0.30190509962673795
28 2100 0.3193416336522553

Train Epoch: 28 Average training loss: 0.3112 

29 0 0.1670362502336502
29 100 0.2513108393455975
29 200 0.3068180754891031
29 300 0.30257724499308375
29 400 0.3110175009899278
29 500 0.32948088224667793
29 600 0.326253093220656
29 700 0.3154039415881829
29 800 0.30534967399971913
29 900 0.3140290408289143
29 1000 0.2983353051289924
29 1100 0.30559712450469134
29 1200 0.30834368148273755
29 1300 0.27160895025089876
29 1400 0.2711202926692819
29 1500 0.2899823742007581
29 1600 0.30704105894029327
29 1700 0.30248364360631064
29 1800 0.2954576270940662
29 1900 0.29854869254185434
29 2000 0.32740334583510466
29 2100 0.3004587267763705

Train Epoch: 29 Average training loss: 0.2980 

30 0 0.27974656224250793
30 100 0.3081535057651733
30 200 0.2992351837410857
30 300 0.2999481464601152
30 400 0.2705066623151661
30 500 0.3071169902004867
30 600 0.31229510997964965
30 700 0.30045942541555165
30 800 0.2731975729105987
30 900 0.3056791729390323
30 1000 0.30425674423719
30 1100 0.30737352683627506
30 1200 0.30390365063844066
30 1300 0.32758540846646145
30 1400 0.29740542375744927
30 1500 0.3012580740611896
30 1600 0.34940414338651
30 1700 0.30288582755080473
30 1800 0.3110349076584569
30 1900 0.3088632731114012
30 2000 0.2886286623463232
30 2100 0.2973794344347668

Train Epoch: 30 Average training loss: 0.3033 

31 0 0.7512338161468506
31 100 0.4826420983898383
31 200 0.3896596844250361
31 300 0.36400305390791715
31 400 0.31007289144002315
31 500 0.30153622800572916
31 600 0.2963032516023828
31 700 0.29999053214743554
31 800 0.2979899655188923
31 900 0.2856509921028991
31 1000 0.2734495934318892
31 1100 0.27740635623196624
31 1200 0.3034791716392442
31 1300 0.3159902893919422
31 1400 0.31170810789680153
31 1500 0.31591076222616515
31 1600 0.31948767393124927
31 1700 0.3205473249763053
31 1800 0.29467082243575193
31 1900 0.3010743442493118
31 2000 0.2967254144515893
31 2100 0.27709795130263476

Train Epoch: 31 Average training loss: 0.3242 

32 0 0.12830211222171783
32 100 0.22637292557601768
32 200 0.2856302750894778
32 300 0.3167037379108798
32 400 0.3421630341536543
32 500 0.3287769513104734
32 600 0.30747278614193146
32 700 0.315443434333124
32 800 0.3133017729565401
32 900 0.280911193472397
32 1000 0.30517947946221347
32 1100 0.3106640368607458
32 1200 0.3138506044507458
32 1300 0.30735962254589716
32 1400 0.2993235388527606
32 1500 0.2903627100854088
32 1600 0.2843005878740907
32 1700 0.2733631964740135
32 1800 0.29540472917764554
32 1900 0.29823397076105085
32 2000 0.3056912073846547
32 2100 0.3006445884404566

Train Epoch: 32 Average training loss: 0.2955 

33 0 0.07229284942150116
33 100 0.191681094334957
33 200 0.25229701377832386
33 300 0.2935415959628109
33 400 0.27408332251358336
33 500 0.29403700076276384
33 600 0.32420410725090854
33 700 0.30442885118513885
33 800 0.3014036691263188
33 900 0.3046973496799284
33 1000 0.2865640277549445
33 1100 0.31179647218025336
33 1200 0.3045677264202354
33 1300 0.34070605118692
33 1400 0.301520427894683
33 1500 0.2923605212364882
33 1600 0.29619080369093564
33 1700 0.3206134424923215
33 1800 0.3223310683592962
33 1900 0.3097431855922488
33 2000 0.2933398020707989
33 2100 0.30628442225207825

Train Epoch: 33 Average training loss: 0.2914 

34 0 0.1765276938676834
34 100 0.24295192614909952
34 200 0.2954168675473942
34 300 0.308631603303236
34 400 0.28133903052018666
34 500 0.2879451394127611
34 600 0.29895795072948916
34 700 0.28285598714034577
34 800 0.2879839836744496
34 900 0.29341890235289736
34 1000 0.28897253636436504
34 1100 0.2872388313659298
34 1200 0.28693249059972026
34 1300 0.2838803260429507
34 1400 0.3211197115212563
34 1500 0.3128966632256943
34 1600 0.33452223888402394
34 1700 0.3071548804764624
34 1800 0.331889996454185
34 1900 0.30758717901467725
34 2000 0.3007163340623269
34 2100 0.3293523095651008

Train Epoch: 34 Average training loss: 0.2963 

35 0 0.21752382814884186
35 100 0.27016294732319573
35 200 0.28832203142625806
35 300 0.31104950974673484
35 400 0.29884354103244365
35 500 0.2935050520429553
35 600 0.30368076182258585
35 700 0.29558448235954365
35 800 0.3244916069598892
35 900 0.30548884991174907
35 1000 0.2953557944845018
35 1100 0.30561088173108214
35 1200 0.2911978431246862
35 1300 0.2922156589160062
35 1400 0.31474882834310947
35 1500 0.3327943333758001
35 1600 0.30618768560714893
35 1700 0.30682365157578784
35 1800 0.29628309395346747
35 1900 0.3061170711540576
35 2000 0.2985389473737285
35 2100 0.30867583831776546

Train Epoch: 35 Average training loss: 0.2989 

36 0 0.24610115587711334
36 100 0.30061983704077094
36 200 0.2883583836441039
36 300 0.2772101832182043
36 400 0.2847629450758361
36 500 0.2869338675030577
36 600 0.30765641022240237
36 700 0.3000844573504381
36 800 0.300938206140416
36 900 0.305391679689562
36 1000 0.3093269829682017
36 1100 0.29291954332528813
36 1200 0.3325527179094143
36 1300 0.30933273655010046
36 1400 0.2897307636621896
36 1500 0.30050695013653866
36 1600 0.2843108811802296
36 1700 0.2697260876231121
36 1800 0.3113104226795339
36 1900 0.3185869504854608
36 2000 0.3018672309602606
36 2100 0.30667928309638165

Train Epoch: 36 Average training loss: 0.2988 

37 0 0.1329171359539032
37 100 0.22706890370262808
37 200 0.2904664357320199
37 300 0.29987575960757745
37 400 0.2925672614053186
37 500 0.30371533534563855
37 600 0.2756081579291961
37 700 0.29020434116041144
37 800 0.293829930401325
37 900 0.2819420485809625
37 1000 0.2995768360298886
37 1100 0.30709925198048715
37 1200 0.3060065982710188
37 1300 0.3112055572464529
37 1400 0.322912084280605
37 1500 0.3157659280775316
37 1600 0.3099763530640704
37 1700 0.30284193637871515
37 1800 0.29898859187029797
37 1900 0.27724289572835065
37 2000 0.31603424791445317
37 2100 0.3391682994689621

Train Epoch: 37 Average training loss: 0.2932 

38 0 0.18393893539905548
38 100 0.2595797932333592
38 200 0.28684279155666126
38 300 0.30971339538015075
38 400 0.31843802741146177
38 500 0.2902389012139379
38 600 0.291157230528342
38 700 0.2911455730100263
38 800 0.289705598239219
38 900 0.2947817244578668
38 1000 0.27953216347422033
38 1100 0.3083088192322278
38 1200 0.28534426663116064
38 1300 0.28627422692885174
38 1400 0.2892737263447368
38 1500 0.30496745655356094
38 1600 0.28164708821282863
38 1700 0.3202892932488688
38 1800 0.30849399797454313
38 1900 0.30294883025433017
38 2000 0.2999424960147915
38 2100 0.3086257841142967

Train Epoch: 38 Average training loss: 0.2959 

39 0 0.9601320028305054
39 100 0.5475943172170897
39 200 0.4124655561291176
39 300 0.35712581756541223
39 400 0.292533972089037
39 500 0.28086860034398137
39 600 0.2882663953665598
39 700 0.31268390499827015
39 800 0.30497609661595826
39 900 0.3101865029878377
39 1000 0.2942236061094792
39 1100 0.30098660602366756
39 1200 0.29439763982056627
39 1300 0.2906503344352258
39 1400 0.3168033241397534
39 1500 0.289047174560172
39 1600 0.30856979365669357
39 1700 0.29619400038011323
39 1800 0.32291707257147323
39 1900 0.315588634307312
39 2000 0.3039411009035057
39 2100 0.2805968500232822

Train Epoch: 39 Average training loss: 0.3321 

40 0 0.26196855306625366
40 100 0.3157489862675209
40 200 0.30745959470643164
40 300 0.3102245807493397
40 400 0.2967679375352581
40 500 0.3032763771996078
40 600 0.29042853364506815
40 700 0.2977887679537691
40 800 0.2947232594455251
40 900 0.29895788686214964
40 1000 0.30996623641703996
40 1100 0.33121424402307487
40 1200 0.2924712798291763
40 1300 0.27423556106427577
40 1400 0.26663444392986646
40 1500 0.28732558416069054
40 1600 0.2839546222493728
40 1700 0.28371469502705254
40 1800 0.2932420664774625
40 1900 0.281376661525109
40 2000 0.30873518845197895
40 2100 0.3245810626603416

Train Epoch: 40 Average training loss: 0.2967 

41 0 0.6199315190315247
41 100 0.4291975967562607
41 200 0.31769074710558437
41 300 0.2933765890122573
41 400 0.28392856665372274
41 500 0.3190040705896736
41 600 0.3238814749230884
41 700 0.324221970326572
41 800 0.32365955989038003
41 900 0.3117518627979276
41 1000 0.30118733606765774
41 1100 0.30122425369242567
41 1200 0.2896759355584908
41 1300 0.30561908801286974
41 1400 0.3026953607848054
41 1500 0.2649991352552875
41 1600 0.28163532697336974
41 1700 0.2937429509942709
41 1800 0.2867101861137411
41 1900 0.31902888203242097
41 2000 0.31554922793712903
41 2100 0.2912589795064666

Train Epoch: 41 Average training loss: 0.3163 

42 0 0.28035882115364075
42 100 0.2928097479136293
42 200 0.28724744405083513
42 300 0.2880202871377908
42 400 0.30150128220553896
42 500 0.3047357983155644
42 600 0.315488729100026
42 700 0.31894508185161713
42 800 0.30208096124431694
42 900 0.27616130242868137
42 1000 0.3217163680793354
42 1100 0.3217440341729369
42 1200 0.2996293168850869
42 1300 0.3171063283194495
42 1400 0.30475027789351394
42 1500 0.3094965912031175
42 1600 0.29119168537211315
42 1700 0.3119963825067949
42 1800 0.29322559836299317
42 1900 0.31073221780208665
42 2000 0.3077698764162295
42 2100 0.2738424992075835

Train Epoch: 42 Average training loss: 0.3009 

43 0 0.1037508174777031
43 100 0.2632789712570258
43 200 0.2980777985393311
43 300 0.32229402309675736
43 400 0.31236107591861456
43 500 0.3198716825341407
43 600 0.3331484153971858
43 700 0.3134973337351771
43 800 0.30229111334464315
43 900 0.30735969664178125
43 1000 0.2877373973186757
43 1100 0.2901881271014898
43 1200 0.30169356493214733
43 1300 0.2939457581840244
43 1400 0.3064796073314751
43 1500 0.33366216776137503
43 1600 0.31495308770470115
43 1700 0.3074596971506901
43 1800 0.2845390836335593
43 1900 0.2720304223488474
43 2000 0.27863576372015103
43 2100 0.26468098884703367

Train Epoch: 43 Average training loss: 0.2935 

44 0 0.6154767870903015
44 100 0.40822906081187
44 200 0.3464566761027533
44 300 0.3219060197082264
44 400 0.32658555065797845
44 500 0.3137931908312956
44 600 0.2991579695941941
44 700 0.30919287767299103
44 800 0.276902236267963
44 900 0.2986513047614818
44 1000 0.3201254935009587
44 1100 0.3005575491641533
44 1200 0.29699075218633186
44 1300 0.29273333389879963
44 1400 0.27501846271819474
44 1500 0.3073391009023987
44 1600 0.3094402273247036
44 1700 0.2965005135820574
44 1800 0.307181049073991
44 1900 0.29455914433214075
44 2000 0.28751750154056194
44 2100 0.27205346216563026

Train Epoch: 44 Average training loss: 0.3158 

45 0 0.09120498597621918
45 100 0.2332275923737392
45 200 0.2812631760948541
45 300 0.29453494207132075
45 400 0.3003656641534103
45 500 0.2989244490029967
45 600 0.29339241140157746
45 700 0.30114612392041956
45 800 0.2990037477583098
45 900 0.2986821521443226
45 1000 0.3163153023488461
45 1100 0.31456983223688245
45 1200 0.2782934845926896
45 1300 0.3148517099814844
45 1400 0.30210005785537486
45 1500 0.2874364549250333
45 1600 0.2994538937496431
45 1700 0.29725225227345803
45 1800 0.3010127167067348
45 1900 0.2931293865913265
45 2000 0.3039967260899167
45 2100 0.3048041260115239

Train Epoch: 45 Average training loss: 0.2916 

46 0 0.2959154546260834
46 100 0.30316111193303347
46 200 0.3369464988327121
46 300 0.36685588883560605
46 400 0.31618180258412076
46 500 0.3161098863200233
46 600 0.2841497999762263
46 700 0.31288796730906954
46 800 0.30375447418107976
46 900 0.3081684825634558
46 1000 0.30261714591692523
46 1100 0.26090457916033827
46 1200 0.25046602863403505
46 1300 0.2800846048941521
46 1400 0.2974309488399073
46 1500 0.28772234047643336
46 1600 0.29539085848745256
46 1700 0.2943021904266219
46 1800 0.2916125800383648
46 1900 0.28351151193172847
46 2000 0.2904917905000586
46 2100 0.2928298565459397

Train Epoch: 46 Average training loss: 0.2992 

47 0 0.2952740788459778
47 100 0.2978648573104566
47 200 0.2903173075370061
47 300 0.29914138591857975
47 400 0.27215109547891786
47 500 0.28794096643063366
47 600 0.27777346146705706
47 700 0.26590739548904735
47 800 0.32220711700405935
47 900 0.31638188357151004
47 1000 0.30888852693346747
47 1100 0.31039685180791216
47 1200 0.29744948956943407
47 1300 0.29345674809133465
47 1400 0.294401849809459
47 1500 0.31017910722907316
47 1600 0.2993293802136588
47 1700 0.29173012904131873
47 1800 0.2814010891392291
47 1900 0.29936442882081177
47 2000 0.3215119841313264
47 2100 0.31158147269922176

Train Epoch: 47 Average training loss: 0.2990 

48 0 0.14655111730098724
48 100 0.22264443583397964
48 200 0.25986321703986204
48 300 0.28980391544513856
48 400 0.30317920675695703
48 500 0.31279707839245563
48 600 0.28751263902304847
48 700 0.28821373166048053
48 800 0.29746580635541015
48 900 0.2814669396570416
48 1000 0.27549757640634287
48 1100 0.2758475153345232
48 1200 0.2976840200623112
48 1300 0.28450812185876195
48 1400 0.30475972492462217
48 1500 0.31196102995973085
48 1600 0.3120781698833476
48 1700 0.3109757861958348
48 1800 0.3141192388439791
48 1900 0.2937143277851876
48 2000 0.32875585576017213
48 2100 0.33537249547192133

Train Epoch: 48 Average training loss: 0.2910 

49 0 0.26425859332084656
49 100 0.28677626046540494
49 200 0.2896345326929685
49 300 0.30304969214087013
49 400 0.31011639434926086
49 500 0.30228437467333663
49 600 0.305521460495719
49 700 0.3161587748871744
49 800 0.30461819982206506
49 900 0.30056676149315864
49 1000 0.3027509375644474
49 1100 0.2873508358130295
49 1200 0.28437587343925164
49 1300 0.29852631057743034
49 1400 0.29062478840827827
49 1500 0.3072272155407575
49 1600 0.2927101004797452
49 1700 0.3125332161817374
49 1800 0.3047002196553268
49 1900 0.29306950941408044
49 2000 0.32678774057079313
49 2100 0.31880827685679386

Train Epoch: 49 Average training loss: 0.2987 

50 0 0.26118528842926025
50 100 0.273950715781101
50 200 0.2880648485507093
50 300 0.28825741832280233
50 400 0.29558044713455295
50 500 0.3050396526849233
50 600 0.32360130847247015
50 700 0.29534350729016157
50 800 0.2867545154137803
50 900 0.3051092325981018
50 1000 0.3091171334129382
50 1100 0.29054953411218415
50 1200 0.30117538782387737
50 1300 0.2850548980045274
50 1400 0.29736489030765867
50 1500 0.30535393726751564
50 1600 0.3231252318089799
50 1700 0.33591796139764407
50 1800 0.29984374564968813
50 1900 0.30844725940766643
50 2000 0.31752574187821125
50 2100 0.311627332069454

Train Epoch: 50 Average training loss: 0.2989 

