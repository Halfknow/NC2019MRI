1 0 0.6157874464988708
1 100 0.41347578817153935
1 200 0.34473870858077926
1 300 0.31204803441648804
1 400 0.3553758371010813
1 500 0.3182465361268753
1 600 0.30653499970092596
1 700 0.3148594546827605
1 800 0.3026587333667172
1 900 0.293891829177921
1 1000 0.304821120125232
1 1100 0.31343228082580615
1 1200 0.3157963407891739
1 1300 0.33928825817696273
1 1400 0.2945169609713593
1 1500 0.2897437744020789
1 1600 0.31086582419211933
1 1700 0.3259762273727551
1 1800 0.3269432875797112
1 1900 0.3166863822171685
1 2000 0.3052725229808099
1 2100 0.30083421450548664

Train Epoch: 1 Average training loss: 0.3243 

2 0 0.38583827018737793
2 100 0.3152879743045015
2 200 0.30972151444040663
2 300 0.32520379223264617
2 400 0.32064850586248683
2 500 0.29883089622716197
2 600 0.3242686407335232
2 700 0.327897202485197
2 800 0.30172899444488877
2 900 0.3126228565870054
2 1000 0.3063551229577476
2 1100 0.31885824088011616
2 1200 0.31427729774370505
2 1300 0.31571297949442234
2 1400 0.3066103191527211
2 1500 0.28397559288997387
2 1600 0.30711014183730145
2 1700 0.3001388957495253
2 1800 0.30823204089033135
2 1900 0.3103278241132665
2 2000 0.3157633740870849
2 2100 0.31559254634814615

Train Epoch: 2 Average training loss: 0.3112 

3 0 0.1590915322303772
3 100 0.24099957746859618
3 200 0.26602362907767985
3 300 0.3003190552338648
3 400 0.31110523682844093
3 500 0.3071736729836318
3 600 0.32774132890380353
3 700 0.32133311247237284
3 800 0.3393080509178899
3 900 0.3168626393839181
3 1000 0.31477402483200473
3 1100 0.30531673013446614
3 1200 0.3077782214331311
3 1300 0.30496785266493825
3 1400 0.28819789548433916
3 1500 0.29375332531263815
3 1600 0.30337095117196805
3 1700 0.30894319483686794
3 1800 0.3108756422126851
3 1900 0.32765809871911483
3 2000 0.3070602643515426
3 2100 0.29221958514724683

Train Epoch: 3 Average training loss: 0.3024 

4 0 0.4983861446380615
4 100 0.37628689010568867
4 200 0.3229976984381645
4 300 0.30367549587139003
4 400 0.3114875644460894
4 500 0.33299156375563016
4 600 0.34404848014389355
4 700 0.29845224604784587
4 800 0.28253644186152904
4 900 0.32003654359511813
4 1000 0.3090684912166602
4 1100 0.3048780375129703
4 1200 0.3086832069547146
4 1300 0.3058562774634483
4 1400 0.2877166268482418
4 1500 0.2800499450868358
4 1600 0.28369955249716117
4 1700 0.3081760704466109
4 1800 0.30733015069410385
4 1900 0.3250059323745404
4 2000 0.3076414576990335
4 2100 0.31091022378404576

Train Epoch: 4 Average training loss: 0.3164 

5 0 0.26455727219581604
5 100 0.3004034731238637
5 200 0.27015707548882667
5 300 0.3116026027332368
5 400 0.312867350499694
5 500 0.3364244610819997
5 600 0.31324871162436785
5 700 0.3210379057260082
5 800 0.32109700840370065
5 900 0.2961574072351818
5 1000 0.29421416408949064
5 1100 0.31855632948416934
5 1200 0.2997255883662422
5 1300 0.3136380618864331
5 1400 0.320159525339074
5 1500 0.3008727052601228
5 1600 0.3088848436156678
5 1700 0.3067430878253527
5 1800 0.27968323304073933
5 1900 0.2843622365464135
5 2000 0.32571221392336475
5 2100 0.3178809875093666

Train Epoch: 5 Average training loss: 0.3060 

6 0 0.18969425559043884
6 100 0.2709372821035839
6 200 0.3367714060103815
6 300 0.3157540987123245
6 400 0.2977664570651794
6 500 0.3029871696208032
6 600 0.33037699140272186
6 700 0.31010329786879876
6 800 0.33940764514139565
6 900 0.3184837982141941
6 1000 0.3224363613559928
6 1100 0.30650411047403797
6 1200 0.3137578935330347
6 1300 0.30055478516763967
6 1400 0.31106552188495784
6 1500 0.3138266474577041
6 1600 0.29444868446205696
6 1700 0.29815320684964863
6 1800 0.31983177853231504
6 1900 0.307805010658081
6 2000 0.28187997561772543
6 2100 0.2953015609023324

Train Epoch: 6 Average training loss: 0.3056 

7 0 0.4108915328979492
7 100 0.3371216572598771
7 200 0.3134270607511287
7 300 0.30174742146204747
7 400 0.31438687479876926
7 500 0.3188292776134957
7 600 0.3099906907956522
7 700 0.3057525566222842
7 800 0.2983781322552367
7 900 0.2855278991876402
7 1000 0.2979508130003899
7 1100 0.33712097156593107
7 1200 0.33116275320173827
7 1300 0.33071210724323385
7 1400 0.307727030064898
7 1500 0.309511952598543
7 1600 0.28925229539553543
7 1700 0.2920929179735949
7 1800 0.2907447035189077
7 1900 0.29276956180827285
7 2000 0.29281897492685866
7 2100 0.32939299319610665

Train Epoch: 7 Average training loss: 0.3119 

8 0 0.1545896679162979
8 100 0.24774981687132375
8 200 0.2831298932710894
8 300 0.32454219815173546
8 400 0.30063406675915494
8 500 0.2746307571111332
8 600 0.29527918297691147
8 700 0.30931639315483994
8 800 0.3162178786048157
8 900 0.31245084190041106
8 1000 0.3083079325148464
8 1100 0.30301501613337695
8 1200 0.30486636472642736
8 1300 0.31334095574226944
8 1400 0.31345385795160025
8 1500 0.3083366777331968
8 1600 0.30730193703526926
8 1700 0.31174405064062677
8 1800 0.3217340079525458
8 1900 0.30510180733869996
8 2000 0.31141669139274725
8 2100 0.30210162647092476

Train Epoch: 8 Average training loss: 0.3007 

9 0 0.1098511666059494
9 100 0.24046951299720634
9 200 0.2926571599153888
9 300 0.308602964672207
9 400 0.29657660799405017
9 500 0.31449741521273344
9 600 0.31574054544868957
9 700 0.322058791403077
9 800 0.29201756164577014
9 900 0.2893699680605875
9 1000 0.29681089275741374
9 1100 0.29873898171877467
9 1200 0.3174555780286208
9 1300 0.35195367496197644
9 1400 0.33341956295715036
9 1500 0.3254017890299396
9 1600 0.3036167818021764
9 1700 0.32260844326089033
9 1800 0.30312841889971376
9 1900 0.29220079422568546
9 2000 0.2968486712166957
9 2100 0.2822339522686371

Train Epoch: 9 Average training loss: 0.2989 

10 0 0.36372146010398865
10 100 0.3221310395483844
10 200 0.2933410492859652
10 300 0.29356997218325964
10 400 0.3019591310330282
10 500 0.2840554611473496
10 600 0.30641468421128754
10 700 0.3448231883363098
10 800 0.3285919291246801
10 900 0.31582518782851077
10 1000 0.3076542925449469
10 1100 0.3286429039269625
10 1200 0.31807528207133895
10 1300 0.312972001037573
10 1400 0.319652119455341
10 1500 0.31635780475381825
10 1600 0.30828227392517965
10 1700 0.3170047384130261
10 1800 0.30712134797720275
10 1900 0.28951556312479587
10 2000 0.302494753946572
10 2100 0.2980999059228879

Train Epoch: 10 Average training loss: 0.3114 

11 0 0.5752418041229248
11 100 0.40466225270552625
11 200 0.34724927131846833
11 300 0.3172621457585579
11 400 0.2916325294723166
11 500 0.31077384495229565
11 600 0.2960244910430565
11 700 0.31369187588647335
11 800 0.30466955165030307
11 900 0.32393524191424444
11 1000 0.3033111165429522
11 1100 0.30317496996885396
11 1200 0.3254590447931798
11 1300 0.31680504459357983
11 1400 0.2982128340490826
11 1500 0.27308899269677733
11 1600 0.30605589991674054
11 1700 0.3030545741785051
11 1800 0.3160876256486229
11 1900 0.34078300469153383
11 2000 0.32446672291854606
11 2100 0.32036099259578926

Train Epoch: 11 Average training loss: 0.3201 

12 0 0.21420450508594513
12 100 0.23755297972946535
12 200 0.28940938719897386
12 300 0.3182183129826776
12 400 0.30003196460673387
12 500 0.3090630655845444
12 600 0.3119768812224163
12 700 0.2784064612653964
12 800 0.28877269006620626
12 900 0.3009765433752399
12 1000 0.32964590370983354
12 1100 0.3147289436479828
12 1200 0.3212450889874234
12 1300 0.34202544669178503
12 1400 0.3077627077941383
12 1500 0.3082820993010725
12 1600 0.3112622929978494
12 1700 0.2872016005538173
12 1800 0.29179246901404954
12 1900 0.2954737303701968
12 2000 0.33826269344395027
12 2100 0.32738053057381505

Train Epoch: 12 Average training loss: 0.3029 

13 0 0.5895689129829407
13 100 0.415671774659163
13 200 0.3371375012691967
13 300 0.35258687838853037
13 400 0.31294697759439577
13 500 0.3047495347862426
13 600 0.29211753375707045
13 700 0.2733943027000201
13 800 0.29858555964005823
13 900 0.30572550534143483
13 1000 0.31229879853456666
13 1100 0.3221050722532909
13 1200 0.3032368568392894
13 1300 0.29639914579563154
13 1400 0.2926907565291461
13 1500 0.2842239263663782
13 1600 0.3083488365487147
13 1700 0.3047170284920024
13 1800 0.3129034468845848
13 1900 0.3124421102788751
13 2000 0.3202294641089988
13 2100 0.31426402835905237

Train Epoch: 13 Average training loss: 0.3201 

14 0 0.17718146741390228
14 100 0.23289316409923455
14 200 0.2711226113143642
14 300 0.3041845813689725
14 400 0.28924869059060176
14 500 0.3195142627222006
14 600 0.28431125109095146
14 700 0.3029075079118161
14 800 0.314489893048221
14 900 0.2969883476142757
14 1000 0.3185805413013484
14 1100 0.33438038840281137
14 1200 0.3147174431360824
14 1300 0.33565783878636585
14 1400 0.32282537888889534
14 1500 0.31200791024997926
14 1600 0.29779352564417105
14 1700 0.2845931384233593
14 1800 0.29995651688935304
14 1900 0.3024361146961716
14 2000 0.3147941956408697
14 2100 0.3021959410945475

Train Epoch: 14 Average training loss: 0.2998 

15 0 0.3865177035331726
15 100 0.3433553482472986
15 200 0.30236623441363336
15 300 0.3278583435924307
15 400 0.31751369474461877
15 500 0.3068424607133845
15 600 0.301538922189198
15 700 0.32074507632750954
15 800 0.3110649843947771
15 900 0.317546619192573
15 1000 0.30839601040428327
15 1100 0.30336993135903495
15 1200 0.2950901747079554
15 1300 0.29237064016879166
15 1400 0.3147484864949918
15 1500 0.3372551191973909
15 1600 0.3035105539578088
15 1700 0.3044790936309638
15 1800 0.27651594475257035
15 1900 0.29347958458486956
15 2000 0.2964005471738568
15 2100 0.3023413963343078

Train Epoch: 15 Average training loss: 0.3095 

16 0 0.23008686304092407
16 100 0.25795128778018855
16 200 0.2887870053297854
16 300 0.29150438479258106
16 400 0.2877877260560171
16 500 0.3017953486462134
16 600 0.3156643253182379
16 700 0.29741314090937954
16 800 0.30489750824332584
16 900 0.3077339182052229
16 1000 0.3170496783781212
16 1100 0.3089085116163712
16 1200 0.3264318229163445
16 1300 0.289666215681247
16 1400 0.2912563804298783
16 1500 0.3021486324581698
16 1600 0.28533092104341967
16 1700 0.3211033051159393
16 1800 0.2963791971348193
16 1900 0.2922753450676114
16 2000 0.3360435100138572
16 2100 0.3071654577553665

Train Epoch: 16 Average training loss: 0.3014 

17 0 0.34654700756073
17 100 0.30677173081011483
17 200 0.33315298658852305
17 300 0.3318009276476895
17 400 0.29247193267555205
17 500 0.3161541682286025
17 600 0.3303739491227257
17 700 0.29325023644376774
17 800 0.294745287624235
17 900 0.30458144966759676
17 1000 0.3348085008715672
17 1100 0.3013474745871132
17 1200 0.2778321779298731
17 1300 0.30162463809154605
17 1400 0.30872754886251796
17 1500 0.29825116870990703
17 1600 0.28494296286169307
17 1700 0.295972026135565
17 1800 0.30590823873466105
17 1900 0.3099552064616072
17 2000 0.3023360843766422
17 2100 0.317736761135966

Train Epoch: 17 Average training loss: 0.3059 

18 0 0.8148323893547058
18 100 0.4957487588388955
18 200 0.39303910093470207
18 300 0.36894908361247636
18 400 0.31962844831685805
18 500 0.2948806134178531
18 600 0.27117504257177444
18 700 0.29976798945167965
18 800 0.33451151316953964
18 900 0.2918806395129976
18 1000 0.278105978949635
18 1100 0.31848492676237117
18 1200 0.3255615019939017
18 1300 0.3273219845273564
18 1400 0.32677544072467835
18 1500 0.31552709100761883
18 1600 0.29834227884100645
18 1700 0.3047063652911683
18 1800 0.29741826825420586
18 1900 0.31121026201481367
18 2000 0.2875792987310836
18 2100 0.29518405094066136

Train Epoch: 18 Average training loss: 0.3307 

19 0 0.1800745129585266
19 100 0.2572322252897834
19 200 0.28437990864892226
19 300 0.30018846934968707
19 400 0.29957297588659065
19 500 0.30449158384370656
19 600 0.3237422475508984
19 700 0.32765646729031894
19 800 0.3241581302719356
19 900 0.3057087207519772
19 1000 0.3051977553150445
19 1100 0.29260944299248
19 1200 0.29819726661216206
19 1300 0.30894146922601734
19 1400 0.31191106796846696
19 1500 0.3013587817596705
19 1600 0.3074966332258464
19 1700 0.3151621598897307
19 1800 0.3082624453588848
19 1900 0.32266084863372685
19 2000 0.3067000557207483
19 2100 0.30026929979041933

Train Epoch: 19 Average training loss: 0.2995 

20 0 0.1859092265367508
20 100 0.28656904895907725
20 200 0.28243913435165297
20 300 0.2718606521768869
20 400 0.30075658113758724
20 500 0.2888085836933074
20 600 0.2858791535383175
20 700 0.3036944147408207
20 800 0.2943830128457664
20 900 0.31556398383618156
20 1000 0.289183916734737
20 1100 0.27555990708617645
20 1200 0.30368717992899036
20 1300 0.29150869522061096
20 1400 0.31051384859440145
20 1500 0.3287837324440507
20 1600 0.30582408437604863
20 1700 0.3139313649961336
20 1800 0.3194175881041124
20 1900 0.32045055001731293
20 2000 0.301826795022329
20 2100 0.27850154407305117

Train Epoch: 20 Average training loss: 0.2997 

21 0 0.162279412150383
21 100 0.249678628365133
21 200 0.3010969516346525
21 300 0.29608368446859146
21 400 0.28634495072036675
21 500 0.29704401553024046
21 600 0.2971680747803787
21 700 0.33409715420051156
21 800 0.297853808016808
21 900 0.2995126434821019
21 1000 0.28268049849673615
21 1100 0.29620610049547713
21 1200 0.31297907782092055
21 1300 0.2887201559721114
21 1400 0.2889968732569004
21 1500 0.290525554943538
21 1600 0.2840194579892718
21 1700 0.3257439712637683
21 1800 0.29556977791546474
21 1900 0.3108129304765742
21 2000 0.3368925265707438
21 2100 0.30280937480921366

Train Epoch: 21 Average training loss: 0.2986 

22 0 0.3722119927406311
22 100 0.31607195809112126
22 200 0.3134551050054156
22 300 0.3019763223760303
22 400 0.28921819200393195
22 500 0.3226967779131585
22 600 0.27668196048522575
22 700 0.313736466873758
22 800 0.31272502233479305
22 900 0.30770072063203435
22 1000 0.27977051307418893
22 1100 0.3102152523968527
22 1200 0.29901596027269983
22 1300 0.3029007805793762
22 1400 0.3457538875206531
22 1500 0.3324164278798595
22 1600 0.28340104267233096
22 1700 0.29331882460504116
22 1800 0.28832886391883467
22 1900 0.290999538456355
22 2000 0.2971383788397217
22 2100 0.31277725471980816

Train Epoch: 22 Average training loss: 0.3071 

23 0 0.23669269680976868
23 100 0.2878583906892909
23 200 0.284478020818014
23 300 0.29055453041178386
23 400 0.2898338400715283
23 500 0.3177742997221136
23 600 0.33858783348815824
23 700 0.32732452318961475
23 800 0.28513181756520495
23 900 0.3092112625113306
23 1000 0.2916136751905679
23 1100 0.3038767952077739
23 1200 0.29662747980308246
23 1300 0.2949437278412112
23 1400 0.3062136555825125
23 1500 0.3177481942749427
23 1600 0.27565429923906326
23 1700 0.2988374439819608
23 1800 0.2929926325065522
23 1900 0.33228621053478147
23 2000 0.3228041134242789
23 2100 0.30727347606005495

Train Epoch: 23 Average training loss: 0.3010 

24 0 0.27667713165283203
24 100 0.2853125083100643
24 200 0.27500246850633103
24 300 0.3100299566676611
24 400 0.30227554860727923
24 500 0.324955293731993
24 600 0.2974659855596232
24 700 0.2975520425042841
24 800 0.2824175517345159
24 900 0.31397450229271295
24 1000 0.32951150947435526
24 1100 0.3206877625562079
24 1200 0.318805199520982
24 1300 0.28779172626034694
24 1400 0.30940009326334517
24 1500 0.3136714296232868
24 1600 0.3140977043336875
24 1700 0.2868986302851921
24 1800 0.2958394092737031
24 1900 0.3203528590522854
24 2000 0.3317487574326187
24 2100 0.3170147003115715

Train Epoch: 24 Average training loss: 0.3038 

25 0 0.4209681451320648
25 100 0.3489126447816879
25 200 0.3051269964284259
25 300 0.2930142029803559
25 400 0.3043765787903803
25 500 0.3067001953537559
25 600 0.3137738493308704
25 700 0.31902604148508
25 800 0.2820436503251775
25 900 0.2874085472941593
25 1000 0.29544641830767854
25 1100 0.3313228304879888
25 1200 0.31244101777218075
25 1300 0.30089279295590693
25 1400 0.3222828200175773
25 1500 0.3154630848104125
25 1600 0.3215056466096891
25 1700 0.30866369195852456
25 1800 0.3308707235424956
25 1900 0.3533063703897164
25 2000 0.32062615874176087
25 2100 0.2905059213252636

Train Epoch: 25 Average training loss: 0.3100 

26 0 0.09157010912895203
26 100 0.21172673630422964
26 200 0.2621448058699441
26 300 0.2833042925195993
26 400 0.31581174604182277
26 500 0.3309289222716239
26 600 0.32010507984135556
26 700 0.29779423129798654
26 800 0.3046476729950313
26 900 0.31361206449385765
26 1000 0.2911972900008555
26 1100 0.3177442734634004
26 1200 0.2896687641117659
26 1300 0.27773594282667463
26 1400 0.31980147623639466
26 1500 0.3177921185060309
26 1600 0.3209077062180282
26 1700 0.2901387828991617
26 1800 0.2836488033504069
26 1900 0.30305428307367316
26 2000 0.3054287447126883
26 2100 0.2981279644504328

Train Epoch: 26 Average training loss: 0.2930 

27 0 0.09732900559902191
27 100 0.2364344839293579
27 200 0.27226767711169086
27 300 0.26964582735291526
27 400 0.29602683610285113
27 500 0.3096509858219429
27 600 0.3026154226088382
27 700 0.3189343933009707
27 800 0.3006665419949901
27 900 0.283179202959057
27 1000 0.28982855466927576
27 1100 0.29028187057791555
27 1200 0.28468859683163483
27 1300 0.296700055976636
27 1400 0.30303396182122394
27 1500 0.29008662251595063
27 1600 0.28832328942680663
27 1700 0.30700653964244695
27 1800 0.30738343036285554
27 1900 0.3131900712540076
27 2000 0.29098579206792996
27 2100 0.31493291920706235

Train Epoch: 27 Average training loss: 0.2915 

28 0 0.13432654738426208
28 100 0.24573417540114775
28 200 0.2862227765636146
28 300 0.29323749797147236
28 400 0.28290619495213215
28 500 0.301021246217924
28 600 0.28950566062381944
28 700 0.28664184675145316
28 800 0.2972089051946147
28 900 0.3272696977725634
28 1000 0.3011500076686419
28 1100 0.3125208615519804
28 1200 0.3033992791709223
28 1300 0.31264633561776495
28 1400 0.332930362450713
28 1500 0.3095723819521244
28 1600 0.2778097120696061
28 1700 0.28413355777249977
28 1800 0.30252586448029223
28 1900 0.3009125868948318
28 2000 0.31120288653939987
28 2100 0.2980931063201917

Train Epoch: 28 Average training loss: 0.2940 

29 0 0.06536263972520828
29 100 0.21991273673823253
29 200 0.25962194162849594
29 300 0.28458569580937243
29 400 0.2840056204658027
29 500 0.28438561836994874
29 600 0.2948291167109177
29 700 0.30081723654949505
29 800 0.32455174302635154
29 900 0.3355267243701666
29 1000 0.32725267855967005
29 1100 0.32001033829782866
29 1200 0.3019413945881167
29 1300 0.31151515175742284
29 1400 0.29659473651227636
29 1500 0.2669421972749472
29 1600 0.25685104689629445
29 1700 0.28218592973247997
29 1800 0.3012786705060713
29 1900 0.3144374516838528
29 2000 0.31555589238168613
29 2100 0.33451364161732694

Train Epoch: 29 Average training loss: 0.2902 

30 0 0.1837620586156845
30 100 0.2692597217234391
30 200 0.26368023084357944
30 300 0.2884288664475557
30 400 0.30081000175961037
30 500 0.313773360304796
30 600 0.2907125775961096
30 700 0.29608509519407883
30 800 0.30732718032280554
30 900 0.28606683067711225
30 1000 0.2771265333314168
30 1100 0.284050047454094
30 1200 0.29966648958756176
30 1300 0.29654062558396854
30 1400 0.29542340649611903
30 1500 0.3074062732643307
30 1600 0.3060505474462693
30 1700 0.29937415735009115
30 1800 0.3146412226773527
30 1900 0.3140637454268111
30 2000 0.32928534795662007
30 2100 0.3213837826883354

Train Epoch: 30 Average training loss: 0.2950 

31 0 0.108660489320755
31 100 0.23482019269594678
31 200 0.2955357046671154
31 300 0.31267087460458226
31 400 0.3157408488836444
31 500 0.29761268464374246
31 600 0.3045850411308758
31 700 0.2873760159231787
31 800 0.27744815117527916
31 900 0.2690449678788738
31 1000 0.2893249549860002
31 1100 0.3079683657810873
31 1200 0.28417452531283327
31 1300 0.26492897692846357
31 1400 0.2842241259284421
31 1500 0.29169552119215963
31 1600 0.30407471340991726
31 1700 0.3200585973502569
31 1800 0.34649780538412733
31 1900 0.3221105423341564
31 2000 0.32875983608508474
31 2100 0.31963279290769514

Train Epoch: 31 Average training loss: 0.2953 

32 0 0.4357931613922119
32 100 0.34936188296047893
32 200 0.31982798603936746
32 300 0.31381056666208956
32 400 0.30154227614163387
32 500 0.290174569976011
32 600 0.29515315910726797
32 700 0.3242388166651267
32 800 0.30218381915719855
32 900 0.3149311916111473
32 1000 0.30419755290968087
32 1100 0.31417592517982057
32 1200 0.31731184779641625
32 1300 0.31590574962136414
32 1400 0.2949983799621296
32 1500 0.279650165967412
32 1600 0.30462716743125323
32 1700 0.2854272605486917
32 1800 0.3311694971587788
32 1900 0.3205229473273851
32 2000 0.3024690511074467
32 2100 0.30520860088570306

Train Epoch: 32 Average training loss: 0.3089 

33 0 0.4632517993450165
33 100 0.3531723261552037
33 200 0.32138420865457185
33 300 0.31884372820895296
33 400 0.31675484729219977
33 500 0.3140308765131235
33 600 0.2943608450054187
33 700 0.3055394250723749
33 800 0.32005916442045973
33 900 0.29809838710035996
33 1000 0.275133311377284
33 1100 0.275211250233345
33 1200 0.3189948096568462
33 1300 0.32749992909934034
33 1400 0.3106366886416471
33 1500 0.29181779051835305
33 1600 0.2980289695674803
33 1700 0.2814142460585009
33 1800 0.31238347775399833
33 1900 0.29746637784915014
33 2000 0.32143153198797536
33 2100 0.28922472456860904

Train Epoch: 33 Average training loss: 0.3108 

34 0 0.04766220226883888
34 100 0.21400200339772
34 200 0.28025384092225136
34 300 0.30009975996148497
34 400 0.3021294894875755
34 500 0.3130871191176542
34 600 0.3086401869529528
34 700 0.3176445742185762
34 800 0.29680277928857196
34 900 0.3022714859643965
34 1000 0.2990725340579775
34 1100 0.30170780943359293
34 1200 0.308943100878515
34 1300 0.298088603580164
34 1400 0.31331567476966804
34 1500 0.279788627381889
34 1600 0.30473578373771404
34 1700 0.299250705013505
34 1800 0.3055934506742999
34 1900 0.29251236892148613
34 2000 0.2900462159879982
34 2100 0.27419937530345745

Train Epoch: 34 Average training loss: 0.2913 

35 0 0.12415316700935364
35 100 0.25272168506624865
35 200 0.28488842609573617
35 300 0.28886792038648607
35 400 0.2986105892981206
35 500 0.3170777170357534
35 600 0.29102848338033627
35 700 0.3119242517809029
35 800 0.33021692643666606
35 900 0.294247346485942
35 1000 0.26490753937809686
35 1100 0.28379682682881563
35 1200 0.30497898770516835
35 1300 0.32102403862877504
35 1400 0.30679968510263705
35 1500 0.2824106672207549
35 1600 0.28671319443495835
35 1700 0.29537088489812663
35 1800 0.2966016760458896
35 1900 0.30655578491812857
35 2000 0.30428535708387955
35 2100 0.3110195923237356

Train Epoch: 35 Average training loss: 0.2934 

36 0 0.5155932903289795
36 100 0.36718624323649596
36 200 0.31733216205651754
36 300 0.3179075097197197
36 400 0.31581013062834984
36 500 0.3103496567365062
36 600 0.3072028983216724
36 700 0.3140698303046224
36 800 0.3292454033554919
36 900 0.2855677628425609
36 1000 0.3085620158460169
36 1100 0.28804491158734075
36 1200 0.3152683524856428
36 1300 0.3200812178318394
36 1400 0.3092154477113456
36 1500 0.2908756486520255
36 1600 0.28310825414544405
36 1700 0.28825364668884096
36 1800 0.2956137235378975
36 1900 0.3074071596153733
36 2000 0.3033105270494559
36 2100 0.3021041499986722

Train Epoch: 36 Average training loss: 0.3123 

37 0 0.6792891025543213
37 100 0.4300519516768603
37 200 0.35085911429103234
37 300 0.3343656509558462
37 400 0.32935822495657385
37 500 0.30883227249947265
37 600 0.30311849247837847
37 700 0.2987810388947407
37 800 0.2843287854357681
37 900 0.30141625760286367
37 1000 0.2693682768977137
37 1100 0.26696882940349476
37 1200 0.28822265859448765
37 1300 0.2942009701731973
37 1400 0.29620589921324975
37 1500 0.267425018297191
37 1600 0.3051937065884736
37 1700 0.3044592589193125
37 1800 0.30549856398703745
37 1900 0.320612063729845
37 2000 0.2962977586952171
37 2100 0.3133856848570502

Train Epoch: 37 Average training loss: 0.3182 

38 0 0.5402335524559021
38 100 0.39854346802883955
38 200 0.35001933661037143
38 300 0.29412936182665056
38 400 0.2979433910422647
38 500 0.27184798531533166
38 600 0.28834349035841
38 700 0.31914574273874047
38 800 0.31864861690477
38 900 0.3092063781370091
38 1000 0.2923152390629308
38 1100 0.3058932271652285
38 1200 0.3219507342220032
38 1300 0.30332600681848343
38 1400 0.268770695607244
38 1500 0.2744604054885963
38 1600 0.2960600089798937
38 1700 0.3127052873867628
38 1800 0.3218212271147793
38 1900 0.31418329810083306
38 2000 0.30703974574687387
38 2100 0.317526782870521

Train Epoch: 38 Average training loss: 0.3126 

39 0 0.1349439173936844
39 100 0.22687328180325814
39 200 0.30328851202474205
39 300 0.2771382537595889
39 400 0.2736564265536777
39 500 0.26344263994052614
39 600 0.3125634113148496
39 700 0.321347589507016
39 800 0.3221949998862818
39 900 0.29959367374457274
39 1000 0.30406104994539673
39 1100 0.34455842503240514
39 1200 0.3101490593681991
39 1300 0.32959229027175724
39 1400 0.3183415826568126
39 1500 0.26949110466115456
39 1600 0.2872965030419887
39 1700 0.27793388003756825
39 1800 0.29743536134817816
39 1900 0.30512867073598393
39 2000 0.2777878210808991
39 2100 0.32127383063699205

Train Epoch: 39 Average training loss: 0.2933 

40 0 0.13752420246601105
40 100 0.24132398953759793
40 200 0.2962685209940014
40 300 0.29430477353544177
40 400 0.26912701067976064
40 500 0.30494310114632056
40 600 0.29724634473098366
40 700 0.33119827566960885
40 800 0.32380457208391217
40 900 0.2931359828380905
40 1000 0.2972693037511558
40 1100 0.28401173492485715
40 1200 0.3024241730421073
40 1300 0.2958718427231684
40 1400 0.3028832453826215
40 1500 0.2861517638971435
40 1600 0.2765157132782688
40 1700 0.29042776986655905
40 1800 0.276137787323602
40 1900 0.30812227242601437
40 2000 0.29108757425438747
40 2100 0.30348840433894914

Train Epoch: 40 Average training loss: 0.2930 

41 0 0.1256844699382782
41 100 0.2380097849658287
41 200 0.2860387049424082
41 300 0.2914115435118138
41 400 0.29114392846789494
41 500 0.3018629195681599
41 600 0.3203416812053745
41 700 0.29051138696427176
41 800 0.2854317325437134
41 900 0.3087701616917932
41 1000 0.2870884233408697
41 1100 0.2914378719339547
41 1200 0.29882800134500903
41 1300 0.30485231275414176
41 1400 0.3117237134106574
41 1500 0.2991681252346756
41 1600 0.29906168143755313
41 1700 0.29785540234724917
41 1800 0.2895211019903407
41 1900 0.30144701664349155
41 2000 0.3104093007654543
41 2100 0.29428746538536077

Train Epoch: 41 Average training loss: 0.2914 

42 0 0.16598182916641235
42 100 0.2636979914802901
42 200 0.27812176916570674
42 300 0.2829618872266341
42 400 0.3018078508448494
42 500 0.30304476948546166
42 600 0.27793888676702344
42 700 0.300682281980127
42 800 0.3124370246736601
42 900 0.3140846226733427
42 1000 0.3049667714976943
42 1100 0.3067837437272425
42 1200 0.3075878535744037
42 1300 0.28998745577886637
42 1400 0.2811123290393198
42 1500 0.3021289650154299
42 1600 0.29817076818874483
42 1700 0.3153838626110044
42 1800 0.3127335186301465
42 1900 0.3154857638550177
42 2000 0.2986153265763798
42 2100 0.2917299636531319

Train Epoch: 42 Average training loss: 0.2953 

43 0 0.11403267085552216
43 100 0.20871179626332614
43 200 0.26208826601681406
43 300 0.2771128830982828
43 400 0.2982167321732817
43 500 0.2922173790487892
43 600 0.31677895098123954
43 700 0.31468261990645563
43 800 0.28781671489407606
43 900 0.28408572598946
43 1000 0.2919148381892549
43 1100 0.3158902684167789
43 1200 0.3113590242457268
43 1300 0.3252400293156978
43 1400 0.2873643444853381
43 1500 0.2963513581026518
43 1600 0.31375193284828057
43 1700 0.29602496313930526
43 1800 0.3195124545015535
43 1900 0.33532217245119306
43 2000 0.3245683503272806
43 2100 0.29062282653454247

Train Epoch: 43 Average training loss: 0.2924 

44 0 0.9838859438896179
44 100 0.5505393287281828
44 200 0.40141245212885723
44 300 0.34035603014285726
44 400 0.31618748574722866
44 500 0.31250114435273035
44 600 0.30064370994108436
44 700 0.32917611127137836
44 800 0.29194803383077605
44 900 0.3024476205348115
44 1000 0.30507363792729175
44 1100 0.3088817053345441
44 1200 0.29608262973836197
44 1300 0.3028831860064223
44 1400 0.29691250633824634
44 1500 0.31739068052989294
44 1600 0.29641033996331395
44 1700 0.26888333935514075
44 1800 0.2848072452432194
44 1900 0.29082388736364095
44 2000 0.30242504067477455
44 2100 0.3044806723999508

Train Epoch: 44 Average training loss: 0.3342 

45 0 0.18246868252754211
45 100 0.24702361831021685
45 200 0.2572398365918735
45 300 0.311431275514775
45 400 0.3116114800336724
45 500 0.2937610328759107
45 600 0.27404256410623007
45 700 0.29998016633087404
45 800 0.32853561329716013
45 900 0.3014021727792504
45 1000 0.31642087621583825
45 1100 0.29278799158738733
45 1200 0.3151679683665801
45 1300 0.2868033570026507
45 1400 0.2842784294264144
45 1500 0.29247925576222594
45 1600 0.3016795350849752
45 1700 0.29784595377741907
45 1800 0.3103918205927431
45 1900 0.32237191829946427
45 2000 0.30393021493916766
45 2100 0.307373581695056

Train Epoch: 45 Average training loss: 0.2953 

46 0 0.3284115493297577
46 100 0.30482642243433966
46 200 0.3067877162299454
46 300 0.28558128768819135
46 400 0.284831682274326
46 500 0.3259962313009817
46 600 0.3042626034315257
46 700 0.292271629657496
46 800 0.2999569122729445
46 900 0.31836022055638263
46 1000 0.30700374859650503
46 1100 0.2917349121519272
46 1200 0.3015939044905446
46 1300 0.3336241243624614
46 1400 0.3132809758361803
46 1500 0.2813775606776725
46 1600 0.2883109190565091
46 1700 0.3337212312236664
46 1800 0.3248270759207365
46 1900 0.30650381346677963
46 2000 0.2949733957670986
46 2100 0.28751821891854046

Train Epoch: 46 Average training loss: 0.3034 

47 0 0.38584840297698975
47 100 0.33013137451178887
47 200 0.31933261110116895
47 300 0.2981738650856336
47 400 0.32566140619155415
47 500 0.31294787026180976
47 600 0.31520264418447963
47 700 0.29133939429333866
47 800 0.2922923404958462
47 900 0.308137929222678
47 1000 0.2869515568107654
47 1100 0.2922664316896291
47 1200 0.3019132297270092
47 1300 0.30987456724660234
47 1400 0.2876369990742902
47 1500 0.301248831382112
47 1600 0.30821958377263947
47 1700 0.2960290060927011
47 1800 0.3020506506285573
47 1900 0.3017040762624094
47 2000 0.29168266342603044
47 2100 0.2929701321903374

Train Epoch: 47 Average training loss: 0.3048 

48 0 0.046808790415525436
48 100 0.22490039929011804
48 200 0.27665573865329673
48 300 0.2831351366151586
48 400 0.29492876580491945
48 500 0.3183775461454466
48 600 0.32817947725884045
48 700 0.3249464811642121
48 800 0.30875842348032084
48 900 0.28048287590008003
48 1000 0.2685802315036739
48 1100 0.3059394035467991
48 1200 0.2870768737984601
48 1300 0.2912587278988131
48 1400 0.30088023030975275
48 1500 0.3010007028902253
48 1600 0.2936057790631828
48 1700 0.29680188253448736
48 1800 0.308256450104691
48 1900 0.3005451158092234
48 2000 0.2933487379787102
48 2100 0.2968827895080593

Train Epoch: 48 Average training loss: 0.2899 

49 0 0.07596656680107117
49 100 0.1851177095737027
49 200 0.25723154567853956
49 300 0.29373008046098487
49 400 0.31607973030404035
49 500 0.2970319654681128
49 600 0.2736498663940938
49 700 0.30525084238009476
49 800 0.3069841745834079
49 900 0.2935201514471688
49 1000 0.2882539735919843
49 1100 0.2876840245803848
49 1200 0.2812416929099878
49 1300 0.33258161144804776
49 1400 0.30528946529184114
49 1500 0.3012631095636035
49 1600 0.30946592603232626
49 1700 0.31126367413132155
49 1800 0.30424470823224803
49 1900 0.2981600145235377
49 2000 0.2881468172758878
49 2100 0.30755803519708047

Train Epoch: 49 Average training loss: 0.2886 

50 0 0.17523200809955597
50 100 0.2675580226617957
50 200 0.28596215532080094
50 300 0.2922237021925571
50 400 0.2890097166302796
50 500 0.2772434613929077
50 600 0.2689785217261297
50 700 0.28057336187171833
50 800 0.31679530979951787
50 900 0.2998574030152065
50 1000 0.3036532018499987
50 1100 0.2992318143350408
50 1200 0.3017396892032618
50 1300 0.29216157810072196
50 1400 0.2617246553580696
50 1500 0.3095498112570418
50 1600 0.2914102058720786
50 1700 0.2988206053619858
50 1800 0.32756505148392656
50 1900 0.31078134632954607
50 2000 0.3281935366570031
50 2100 0.3238571405304398

Train Epoch: 50 Average training loss: 0.2930 

