{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import compare_ssim\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_complex_np(data):\n",
    "\n",
    "    data = data.numpy()\n",
    "    return data[..., 0] + 1j * data[..., 1]\n",
    "\n",
    "\n",
    "def to_tensor(data):\n",
    "\n",
    "    if np.iscomplexobj(data):\n",
    "        data = np.stack((data.real, data.imag), axis=-1)\n",
    "    return torch.from_numpy(data)\n",
    "\n",
    "\n",
    "def apply_mask(data, mask_func, seed=None):\n",
    "\n",
    "    shape = np.array(data.shape)\n",
    "    shape[:-3] = 1\n",
    "    mask = mask_func(shape, seed)\n",
    "    return torch.where(mask == 0, torch.Tensor([0]), data), mask\n",
    "\n",
    "\n",
    "def fft2(data):\n",
    "\n",
    "    assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.fft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def ifft2(data):\n",
    "\n",
    "    assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.ifft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def complex_abs(data):\n",
    "\n",
    "    assert data.size(-1) == 2\n",
    "    return (data ** 2).sum(dim=-1).sqrt()\n",
    "\n",
    "\n",
    "def root_sum_of_squares(data, dim=0):\n",
    "\n",
    "    return torch.sqrt((data ** 2).sum(dim))\n",
    "\n",
    "\n",
    "def center_crop(data, shape):\n",
    "\n",
    "    assert 0 < shape[0] <= data.shape[-2]\n",
    "    assert 0 < shape[1] <= data.shape[-1]\n",
    "    w_from = (data.shape[-2] - shape[0]) // 2\n",
    "    h_from = (data.shape[-1] - shape[1]) // 2\n",
    "    w_to = w_from + shape[0]\n",
    "    h_to = h_from + shape[1]\n",
    "    return data[..., w_from:w_to, h_from:h_to]\n",
    "\n",
    "\n",
    "def complex_center_crop(data, shape):\n",
    "\n",
    "    assert 0 < shape[0] <= data.shape[-3]\n",
    "    assert 0 < shape[1] <= data.shape[-2]\n",
    "    w_from = (data.shape[-3] - shape[0]) // 2\n",
    "    h_from = (data.shape[-2] - shape[1]) // 2\n",
    "    w_to = w_from + shape[0]\n",
    "    h_to = h_from + shape[1]\n",
    "    return data[..., w_from:w_to, h_from:h_to, :]\n",
    "\n",
    "\n",
    "def normalize(data, mean, stddev, eps=0.):\n",
    "\n",
    "    return (data - mean) / (stddev + eps)\n",
    "\n",
    "\n",
    "def normalize_instance(data, eps=0.):\n",
    "\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    return normalize(data, mean, std, eps), mean, std\n",
    "    \n",
    "\n",
    "def roll(x, shift, dim):\n",
    "\n",
    "    if isinstance(shift, (tuple, list)):\n",
    "        assert len(shift) == len(dim)\n",
    "        for s, d in zip(shift, dim):\n",
    "            x = roll(x, s, d)\n",
    "        return x\n",
    "    shift = shift % x.size(dim)\n",
    "    if shift == 0:\n",
    "        return x\n",
    "    left = x.narrow(dim, 0, x.size(dim) - shift)\n",
    "    right = x.narrow(dim, x.size(dim) - shift, shift)\n",
    "    return torch.cat((right, left), dim=dim)\n",
    "\n",
    "\n",
    "def fftshift(x, dim=None):\n",
    "\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [dim // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = x.shape[dim] // 2\n",
    "    else:\n",
    "        shift = [x.shape[i] // 2 for i in dim]\n",
    "    return roll(x, shift, dim)\n",
    "\n",
    "\n",
    "def ifftshift(x, dim=None):\n",
    "\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [(dim + 1) // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = (x.shape[dim] + 1) // 2\n",
    "    else:\n",
    "        shift = [(x.shape[i] + 1) // 2 for i in dim]\n",
    "    return roll(x, shift, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskFunc:\n",
    "\n",
    "    def __init__(self, center_fractions, accelerations):\n",
    "       \n",
    "        if len(center_fractions) != len(accelerations):\n",
    "            raise ValueError('Number of center fractions should match number of accelerations')\n",
    "\n",
    "        self.center_fractions = center_fractions\n",
    "        self.accelerations = accelerations\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, shape, seed=None):\n",
    "\n",
    "        if len(shape) < 3:\n",
    "            raise ValueError('Shape should have 3 or more dimensions')\n",
    "\n",
    "        self.rng.seed(seed)\n",
    "        num_cols = shape[-2]\n",
    "\n",
    "        choice = self.rng.randint(0, len(self.accelerations))\n",
    "        center_fraction = self.center_fractions[choice]\n",
    "        acceleration = self.accelerations[choice]\n",
    "\n",
    "        num_low_freqs = int(round(num_cols * center_fraction))\n",
    "        prob = (num_cols / acceleration - num_low_freqs) / (num_cols - num_low_freqs)\n",
    "        mask = self.rng.uniform(size=num_cols) < prob\n",
    "        pad = (num_cols - num_low_freqs + 1) // 2\n",
    "        mask[pad:pad + num_low_freqs] = True\n",
    "\n",
    "        mask_shape = [1 for _ in shape]\n",
    "        mask_shape[-2] = num_cols\n",
    "        mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "        data_list[train_and_val[i]] = [] \n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "            if not os.path.isfile(subject_data_path): continue\n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "            \n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        \n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "\n",
    "    fname, rawdata_path, slice = subject_id\n",
    "    \n",
    "    with h5py.File(rawdata_path, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "    \n",
    "    img_und = ifft2(masked_kspace)\n",
    "    img_und = complex_center_crop(img_und, [320, 320])\n",
    "    img_und = complex_abs(img_und)\n",
    "    norm = img_und.max()\n",
    "    \n",
    "    img_und, mean, std = normalize_instance(img_und, eps=1e-11)\n",
    "    img_und = img_und.clamp(-6, 6)\n",
    "    \n",
    "    img_gt = ifft2(slice_kspace)\n",
    "    img_gt = complex_center_crop(img_gt, [320, 320])\n",
    "    img_gt = complex_abs(img_gt)\n",
    "    img_gt = normalize(img_gt, mean, std, eps=1e-11)\n",
    "    img_gt = img_gt.clamp(-6, 6)\n",
    "    \n",
    "    return img_gt, img_und, masked_kspace.squeeze(0), masks.squeeze(0), mean, std, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(gt, pred):\n",
    "\n",
    "    return compare_ssim(gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), \n",
    "                        multichannel=True, data_range=gt.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reconstructions(test_loader, model_log):\n",
    "#     model.load_state_dict(torch.load(root + model_log))\n",
    "#     for in enumerate(test_loader):\n",
    "#     outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructions(reconstructions, out_dir):\n",
    "\n",
    "    for fname, recons in reconstructions.items():\n",
    "        subject_path = os.path.join(out_dir, fname)\n",
    "        print(subject_path)\n",
    "        with h5py.File(subject_path, 'w') as f:\n",
    "            f.create_dataset('reconstruction', data=recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None):\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
    "            f'drop_prob={self.drop_prob})'\n",
    "\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob)\n",
    "\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob)]\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            nn.Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, interval, model_save, model_log):\n",
    "        \n",
    "    if not model_log is None:\n",
    "        model.load_state_dict(torch.load(root + model_log))\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):    \n",
    "        model.train()\n",
    "        \n",
    "        avg_loss = 0\n",
    "        train_loss = 0\n",
    "        iter_loss = []\n",
    "        \n",
    "        for iteration, sample in enumerate(train_loader):\n",
    "            img_gt, img_und, masked_kspace, masks, mean, std, norm = sample\n",
    "\n",
    "            X = img_und\n",
    "            Y = img_gt\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = F.mse_loss(output, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = 0.99 * train_loss + 0.01 * loss.item() if iteration > 0 else loss.item()\n",
    "            iter_loss.append(train_loss)\n",
    "\n",
    "            if iteration % interval == 0:\n",
    "                print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(epoch,\n",
    "                iteration, len(train_dataset),100. * iteration / len(train_dataset), train_loss))\n",
    "        \n",
    "        avg_loss = np.mean(iter_loss)\n",
    "        print('\\nTrain Epoch: {} Average training loss: {:.4f} \\n'.format(epoch, avg_loss))\n",
    "        torch.save(model.state_dict(), root + model_save)\n",
    "\n",
    "\n",
    "def val(model_log):\n",
    "\n",
    "    model.load_state_dict(torch.load(root + model_log))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    val_loss = []\n",
    "    avg_loss = 0\n",
    "    val_ssim = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in val_loader:\n",
    "            img_gt, img_und, masked_kspace, masks, mean, std, norm = sample\n",
    "\n",
    "            X = img_und\n",
    "            Y = img_gt\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            \n",
    "            result = model(X)\n",
    "            mean = mean.unsqueeze(1).unsqueeze(2).to(device)\n",
    "            std = std.unsqueeze(1).unsqueeze(2).to(device)\n",
    "            Y = Y * std + mean\n",
    "            result = result * std + mean\n",
    "            \n",
    "            norm = norm.unsqueeze(1).unsqueeze(2).to(device)\n",
    "            loss = mse_loss(result / norm, Y / norm, size_average=False)\n",
    "            val_loss.append(loss.item())\n",
    "                      \n",
    "            result, Y = result.cpu(), Y.cpu()\n",
    "            val_ssim += ssim(Y.squeeze(0).numpy(), result.squeeze(0).numpy())\n",
    "\n",
    "    avg_loss = np.mean(val_loss)\n",
    "    val_ssim /= len(val_dataset)\n",
    "    print('\\nAverage loss: {:.4f} \\nAverage SSIM: {:.4f}'.format(avg_loss, val_ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    root = '/home/kevinxu/NC2019MRI/fastMRI/models/'\n",
    "    data_path_train = '/home/kevinxu/Documents/NC2019MRI/train/'\n",
    "    data_path_val = '/home/kevinxu/Documents/NC2019MRI/train/'\n",
    "    data_list = load_data_path(data_path_train, data_path_val)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    acc = 8\n",
    "    cen_fract = 0.04 \n",
    "    seed = False \n",
    "    num_batch = 1\n",
    "    num_workers = 12\n",
    "    \n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, \n",
    "                    center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, \n",
    "                    batch_size=num_batch, num_workers=num_workers)\n",
    "\n",
    "    val_dataset = MRIDataset(data_list['val'], acceleration=acc, \n",
    "                    center_fraction=cen_fract, use_seed=seed)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True, \n",
    "                    batch_size=num_batch, num_workers=num_workers) \n",
    "    \n",
    "    model = UnetModel(in_chans=1, out_chans=1, chans=32, # 64, 128\n",
    "                        num_pool_layers=4, drop_prob=0.5).to(device)\n",
    "    \n",
    "    optimizer = optim.RMSprop(params=model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 1 [0/2134 (0%)]\tLoss: 1.2922\n",
      "\n",
      "Train Epoch: 1 [20/2134 (1%)]\tLoss: 1.2038\n",
      "\n",
      "Train Epoch: 1 [40/2134 (2%)]\tLoss: 1.0796\n",
      "\n",
      "Train Epoch: 1 [60/2134 (3%)]\tLoss: 0.9903\n",
      "\n",
      "Train Epoch: 1 [80/2134 (4%)]\tLoss: 0.8855\n",
      "\n",
      "Train Epoch: 1 [100/2134 (5%)]\tLoss: 0.8033\n",
      "\n",
      "Train Epoch: 1 [120/2134 (6%)]\tLoss: 0.7408\n",
      "\n",
      "Train Epoch: 1 [140/2134 (7%)]\tLoss: 0.6756\n",
      "\n",
      "Train Epoch: 1 [160/2134 (7%)]\tLoss: 0.6272\n",
      "\n",
      "Train Epoch: 1 [180/2134 (8%)]\tLoss: 0.5999\n",
      "\n",
      "Train Epoch: 1 [200/2134 (9%)]\tLoss: 0.5702\n",
      "\n",
      "Train Epoch: 1 [220/2134 (10%)]\tLoss: 0.5509\n",
      "\n",
      "Train Epoch: 1 [240/2134 (11%)]\tLoss: 0.5279\n",
      "\n",
      "Train Epoch: 1 [260/2134 (12%)]\tLoss: 0.5256\n",
      "\n",
      "Train Epoch: 1 [280/2134 (13%)]\tLoss: 0.5070\n",
      "\n",
      "Train Epoch: 1 [300/2134 (14%)]\tLoss: 0.4967\n",
      "\n",
      "Train Epoch: 1 [320/2134 (15%)]\tLoss: 0.5073\n",
      "\n",
      "Train Epoch: 1 [340/2134 (16%)]\tLoss: 0.4903\n",
      "\n",
      "Train Epoch: 1 [360/2134 (17%)]\tLoss: 0.4770\n",
      "\n",
      "Train Epoch: 1 [380/2134 (18%)]\tLoss: 0.4687\n",
      "\n",
      "Train Epoch: 1 [400/2134 (19%)]\tLoss: 0.5040\n",
      "\n",
      "Train Epoch: 1 [420/2134 (20%)]\tLoss: 0.4884\n",
      "\n",
      "Train Epoch: 1 [440/2134 (21%)]\tLoss: 0.4924\n",
      "\n",
      "Train Epoch: 1 [460/2134 (22%)]\tLoss: 0.4853\n",
      "\n",
      "Train Epoch: 1 [480/2134 (22%)]\tLoss: 0.4753\n",
      "\n",
      "Train Epoch: 1 [500/2134 (23%)]\tLoss: 0.4651\n",
      "\n",
      "Train Epoch: 1 [520/2134 (24%)]\tLoss: 0.4770\n",
      "\n",
      "Train Epoch: 1 [540/2134 (25%)]\tLoss: 0.4464\n",
      "\n",
      "Train Epoch: 1 [560/2134 (26%)]\tLoss: 0.4491\n",
      "\n",
      "Train Epoch: 1 [580/2134 (27%)]\tLoss: 0.4460\n",
      "\n",
      "Train Epoch: 1 [600/2134 (28%)]\tLoss: 0.4452\n",
      "\n",
      "Train Epoch: 1 [620/2134 (29%)]\tLoss: 0.4598\n",
      "\n",
      "Train Epoch: 1 [640/2134 (30%)]\tLoss: 0.4627\n",
      "\n",
      "Train Epoch: 1 [660/2134 (31%)]\tLoss: 0.4601\n",
      "\n",
      "Train Epoch: 1 [680/2134 (32%)]\tLoss: 0.4747\n",
      "\n",
      "Train Epoch: 1 [700/2134 (33%)]\tLoss: 0.4567\n",
      "\n",
      "Train Epoch: 1 [720/2134 (34%)]\tLoss: 0.4463\n",
      "\n",
      "Train Epoch: 1 [740/2134 (35%)]\tLoss: 0.4440\n",
      "\n",
      "Train Epoch: 1 [760/2134 (36%)]\tLoss: 0.4386\n",
      "\n",
      "Train Epoch: 1 [780/2134 (37%)]\tLoss: 0.4173\n",
      "\n",
      "Train Epoch: 1 [800/2134 (37%)]\tLoss: 0.4163\n",
      "\n",
      "Train Epoch: 1 [820/2134 (38%)]\tLoss: 0.4373\n",
      "\n",
      "Train Epoch: 1 [840/2134 (39%)]\tLoss: 0.4572\n",
      "\n",
      "Train Epoch: 1 [860/2134 (40%)]\tLoss: 0.4548\n",
      "\n",
      "Train Epoch: 1 [880/2134 (41%)]\tLoss: 0.4423\n",
      "\n",
      "Train Epoch: 1 [900/2134 (42%)]\tLoss: 0.4466\n",
      "\n",
      "Train Epoch: 1 [920/2134 (43%)]\tLoss: 0.4405\n",
      "\n",
      "Train Epoch: 1 [940/2134 (44%)]\tLoss: 0.4289\n",
      "\n",
      "Train Epoch: 1 [960/2134 (45%)]\tLoss: 0.4294\n",
      "\n",
      "Train Epoch: 1 [980/2134 (46%)]\tLoss: 0.4443\n",
      "\n",
      "Train Epoch: 1 [1000/2134 (47%)]\tLoss: 0.4483\n",
      "\n",
      "Train Epoch: 1 [1020/2134 (48%)]\tLoss: 0.4585\n",
      "\n",
      "Train Epoch: 1 [1040/2134 (49%)]\tLoss: 0.4610\n",
      "\n",
      "Train Epoch: 1 [1060/2134 (50%)]\tLoss: 0.4774\n",
      "\n",
      "Train Epoch: 1 [1080/2134 (51%)]\tLoss: 0.4877\n",
      "\n",
      "Train Epoch: 1 [1100/2134 (52%)]\tLoss: 0.4837\n",
      "\n",
      "Train Epoch: 1 [1120/2134 (52%)]\tLoss: 0.4789\n",
      "\n",
      "Train Epoch: 1 [1140/2134 (53%)]\tLoss: 0.4740\n",
      "\n",
      "Train Epoch: 1 [1160/2134 (54%)]\tLoss: 0.4719\n",
      "\n",
      "Train Epoch: 1 [1180/2134 (55%)]\tLoss: 0.4589\n",
      "\n",
      "Train Epoch: 1 [1200/2134 (56%)]\tLoss: 0.4596\n",
      "\n",
      "Train Epoch: 1 [1220/2134 (57%)]\tLoss: 0.4543\n",
      "\n",
      "Train Epoch: 1 [1240/2134 (58%)]\tLoss: 0.4349\n",
      "\n",
      "Train Epoch: 1 [1260/2134 (59%)]\tLoss: 0.4377\n",
      "\n",
      "Train Epoch: 1 [1280/2134 (60%)]\tLoss: 0.4291\n",
      "\n",
      "Train Epoch: 1 [1300/2134 (61%)]\tLoss: 0.4247\n",
      "\n",
      "Train Epoch: 1 [1320/2134 (62%)]\tLoss: 0.4549\n",
      "\n",
      "Train Epoch: 1 [1340/2134 (63%)]\tLoss: 0.4466\n",
      "\n",
      "Train Epoch: 1 [1360/2134 (64%)]\tLoss: 0.4435\n",
      "\n",
      "Train Epoch: 1 [1380/2134 (65%)]\tLoss: 0.4453\n",
      "\n",
      "Train Epoch: 1 [1400/2134 (66%)]\tLoss: 0.4416\n",
      "\n",
      "Train Epoch: 1 [1420/2134 (67%)]\tLoss: 0.4344\n",
      "\n",
      "Train Epoch: 1 [1440/2134 (67%)]\tLoss: 0.4396\n",
      "\n",
      "Train Epoch: 1 [1460/2134 (68%)]\tLoss: 0.4517\n",
      "\n",
      "Train Epoch: 1 [1480/2134 (69%)]\tLoss: 0.4516\n",
      "\n",
      "Train Epoch: 1 [1500/2134 (70%)]\tLoss: 0.4512\n",
      "\n",
      "Train Epoch: 1 [1520/2134 (71%)]\tLoss: 0.4582\n",
      "\n",
      "Train Epoch: 1 [1540/2134 (72%)]\tLoss: 0.4510\n",
      "\n",
      "Train Epoch: 1 [1560/2134 (73%)]\tLoss: 0.4435\n",
      "\n",
      "Train Epoch: 1 [1580/2134 (74%)]\tLoss: 0.4244\n",
      "\n",
      "Train Epoch: 1 [1600/2134 (75%)]\tLoss: 0.4382\n",
      "\n",
      "Train Epoch: 1 [1620/2134 (76%)]\tLoss: 0.4275\n",
      "\n",
      "Train Epoch: 1 [1640/2134 (77%)]\tLoss: 0.4106\n",
      "\n",
      "Train Epoch: 1 [1660/2134 (78%)]\tLoss: 0.4141\n",
      "\n",
      "Train Epoch: 1 [1680/2134 (79%)]\tLoss: 0.4260\n",
      "\n",
      "Train Epoch: 1 [1700/2134 (80%)]\tLoss: 0.4114\n",
      "\n",
      "Train Epoch: 1 [1720/2134 (81%)]\tLoss: 0.3907\n",
      "\n",
      "Train Epoch: 1 [1740/2134 (82%)]\tLoss: 0.4010\n",
      "\n",
      "Train Epoch: 1 [1760/2134 (82%)]\tLoss: 0.4259\n",
      "\n",
      "Train Epoch: 1 [1780/2134 (83%)]\tLoss: 0.4284\n",
      "\n",
      "Train Epoch: 1 [1800/2134 (84%)]\tLoss: 0.4284\n",
      "\n",
      "Train Epoch: 1 [1820/2134 (85%)]\tLoss: 0.4220\n",
      "\n",
      "Train Epoch: 1 [1840/2134 (86%)]\tLoss: 0.4352\n",
      "\n",
      "Train Epoch: 1 [1860/2134 (87%)]\tLoss: 0.4470\n",
      "\n",
      "Train Epoch: 1 [1880/2134 (88%)]\tLoss: 0.4330\n",
      "\n",
      "Train Epoch: 1 [1900/2134 (89%)]\tLoss: 0.4243\n",
      "\n",
      "Train Epoch: 1 [1920/2134 (90%)]\tLoss: 0.4202\n",
      "\n",
      "Train Epoch: 1 [1940/2134 (91%)]\tLoss: 0.4352\n",
      "\n",
      "Train Epoch: 1 [1960/2134 (92%)]\tLoss: 0.4347\n",
      "\n",
      "Train Epoch: 1 [1980/2134 (93%)]\tLoss: 0.4444\n",
      "\n",
      "Train Epoch: 1 [2000/2134 (94%)]\tLoss: 0.4467\n",
      "\n",
      "Train Epoch: 1 [2020/2134 (95%)]\tLoss: 0.4628\n",
      "\n",
      "Train Epoch: 1 [2040/2134 (96%)]\tLoss: 0.4578\n",
      "\n",
      "Train Epoch: 1 [2060/2134 (97%)]\tLoss: 0.4500\n",
      "\n",
      "Train Epoch: 1 [2080/2134 (97%)]\tLoss: 0.4344\n",
      "\n",
      "Train Epoch: 1 [2100/2134 (98%)]\tLoss: 0.4244\n",
      "\n",
      "Train Epoch: 1 [2120/2134 (99%)]\tLoss: 0.4378\n",
      "\n",
      "Train Epoch: 1 Average training loss: 0.4902 \n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/NC2019MRI/fastMRI/models/unet_test.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9c13c571e9e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unet_test.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-bafff8e56c8d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, interval, model_save, model_log)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTrain Epoch: {} Average training loss: {:.4f} \\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/NC2019MRI/fastMRI/models/unet_test.pt'"
     ]
    }
   ],
   "source": [
    "train(1, 20, 'unet_test.pt', model_log=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
